[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Yekta’s Dissertation Website",
    "section": "",
    "text": "Hi! Welcome to my dissertation website. I started my PhD in Management Science & Engineering at University of Waterloo in August 2021. This website will walk you through my entire project, which took me over 3 years to finish. I’m hoping this project can be used by others as inspiration, and hopefully, as a portfolio project for my own! I find a lot of this stuff fun! I’m also hoping to make everything super accessible to everyone, even if you’re coming from no background in Statistics/Math/Computer Science.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi! My Name is Yekta (read exactly how you write it), my pronouns are She/Her. I was born in Tehran, Iran. Since I was a kid, I’ve always loved learning and adventures. Movies are my first passion! After that, it’s TV shows, music, gadgets and technology, books, writing and studying! Yes! I actually like studying!\nI quickly found out I was much better at Math and Physics than Science and any other field that required memorizing facts (History). I wanted to be an astronaut, then an actress, then a filmmaker, then a writer, then a super model (plans didn’t work out, I stopped growing taller at 14!), then a robotics engineer, then a computer engineer, then an electrical engineer, then a photographer, etc. I thought actor was the one that made the most sense, since I wanted to be so many things! At least with acting, I could pretend to be all of those things!\nWell, I chose mathematics in highschool, and since I was good at it, I continued with mathematics to college. Big Mistake (For Me)! Studying math in undergrad at University of Tehran was like studying Chinese, Greek, Russian and Arabic at the same time. You don’t see numbers after Calculus 2, which you take in the second semester/term. All abstract ideas, proofs, theorems, n-dimensional spaces and curves that curve inside of themselves! “I should have studied Accounting!” (Which I did, btw! I actually took an entire accounting training course outside of university and found it to be sould-crushingly boring, no offense to all accountants.)\nOk, so if math isn’t about numbers and I don’t actually want to work with numbers - then where do I go?! So, thankfully, at UT, I was able to take most of my electives courses from the two other majors in the math department: computer science and statistics. This was the turning point for me. “Here’s where that theory applies, you know, the one you kept asking why you’re even learning its proof”… So I figured I should change my major. Mathematics had taught me nothing but how to think, how to analyze, how to be logical and how important it was to know why stuff made sense… so really, nothing much! I didn’t even realize how much studying mathematics had taught me until years later.\nI still changed my major for graduate school, and decided to try Industrial & Systems Engineering, with a specialization on Macrosystems engineering. I taught myself Python and SQL, I did very well during graduate school with my courses, I worked on research projects and had the best time writing my thesis. So I thought, wow, I actually like all of this! Why not do a PhD? And here I am! I love learning, getting deep in a topic, planning a project from the start to the end."
  },
  {
    "objectID": "about.html#about-me",
    "href": "about.html#about-me",
    "title": "About",
    "section": "",
    "text": "Hi! My Name is Yekta (read exactly how you write it), my pronouns are She/Her. I was born in Tehran, Iran. Since I was a kid, I’ve always loved learning and adventures. Movies are my first passion! After that, it’s TV shows, music, gadgets and technology, books, writing and studying! Yes! I actually like studying!\nI quickly found out I was much better at Math and Physics than Science and any other field that required memorizing facts (History). I wanted to be an astronaut, then an actress, then a filmmaker, then a writer, then a super model (plans didn’t work out, I stopped growing taller at 14!), then a robotics engineer, then a computer engineer, then an electrical engineer, then a photographer, etc. I thought actor was the one that made the most sense, since I wanted to be so many things! At least with acting, I could pretend to be all of those things!\nWell, I chose mathematics in highschool, and since I was good at it, I continued with mathematics to college. Big Mistake (For Me)! Studying math in undergrad at University of Tehran was like studying Chinese, Greek, Russian and Arabic at the same time. You don’t see numbers after Calculus 2, which you take in the second semester/term. All abstract ideas, proofs, theorems, n-dimensional spaces and curves that curve inside of themselves! “I should have studied Accounting!” (Which I did, btw! I actually took an entire accounting training course outside of university and found it to be sould-crushingly boring, no offense to all accountants.)\nOk, so if math isn’t about numbers and I don’t actually want to work with numbers - then where do I go?! So, thankfully, at UT, I was able to take most of my electives courses from the two other majors in the math department: computer science and statistics. This was the turning point for me. “Here’s where that theory applies, you know, the one you kept asking why you’re even learning its proof”… So I figured I should change my major. Mathematics had taught me nothing but how to think, how to analyze, how to be logical and how important it was to know why stuff made sense… so really, nothing much! I didn’t even realize how much studying mathematics had taught me until years later.\nI still changed my major for graduate school, and decided to try Industrial & Systems Engineering, with a specialization on Macrosystems engineering. I taught myself Python and SQL, I did very well during graduate school with my courses, I worked on research projects and had the best time writing my thesis. So I thought, wow, I actually like all of this! Why not do a PhD? And here I am! I love learning, getting deep in a topic, planning a project from the start to the end."
  },
  {
    "objectID": "study1.html#part-2.",
    "href": "study1.html#part-2.",
    "title": "The Inner Workings of Mobile Banking Adoption: A Systematic Literature Review of Intrinsic Factors",
    "section": "Part 2.",
    "text": "Part 2.",
    "crumbs": [
      "Home",
      "Projects",
      "Project 1. SLR"
    ]
  },
  {
    "objectID": "study1.html#part-3.",
    "href": "study1.html#part-3.",
    "title": "The Inner Workings of Mobile Banking Adoption: A Systematic Literature Review of Intrinsic Factors",
    "section": "Part 3.",
    "text": "Part 3.",
    "crumbs": [
      "Home",
      "Projects",
      "Project 1. SLR"
    ]
  },
  {
    "objectID": "study1.html#part-2.-data-analysis",
    "href": "study1.html#part-2.-data-analysis",
    "title": "The Inner Workings of Mobile Banking Adoption: A Systematic Literature Review of Intrinsic Factors",
    "section": "",
    "text": "The R libraries used for data analysis are as follows:\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(purrr)\nlibrary(ggplot2)\nlibrary(psych)\nlibrary(tidyr)\nlibrary(stargazer)\nlibrary(forcats)\nlibrary(xtable)\nlibrary(ggraph)\nlibrary(igraph)\nlibrary(gt)\nlibrary(ggpubr)\n\nLooking at the data:\n\ndf &lt;- read.csv(\"data/P2_AR_07.csv\") \nglimpse(df)\n\nSummary statiscs\n\npsych::describe(df %&gt;% \n    dplyr::select(Year, Match, Num_Factors, NUM_FAC_NOTSIG, SampleSize)) %&gt;% \n    dplyr::select(vars, n, mean, sd, median, min, max) \n\nWoah! One paper has 25,000 and that is messing up the sample sizes. Remembering this study’s ID:\n\ndf %&gt;% filter(SampleSize == 25000) %&gt;% \n    dplyr::select(ID, Title, SampleSize)\n\nSetting aside the study with sample size of 25,000:\n\npsych::describe(\n    df %&gt;% dplyr::select(Year, Match, Num_Factors, NUM_FAC_NOTSIG, SampleSize) %&gt;% \n    filter(SampleSize != 25000)) %&gt;% \n    dplyr::select(vars, n, mean, sd, median, min, max) \n\nWelp! Another large study.\n\ndf %&gt;% filter(SampleSize == 21526) %&gt;% \n    dplyr::select(ID, Title, SampleSize)\n\nSetting aside the study with sample size of 25,000 and the one with 21,52 as they are outliers:\n\npsych::describe(\n    df %&gt;% filter(!ID %in% c('p2_59','p2_77')) %&gt;% \n    dplyr::select(ID, Year,Match, Num_Factors, NUM_FAC_NOTSIG, SampleSize)) %&gt;% \n    dplyr::select(vars, n, mean, sd, median, min, max) \n\nCounting the unique values for each of the columns:\n\nresults &lt;- c(\n  paste('Number of Unique Values in ID: ', n_distinct(df$ID)),\n  paste('Number of Unique Values in Title: ', n_distinct(df$Title)),\n  paste('Number of Unique Values in PublicationTitles: ', n_distinct(df$PublicationTitle)),\n  paste('Number of Unique Values in Publisher: ', n_distinct(df$Publisher)),\n  paste('Number of Unique Values in AffiliationCountry: ', n_distinct(df$AffiliationCountry)),\n  paste('Number of Unique Values in Factors: ', dplyr::n_distinct(df %&gt;% dplyr::select(F1:F9) %&gt;% unlist())),\n  paste('Number of Unique Values in Not Sig: ', dplyr::n_distinct(df %&gt;% dplyr::select(FNS1:FNS4) %&gt;% unlist())),\n  paste('Number of Unique Values in Methods: ', dplyr::n_distinct(df %&gt;% dplyr::select(METHOD1:METHOD4) %&gt;% unlist())),\n  paste('Number of Unique Values in Theory: ', dplyr::n_distinct(df %&gt;% dplyr::select(THEORY1:THEORY4) %&gt;% unlist())),\n  paste('Number of Unique Values in Limits: ', dplyr::n_distinct(df %&gt;% dplyr::select(LIMIT1:LIMIT3) %&gt;% unlist())),\n  paste('Number of Unique Values in ResearchType: ', n_distinct(df$ResearchType)),\n  paste('Number of Unique Values in Authors: ', n_distinct(df$Creators)),\n  paste('Number of Unique Values in Keywords: ', dplyr::n_distinct(df %&gt;% dplyr::select(K1:K10) %&gt;% unlist())),\n  paste('Number of Unique Values in Tech: ', n_distinct(df$Tech)),\n  paste('Number of Unique Values in Themes: ', n_distinct(df$DecisionTheme))\n)\n\ncat(results, sep = \"\\n\")\n\nChecking the sample sizes Without the outliers:\n\npsych::describe(\n    df %&gt;% filter(!ID %in% c('p2_59','p2_77')) %&gt;% \n    dplyr::select(SampleSize)) %&gt;% \n    dplyr::select(n, mean, sd, median, min, max) \n\n\nnoOutliers &lt;- df %&gt;% filter(!ID %in% c('p2_59','p2_77'))\n\nquantiles &lt;- quantile(noOutliers$SampleSize, na.rm = T)\n\nquantile_binned &lt;- cut(df$SampleSize, \n                breaks = quantiles, \n                labels = c(\"SQ1\", \"SQ2\", \"SQ3\", \"SQ4\"), \n                include.lowest = TRUE)\n\ndf$SampleSizeBin &lt;- quantile_binned\n\ndf &lt;- df %&gt;% mutate(\n    SampleSizeBin = if_else(\n        is.na(SampleSizeBin),\n        \"NotStated\",\n        SampleSizeBin\n    )\n)\n\ndf %&gt;% count(SampleSizeBin)\n\nLet’s calculate the scores for factors that are significant and non-significant:\n\nF_counts &lt;- df %&gt;%\n  dplyr::select(F1:F9) %&gt;%\n  unlist() %&gt;%\n  table() %&gt;%\n  as.data.frame() %&gt;%\n  rename(FAC = \".\", F_count = \"Freq\")\n\nFNS_counts &lt;- df %&gt;%\n  dplyr::select(FNS1:FNS4) %&gt;%\n  unlist() %&gt;%\n  table() %&gt;%\n  as.data.frame() %&gt;%\n  rename(FAC = \".\", FNS_counts = \"Freq\")\n\n# Count occurrences of each factor in all columns (F1 to F9 + FNS1 to FNS4)\nTotal_counts &lt;- df %&gt;%\n  dplyr::select(c(F1:F9, FNS1:FNS4)) %&gt;%\n  unlist() %&gt;%\n  table() %&gt;%\n  as.data.frame() %&gt;%\n  rename(FAC = \".\", Total_count = \"Freq\")\n\n# Merge the two count tables\nfactor_scores &lt;- merge(F_counts,FNS_counts, by = \"FAC\", all = TRUE)\nfactor_scores &lt;- merge(factor_scores, Total_counts, by = \"FAC\", all = TRUE)\n\n\n# Replace NAs with 0 for cases where factors appear in some but not all sections\nfactor_scores[is.na(factor_scores)] &lt;- 0\n\nfactor_scores &lt;- factor_scores %&gt;%\n  mutate(Score_Sig = round(F_count / Total_count, 2),\n         Score_NOT_Sig = round(FNS_counts / Total_count, 2)) %&gt;% filter(FAC != \"\")\n\n\nhead(factor_scores)\n\n\n\n\nNow let’s actually do some analysis. Let’s visualize how the themes of the papers have changed across the years. I will first generate a bar plot that fills the bars at each year (as a categorical factor) with proportions of themes in that year. This is an aggregation that happens under the hood, and using position = \"fill\" will actually make sure all the bars consider things relative to eachother, filling the full 100% of the bar.\n\nggplot(df, aes(x = as.factor(Year), fill = DecisionTheme)) +\n  geom_bar(position = \"fill\") +\n  theme_minimal() +\n  labs(fill = \"Theme\",\n       x = \"Year\",\n       y = \"Total Count\") +\n  fill_palette(\"Set3\")\n\nTo see how things move/flow over the years, a line chart is a great idea:\n\ndf %&gt;%\n    dplyr::count(DecisionTheme, Year) %&gt;%\n    ggplot(aes(x = as.factor(Year), y = n, color = DecisionTheme, group = DecisionTheme)) +\n  geom_line() +\n  geom_point() +\n  theme_minimal() +\n  labs(fill = \"Theme\",\n       x = \"Year\",\n       y = \"Total Count\") +\n  fill_palette(\"Dark2\")\n\nFor analysis, I will need to convert the data to long format. Since I want to avoid making it too big, I’ll do this separately for each key variable.\n\ntheory_long &lt;- df %&gt;% \n    pivot_longer(\n        cols = THEORY1:THEORY4,\n        names_to = \"THEORY_NAME\", \n        values_to = \"THEORY\"\n    ) \n\nmethod_long &lt;- df %&gt;% \n    pivot_longer(\n        cols = METHOD1:METHOD4,\n        names_to = \"METHODNAME\", \n        values_to = \"METHOD\"\n    ) \n\nlimit_long &lt;- df %&gt;% \n    pivot_longer(\n        cols = LIMIT1:LIMIT3,\n        names_to = \"LIMITNAME\", \n        values_to = \"LIMIT\"\n    ) \n\nfac_long &lt;- df %&gt;% \n    pivot_longer(\n        cols = F1:F9,\n        names_to = \"FACNAME\",\n        values_to = \"FAC\"\n    )\n\nfac_NS_long &lt;- df %&gt;% \n    pivot_longer(\n        cols = FNS1:FNS4,\n        names_to = \"FAC_NS_NAME\",\n        values_to = \"FAC_NS\"\n    )\n\nfactors_based_on_themes &lt;- df %&gt;% pivot_longer(\n    cols = F1_THEME:F9_THEME,\n    names_to = \"FAC_THEMES_NAMES\",\n    values_to = \"FACTHEME\"\n)\n\nRemove all the empty rows:\n\ntheory_long &lt;- theory_long %&gt;% filter(THEORY != \"\") #\nmethod_long &lt;- method_long %&gt;% filter(METHOD != \"\") #\nlimit_long &lt;- limit_long %&gt;% filter(LIMIT != \"\") #\nfac_long &lt;- fac_long %&gt;% filter(FAC != \"\")\nfac_NS_long &lt;- fac_NS_long %&gt;% filter(FAC_NS != \"\")\n\nAdd factor scores to the long factors and non-signficant factors’ data:\n\nfac_long &lt;- merge(fac_long, factor_scores, by = \"FAC\", all = T)\n\n\nfactor_scores &lt;- factor_scores %&gt;% mutate(FAC_NS = FAC) %&gt;% dplyr::select(FAC_NS,Score_Sig, Score_NOT_Sig)\nfac_NS_long &lt;- merge(fac_NS_long, factor_scores, by = \"FAC_NS\", all = T)\n\n\n\n\nNow, I want to explore the interactions between the following properties: themes, theories, methodologies, limitations, factors, years, research types, sample sizes, technologies, and non-significant factors. Some questions that can be answered from such an analysis are:\n\nAre there notable differences in the distribution of themes, theories, methodologies, limitations, factors, research types, sample sizes, technologies, and non-significant factors.across years?\nAre themes, theories, methodologies, limitations, factors, years, research types, sample sizes, and non-significant factors significantly associated with specific technologies?\nAre there significant differences in sample sizes across themes, theories, methodologies, limitations, factors, years, research types, technologies, and non-significant factors?\nDo research types vary significantly among different themes, theories, methodologies, limitations, factors, years, sample sizes, technologies, and non-significant factors?\nAre there significant differences in methods used across themes, theories, limitations, factors, years, research types, sample sizes, technologies, and non-significant factors?\nAre the significant factors identified notably different among themes, theories, methodologies, limitations, years, research types, sample sizes, technologies, and non-significant factors?\nAre the non-significant factors identified notably different among themes, theories, methodologies, limitations, factors, years, research types, sample sizes, and technologies?\n\nTo do this, I will first decide if further investigation is even worthwhile. First, I will use ANOVA to figure out if there are significant differences between groups of the same variable. That is, are themes, theories, methodologies, limitations, technologies, factors, years, research types, sample sizes, and non-significant factors actually different across the dataset?\n\nbuild_anova &lt;- function(nameOfCol){\n    counts_df &lt;- df %&gt;% count({{nameOfCol}}) %&gt;% arrange(desc(n))\n\n    counts_df_long &lt;- data.frame(\n        Group = rep(as.character(counts_df[[1]]), times = counts_df$n),\n        Value = unlist(lapply(counts_df$n, function(x) seq_len(x)))\n    )\n\n    anova_result &lt;- aov(Value ~ Group, data = counts_df_long)\n    return(summary(anova_result))\n}\n\nThemes are significantly different.\n\nbuild_anova(DecisionTheme)\n\nSo, let’s see how they differ across other factors - starting with the ones that do not require pivoting the dataframe! (Year, Tech, SampleSizeBin, ResearchType). This time, I will use a \\chi^2 test of independence.\n\nbuild_contingency_table &lt;- function(nameOfCol){\n    data_combine &lt;- df %&gt;% group_by(DecisionTheme) %&gt;% count({{nameOfCol}}) \n\n    contingency_table &lt;- xtabs(n ~ DecisionTheme + {{nameOfCol}}, data = data_combine)\n    chi_sq_result &lt;- chisq.test(contingency_table)\n    chi_sq_result\n}\n\n\n#build_contingency_table(SampleSizeBin)\n\nYou can also calculate the Cramer V:\n\ntable(is.na(df$SampleSizeBin))\n\n\n#cramerV(build_contingency_table(SampleSizeBin))",
    "crumbs": [
      "Home",
      "Projects",
      "Project 1. SLR"
    ]
  },
  {
    "objectID": "study1.html#part-3.-results",
    "href": "study1.html#part-3.-results",
    "title": "The Inner Workings of Mobile Banking Adoption: A Systematic Literature Review of Intrinsic Factors",
    "section": "Part 3. Results",
    "text": "Part 3. Results",
    "crumbs": [
      "Home",
      "Projects",
      "Project 1. SLR"
    ]
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "The proliferation of Mobile banking, referred to in this project as m-banking, has transformed financial services, enabling consumers to conduct transactions conveniently through mobile devices. Despite the advantages, m-banking adoption rates vary significantly. Adoption is the early stage usage or uptake of a technology. My dissertation aims to bridge critical gaps in understanding how different factors (according to literature trends and through novel contributions) and device-specific attributes shape m-banking adoption. The central research question addressed in this dissertation is: “What are the factors that influence mobile banking adoption across different user segments?”\nThe current chapter is a holistic introduction to the dissertation. To answer the main research question, I conducted three distinct yet interconnected studies. These studies, presented in chronological order in the following chapters, are:\nTheoretical Contributions\nThis dissertation makes several contributions to the field of technology adoption and financial services research.\nPractical Contributions",
    "crumbs": [
      "Home",
      "Introduction"
    ]
  },
  {
    "objectID": "introduction.html#the-inner-workings-of-mobile-banking-adoption-a-systematic-literature-review-of-intrinsic-factors",
    "href": "introduction.html#the-inner-workings-of-mobile-banking-adoption-a-systematic-literature-review-of-intrinsic-factors",
    "title": "Introduction",
    "section": "1 ### The Inner Workings of Mobile Banking Adoption: A Systematic Literature Review of Intrinsic Factors",
    "text": "1 ### The Inner Workings of Mobile Banking Adoption: A Systematic Literature Review of Intrinsic Factors\nDespite extensive research on m-banking adoption, a clear framework categorizing user-specific influences is still missing.\nAfter investigating the m-banking adoption literature, I found that uniform definitions for factors 1 influencing m-banking adoption was lacking. Furthermore, there was an over-reliance on only a few factors and theoretical frameworks. This prompted the undertaking of the SLR study in study 1. To address the problem of lack of uniform definitions, I categorized adoption factors based on context and impact. This way, using group/category membership help identify similarities between factors. Thus making definitions more cohesive.\nSeveral studies support the idea that categorization helps in defining and understanding concepts more easily. Research suggests that categorization plays a crucial role in defining concepts, moving from a classical view (fixed definitions) to a probabilistic view where categorization helps in making sense of concepts based on shared characteristics Medin (1989). Categorization enables more effective organization and processing of information which are essential for learning new concepts (zentallCATEGORIZATIONCONCEPTLEARNING2002?).\nTechnology adoption factors are complex and often belong to multiple categories. Because of this, studies have never explicitly categorized factors across the board. I adopt a broad approach to classifying factors, focusing on two key dimensions of decision-making based on how they relate to the user: Intrinsic and Extrinsic. In the context of m-banking adoption, intrinsic factors discuss how individuals evaluate an m-banking application (Often shortened to “app”) internally — based on perceptions, goals, pressure felt from other people’s judgment, and emotions. Extrinsic factors, on the other hand, refer to the apps’ measurable features, such as performance and functionality. Since extrinsic factors are easy to quantify and experienced similarly across the board, they are also straightforward to study. Thus, I focused on intrinsic factors. Because intrinsic factors relate to so many different inner processes (mentioned above), I further categorized them into four main groups:\n\nPerceptive, which are based on beliefs and perceptions.\nPersonal, which are based on individual motivations and traits.\nSocial, which are based on the impact of others on the decision-maker.\nPsychological, which are based on based on cognitive, emotional and mental processes.\n\nThis categorization provides a useful context-specific grouping. Following this, a systematic search of the m-banking literature for intrinsic factors was carried out. Scientific articles gathered were given themes using text-mining techniques – specifically, lda for Topic Modeling. Some of these themes matched my categorization, as well. I also extended prior SLR studies by using statistical techniques – specifically, anova – to mathematically validate my findings. My results provide a strong empirical foundation for future researchers to do context-focused investigation.\nThe Outcome of this study is to help enhance the understanding of intrinsic factors, highlight underutilized methodologies, and identify research gaps. Additionally, I highlight the dominance of certain theoretical models while advocating for greater exploration of under-studied theories and methods. My findings show notable geographical and study-design biases, particularly longitudinal research in developed nations. My categorized framework helps scholars identify intrinsic factors, relevant theories, and research gaps, which promotes targeted future research.\nFor practitioners and policymakers, I recommend designing emotionally engaging apps, ensuring transparent risk communication, and educating users on safe practices. These steps enhance m-banking adoption and effectiveness.\n\n1.1 The Relationship Between Mental Health and Mobile Banking Adoption: Evidence from Canada\nFollowing the work of the previous study, I focused on intrinsic factors. One of the factors I found to be under-explored in the literature was mental health.\nPsychological factors are comparatively less-studied across the m-banking literature Venkatesh et al. (2012), Zou et al. (2023-05, 2023), Tiwari et al. (2021-12, 2021). I verified this further doing a quick preliminary search on Web-of-Science. Using the following search query returned 1,067 studies with no filters set on the results: !30(“mobile banking” OR “mbanking” OR “m-banking”) AND (“adoption”) When changing the search query to find studies specifically on psychological factors, the total number of studies returned were 157, which is about 14\\% of the total. The new search query was: !30\\parbox0.8 (“mobile banking” OR “mbanking” OR “m-banking”) AND \\ (“adoption”) AND (“affective” OR “psychological” OR “affect based” OR “affect-based” OR “emotional” OR “cognitive” OR “mental” OR “personal”)  \\ Using the same search queries in Scopus, 753 results were returned for the first, and 101 in the second search (13.4\\%). It is clear that only a small portion of the literature in m-banking adoption is focused on psychological factors.\nIn this study, I investigated the direct and moderated effect of mh on m-banking adoption. Moderators were extracted from theories in technology adoption or from literature related to mental health: rs, sd, and sns. I used a fixed effect logistic regression model grouped based on Canadian provinces following the cluster sampling design of my dataset. The results showed that mental health significantly and negatively affects m-banking adoption: better mental health outcomes were associated with lower likelihood of m-banking adoption. I did not find sufficient evidence for the moderating effects.\n%———————————————————————- Device Divide: Unpacking Mobile Banking Adoption Differences for Smartphones and Smart Wearable Devices -section-p3 %———————————————————————- Moving on from focusing only on smartphones, I decided to do a comparative study considering newer devices used in m-banking. One of the findings of my SLR study from Section -section-p2 was that few comparative studies exist in general, and studies that focus on various tools used for m-banking are increasingly important. In this chapter, I examined the nuances of m-banking adoption across two mobile devices: smartphones and smart wearable devices. I investigated the impact of the following factors: trust, perceived security, perceived value (time savings), and demographic variables. Demographic factors are important as numerous studies identify these (e.g., age, gender, income, and education) as key factors influencing m-banking adoption Chawla & Joshi (2018), lyInternetBankingAdoption2022e.\nThe results from this chapter are device-specific insights. This study also refines existing technology adoption models. Additionally, to the best of my knowledge, this is the first study to compare behavioral differences between smartphone and smart wearable users in the context of m-banking. I found that trust impacts smartphone users more strongly, while wearable users prioritize time efficiency. Users perceived smartphones as more secure. Demographic factors such as age, education, and gender exhibited varying influences based on device type as well.\n\n\n1.2 References"
  },
  {
    "objectID": "introduction.html#footnotes",
    "href": "introduction.html#footnotes",
    "title": "Introduction",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe words factor and construct are used interchangeably throughout most of m-banking adoption literature with no significant distinction between them Oyetade et al. (2020).↩︎",
    "crumbs": [
      "Home",
      "Introduction"
    ]
  },
  {
    "objectID": "introduction.html#the-relationship-between-mental-health-and-mobile-banking-adoption-evidence-from-canada",
    "href": "introduction.html#the-relationship-between-mental-health-and-mobile-banking-adoption-evidence-from-canada",
    "title": "Introduction",
    "section": "2 ### The Relationship Between Mental Health and Mobile Banking Adoption: Evidence from Canada",
    "text": "2 ### The Relationship Between Mental Health and Mobile Banking Adoption: Evidence from Canada\nFollowing the work of the previous study, I focused on intrinsic factors. One of the factors I found to be under-explored in the literature was mental health.\nPsychological factors are comparatively less-studied across the m-banking literature Venkatesh et al. (2012), Zou et al. (2023-05, 2023), Tiwari et al. (2021-12, 2021). I verified this further doing a quick preliminary search on Web-of-Science. Using the following search query returned 1,067 studies with no filters set on the results: !30(“mobile banking” OR “mbanking” OR “m-banking”) AND (“adoption”) When changing the search query to find studies specifically on psychological factors, the total number of studies returned were 157, which is about 14\\% of the total. The new search query was: !30\\parbox0.8 (“mobile banking” OR “mbanking” OR “m-banking”) AND \\ (“adoption”) AND (“affective” OR “psychological” OR “affect based” OR “affect-based” OR “emotional” OR “cognitive” OR “mental” OR “personal”)  \\ Using the same search queries in Scopus, 753 results were returned for the first, and 101 in the second search (13.4\\%). It is clear that only a small portion of the literature in m-banking adoption is focused on psychological factors.\nIn this study, I investigated the direct and moderated effect of mh on m-banking adoption. Moderators were extracted from theories in technology adoption or from literature related to mental health: rs, sd, and sns. I used a fixed effect logistic regression model grouped based on Canadian provinces following the cluster sampling design of my dataset. The results showed that mental health significantly and negatively affects m-banking adoption: better mental health outcomes were associated with lower likelihood of m-banking adoption. I did not find sufficient evidence for the moderating effects.\n%———————————————————————- Device Divide: Unpacking Mobile Banking Adoption Differences for Smartphones and Smart Wearable Devices -section-p3 %———————————————————————- Moving on from focusing only on smartphones, I decided to do a comparative study considering newer devices used in m-banking. One of the findings of my SLR study from Section -section-p2 was that few comparative studies exist in general, and studies that focus on various tools used for m-banking are increasingly important. In this chapter, I examined the nuances of m-banking adoption across two mobile devices: smartphones and smart wearable devices. I investigated the impact of the following factors: trust, perceived security, perceived value (time savings), and demographic variables. Demographic factors are important as numerous studies identify these (e.g., age, gender, income, and education) as key factors influencing m-banking adoption Chawla & Joshi (2018), lyInternetBankingAdoption2022e.\nThe results from this chapter are device-specific insights. This study also refines existing technology adoption models. Additionally, to the best of my knowledge, this is the first study to compare behavioral differences between smartphone and smart wearable users in the context of m-banking. I found that trust impacts smartphone users more strongly, while wearable users prioritize time efficiency. Users perceived smartphones as more secure. Demographic factors such as age, education, and gender exhibited varying influences based on device type as well.\n\n2.1 References"
  },
  {
    "objectID": "introduction.html#the-device-divide-unpacking-mobile-banking-adoption-differences-for-smartphones-and-smart-wearable-devices-labelintro-section-p3",
    "href": "introduction.html#the-device-divide-unpacking-mobile-banking-adoption-differences-for-smartphones-and-smart-wearable-devices-labelintro-section-p3",
    "title": "Introduction",
    "section": "1 The Device Divide: Unpacking Mobile Banking Adoption Differences for Smartphones and Smart Wearable Devices “labelintro-section-p3",
    "text": "1 The Device Divide: Unpacking Mobile Banking Adoption Differences for Smartphones and Smart Wearable Devices “labelintro-section-p3\n%———————————————————————- Moving on from focusing only on smartphones, I decided to do a comparative study considering newer devices used in m-banking. One of the findings of my SLR study from Section “refintro-section-p2 was that few comparative studies exist in general, and studies that focus on various tools used for m-banking are increasingly important. In this chapter, I examined the nuances of m-banking adoption across two mobile devices: smartphones and smart wearable devices. I investigated the impact of the following factors: trust, perceived security, perceived value (time savings), and demographic variables. Demographic factors are important as numerous studies identify these (e.g., age, gender, income, and education) as key factors influencing m-banking adoption Chawla & Joshi (2018), lyInternetBankingAdoption2022e.\nThe results from this chapter are device-specific insights. This study also refines existing technology adoption models. Additionally, to the best of my knowledge, this is the first study to compare behavioral differences between smartphone and smart wearable users in the context of m-banking. I found that trust impacts smartphone users more strongly, while wearable users prioritize time efficiency. Users perceived smartphones as more secure. Demographic factors such as age, education, and gender exhibited varying influences based on device type as well.\n\n1.1 References"
  },
  {
    "objectID": "study1.html",
    "href": "study1.html",
    "title": "The Inner Workings of Mobile Banking Adoption: A Systematic Literature Review of Intrinsic Factors",
    "section": "",
    "text": "This study is the second chapter of my Dissertation, “Essays on Mobile Banking Adoption”. I hope it provides a clear framework for categorizing the various factors that influence mobile banking (m-banking) adoption, which have lacked uniform definitions in previous research. Really, what even is perceived value of something? If depends on so many things and the context, and from problem to problem and person to person, it could be totally different! Digging deep into the literature, this was one thing that was really challenging and I only found other studies saying the same thing… but no one really addressed it. There’s also an over-reliance on a limited number of factors and theoretical frameworks in existing m-banking adoption literature. Like, we get it, technology acceptance model is very cool and strong! We know it works! Why are people still trying to show that it works?\nHere, in an attempt to offer some relief to researchers from the above issues, I introduce a novel categorization of adoption factors - breaking all of them into two main dimensions:\n\nIntrinsic\nExtrinsic\n\nDepending on how it relates to the decision maker. That basically means: is the decision you are making (about adoption of m-banking app) being influenced by factors (things) outside of your head that everyone else experiences (almost) the same as you do? Or is it more complicated than that, and it has to do with who you are, what you want, and how you perceive things? Those are external (to the user) and internal (to the user) factors. The focus of this study is on intrinsic factors, which relate to how individuals internally evaluate an m-banking application based on their perceptions (Perceptive Factors), goals and motivations (Personal Factors), social pressures (Social Factors), and emotions/mood (Psychological Factors). In contrast, extrinsic factors are measurable features of the app, like performance and functionality, which are generally easier to quantify and study - (almost) everyone experiences these things the same way.\nTo be even more detailed, I break intrinsic factors into further divisions - four main sub-categories:\n\nPerceptive (based on beliefs and perceptions),\nPersonal (based on individual motivations and traits),\nSocial (based on the impact of others),\nPsychological (based on cognitive, emotional, and mental processes).\n\nThis structured categorization is to clarify the meaning of some factors, and hopefully enable more focused research, as well as highlight overlooked patterns in the literature. To introduce these factors, I do a systematic literature review of m-banking adoption with these factors in mind. Here’s another novelty of this chapter: I used text-mining techniques — nothing fancy, just Latent Dirichlet Allocation (LDA) for topic modeling and a neat custom algorithm — to assign themes to collected scientific articles. Because of a colleagues comments, I also decided to apply some statistical techniques, to validate the findings empirically. I have a large enough dataset, so why not?\nThis should provide a robust foundation for future context-focused research, help identify gaps, and advocate for the exploration of under-studied theories and methods in m-banking adoption.\nI’ll briefly introduce the three main parts of this research. I didn’t have time to write these sections, but verbally explained to  what I wanted and only edited it a little bit after. It sounds too academic, but not that scary:\n\nTopic Modeling\nTopic modeling is a crucial step in analyzing text data to identify patterns and themes within a collection of articles. In this chapter, the process begins with extracting text from PDF files using PyMuPDF and then cleaning the text by converting it to lowercase, removing special characters, extra spaces, and stop words using NLTK. The cleaned text is then broken down into uni-grams (single words) and bi-grams (word pairs) through a process called tokenizing.\nTo identify the most important tokens, the Term Frequency-Inverse Document Frequency (TF-IDF) method is applied, followed by Latent Dirichlet Allocation (LDA) to discover latent topics within the corpus. LDA models documents as mixtures of latent topics, where each topic is represented by a distribution of words. The number of topics for the LDA algorithm is determined by evaluating models with a range of topics (|T|\\in[5,15]) and selecting the number that yields the highest coherence score, which indicates better model performance. For this study, eight topics were found to be optimal when using bi-grams.\nThe LDA-generated topics are then clarified and categorized. Tokens are grouped into categories such as Perceptive (perception-related), Psychological (cognition, emotions, mind), Personal (individual characteristics), Demographic (age, gender, income, education), Cultural (country, region, religion, culture), External (universally experienced, extrinsic factors), Market (technologies, devices), and Generic (unclassified or vague). Generic and external tokens are excluded from theme classification to maintain focus on intrinsic factors.\nFor a walk through of this part, read Topic Modeling.\n\n\nTheme Assignment\nFollowing topic modeling, each paper is assigned themes by the authors, with typically one to three themes per paper. To validate these assignments, a rule-based text mining approach using TF-IDF is employed. This method verifies theme assignments by counting the occurrences and impact of tokens within each article, allowing for up to three themes to be assigned per paper.\nThe algorithm works by using a pre-defined Python dictionary where each key represents a theme (e.g., “Perceptive,” “Psychological”) and its value is a list of factors associated with that theme. The algorithm calculates a score for each theme in an article based on the frequency of its associated factors. The themes corresponding to the top three highest scores are then assigned to the paper. If there is an overlap between author-assigned themes and algorithm-assigned themes, the matching theme with the highest weight is selected as the main theme. In cases where no themes match, the paper is re-reviewed for a decision. The analysis showed a high agreement rate of 90.2% between manual and rule-based assignments.\nFor a walk through of this part, read Theme Assignment.\n\n\nData Analysis\nThe dataset used for this chapter’s analysis comprises 143 articles published between 2018 and 2024, with significant variation in sample sizes across studies. Key information, such as factors of influence, research type, sample size, methods, and foundational theories, was manually extracted from each paper and stored in a CSV file.\nAn exploratory data analysis revealed several insights into the m-banking adoption literature. Most studies are quantitative (80), followed by empirical (27). The Technology Acceptance Model (TAM) and the Unified Theory of Acceptance and Use of Technology (UTAUT) are the most dominant theoretical frameworks, accounting for 70% of the literature, suggesting a potential over-reliance on these models. Structural Equation Modeling (SEM) and Partial Least Squares SEM (PLS-SEM) are the most common methodologies used.\nThe most frequently cited limitations in the studies include limited generalizability due to a focus on specific countries, sample-related issues (such as unrepresentative samples), and the lack of longitudinal studies. Perceived Usefulness, Perceived Ease of Use, and Trust are among the most frequently identified significant factors. The analysis also investigates non-significant factors and explores patterns in how these factors are reported across different study characteristics. Statistical analyses, such as chi-squared tests, are used to examine relationships between themes, technologies, research types, and other variables.\nFor a walk through of this part, read Data Analysis.",
    "crumbs": [
      "Home",
      "Projects",
      "Project 1. SLR"
    ]
  },
  {
    "objectID": "study1.html#part-1.-data-collection",
    "href": "study1.html#part-1.-data-collection",
    "title": "The Inner Workings of Mobile Banking Adoption: A Systematic Literature Review of Intrinsic Factors",
    "section": "Part 1. Data Collection",
    "text": "Part 1. Data Collection\nI downloaded the pdf of all the papers (143), reading them and extracting meta data based on the following:\n\nimport numpy as np \n\ndatabase = np.array([\n    {\n        'id': 'string', # unique identifier for the paper following convention P2_#number \n        'title': 'string', # title of the paper\n        'AffiliationCountry': 'string' , #name of country the study was conducted in,\n        'year': 2018-2024, # year of publication a value between 2018 and 2024\n        'journal': 'string', # name of the journal the paper was published in\n        'citations': 0-1000, # number of citations the paper has received - not reported in the paper \n        'year_since': 3, # number of years since publication - not reported in the paper \n        'cpy': 0, # number of citations per year - not reported in the paper \n        'keywords': ['TAM', 'mbanking', 'awareness'], # list of keywords, broken into K1-K10\n        'abstract': 'string', # abstract of the paper \n        'F': ['perceived usefulness'], # factors significant in the study, broken into F1-F9 \n        'FN': ['another factor'], # factors not significant in the study, broken into FNS1-FNS4 \n        'limit': ['geographical context'], # limitations of the study, broken into LIMIT1-LIMIT3 \n        'typeofResearch': 'string', # type of research conducted in the study \n        'methods': ['regression analysis'], # methods used in the study, broken into METHOD1-METHOD4\n        'theory': ['TAM'] # theories used in the study, broken into THEORY1-THEORY4\n        'sampleSize': 100, # sample size of the study \n        'tech': 'string', # main technology studied \n        'man_theme': 'string', # Theme manually assigned by me \n        'algo_theme': 'string', # Theme assigned by the algorithm \n        'decision_Theme': 'string', # Final theme of the paper  \n        'Score_Sig': 0.0, # % of significance for factors \n        'Score_NOT_Sig': 0.0, # % of non-significance for factors\n    }\n])\n\n\n\nIdea for future\n\n🤖 Build an Agentic AI application that automates this process.\n\n\nPart 1.1 Finding Out Themes\nFirst, install the following Python modules:\n\n%%capture \n!pip install nltk\n!pip install gensim\n!pip install itertools\n!pip install spacy\n!pip install langdetect\n!pip install pprint\n!pip install pyLDAvis\n!pip install textract\n!pip install spacy\n!pip install pymupdf\n!pip uninstall matplotlib seaborn -y\n!pip install matplotlib seaborn  \n!pip install --upgrade matplotlib seaborn\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n\n!pip install scipy==1.12.0 --quiet\n\nSince I am not familiar with Docker, I couldn’t resolve the package dependencies. This took so much time for me and I finally managed to fix it with this specific configuration. The imports look scary:\n\nimport string\nimport os \nimport re # regular expression \nimport pandas as pd\nimport numpy as np\n__requires__= 'scipy==1.12.0'\nimport scipy \nimport itertools\nimport textract # PDF text extraction \nimport math\nimport spacy\nimport fitz #PyMuPDF - another (better) PDF text extraction \n\n#NLP imports\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tokenize import sent_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.util import ngrams\nfrom nltk.tokenize import RegexpTokenizer\n# from nltk import pos_tag # didn't actually use it \n\n#SKLEARN\nfrom sklearn import metrics\nfrom sklearn import neighbors\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\n# from sklearn.decomposition import PCA\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\n# from sklearn.ensemble import RandomForestClassifier\n# from sklearn.metrics import classification_report\n# from sklearn.naive_bayes import MultinomialNB\n# from sklearn.neighbors import NearestNeighbors\n# from sklearn.preprocessing import LabelEncoder\n# from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n# from sklearn.naive_bayes import (\n# BernoulliNB,\n# ComplementNB,\n# MultinomialNB,\n# )\n#from sklearn.neighbors import KNeighborsClassifier\n# from sklearn.tree import DecisionTreeClassifier\n# from sklearn.ensemble import RandomForestClassifier\n# from sklearn.svm import SVC\n# from sklearn.linear_model import LogisticRegression\n# from sklearn.neural_network import MLPClassifier\n# from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.metrics.pairwise import cosine_similarity\n#from sklearn.decomposition import LatentDirichletAllocation\n\n#GENSIMimports\nimport gensim\nfrom gensim.models import Phrases\nfrom gensim.models.phrases import Phraser\nfrom gensim.corpora.dictionary import Dictionary\nfrom gensim.corpora import MmCorpus\nfrom gensim.models.tfidfmodel import TfidfModel\nfrom gensim.models import CoherenceModel\nfrom gensim.models import KeyedVectors\n\n#PyLDAvis imports for visualization of topic modeling results \n# import pyLDAvis\n# import pyLDAvis.gensim_models as gensimvis\n# import pyLDAvis.gensim\n# import pyLDAvis.gensim_models\n\n#MISC imports\nfrom collections import Counter\nfrom collections import defaultdict\nfrom string import punctuation\nfrom pprint import pprint\nfrom numpy import triu\n#from scipy.linalg.special_matrices import triu\nfrom scipy.sparse import csr_matrix\n\n#TRANSFORMERS\n#import torch\n#importtensorflowastf\n#from transformers import BertTokenizer, BertModel\n#from transformers import AutoTokenizer, AutoModel\n#fromtensorflow.keras.modelsimportSequential\n#fromtensorflow.keras.preprocessing.textimportTokenizer\n#fromtensorflow.keras.preprocessing.sequenceimportpad_sequences\n#fromtensorflow.keras.layersimportDense,Embedding,LSTM,SpatialDropout1D\n#fromtensorflow.keras.layersimportLeakyReLU\n\n#MATPLOT\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nDownload some of the language support stuff:\n\n# only run once\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('wordnet')\nnltk.download('punkt_tab')\nnltk.download('omw-1.4')  # Optional \n#nltk.download('averaged_perceptron_tagger')  # For POS tagging\n#nltk.download('averaged_perceptron_tagger_eng') # POS tagging \n\nI saved the pdf files’ name in a dictionary like this:\n\nname_of_pdfs = {\n    'p2_01': \"Lonkani et al_2020_A comparative study of trust in mobile banking.pdf\", \n    'p2_02': \"Saprikis et al_2022_A comparative study of users versus non-users' behavioral intention towards.pdf\", \n    'p2_03': \"Malaquias et al_2021_A cross-country study on intention to use mobile banking.pdf\", \n    'p2_04': \"Merhi et al_2019_A cross-cultural study of the intention to use mobile banking between Lebanese.pdf\", \n    'p2_05': \"Frimpong et al. - 2020 - A cross‐national investigation of trait antecedent.pdf\", \n    # and so on ... \n}\n\nAdditionally, I defined a dictionary “look up” for all the factors in the dataset with their related theme that looks like this (shortened for this presentation):\n\ntheme_of_words = {\n    'demographic': \n        list(set(['women', 'woman', 'female', 'men', 'man', 'male', 'sex', 'gender', 'age', 'income', \n            'demographic variables', 'elderly', 'education', 'gender differences', 'generation y', 'millennial generation',\n            'millennial', 'gen y', 'gen Z', 'gen alpha', 'gen X', 'boomer', 'babyboomer', 'generation X', 'generation z',\n            'young consumers', \n            # A lot more factors ...\n            ])),\n    \n    #----------------------------------------------------------------------------------------------------------------------------------\n    'cultural': \n        list(set(['developing countries','malaysia','transition country','pakistan',\n            'zakat','developing country','ghana','USA','srilanka', 'sri lanka',\n            'india','maldives','saudi-arabia','saudi arabia', 'nigeria','thailand','united states',\n            'yemen','citizenship','zimbabwe','palestine','culture',\n            'Country perspective', \n            # ... \n            ])),\n    \n    #----------------------------------------------------------------------------------------------------------------------------------\n    'psychological':\n        list(set(['anxiety','satisfaction','behavior','behaviour','attitudes','attitude','awareness',\n            'technology anxiety','consumer-behavior','trust','benv','consumer behaviour',\n            'covid-19 related psychological distress','psychological distress','psychological','distress',\n            'behavioral','computer anxiety','customer satisfaction', 'cognitive resistance',\n            # A LOT more ... \n            ]))\n            , \n            # ... few other key value pairs corresponding to themes \n\n}\n\nI also needed to delete some stop words, and decided to add more words that I knew would be frequently repeated. I also define the lemmer and stemmer.\n\nstop_words = stopwords.words('english')\nstop_words.extend([\"bank\", \"banking\", \"banks\", \n                   \"mobile\", \"mbank\", \"mbanking\", \"m-bank\", \"m bank\",\n                   \"online\", \"e\", \"e-bank\", \"ebank\", \"mobile banking\", \"mobile bank\", \n                   \"adoption\", \"acceptance\", \"accept\", \"theory\", \n                   \"purpose\", \"result\", \"method\", #from abstracts \n                   \"journal\", \"volume\", \"pp\", \"no\", \"doi\", \"http\", \"https\", \"et al\", \"issue\",\n                   \"technology\", \"internet\", \"information system\", \"international information\",\n                   \"information technology\", \"computer human\", \"mis quarterly\", \"electornic commerce\",\n                   \"j market\", \"telematics and informatics\", \"telematics informatics\", \"retail consumer\",\n                   \"international volume\", \"international business\", \"global information\",\n                   \"et\", \"al\", \"al.\", \"tam\", \"sem\", \"pls\", \"utaut\", \"tpb\",\n                   \".com\", \"management\", \"marketing\", \"published\", \"study\",\n                   \"research\", \"literature\", \"model\", #from journal information \n                   \"app\", \"application\", \"usage\"])\n\nstemmer = PorterStemmer()\nlemmatizer = WordNetLemmatizer()\n\n\nSet up.\nSo, I need a few functions as set up for cleaning the text. Function extract_text_from_pdf() is using PyMuPDF to extract text from a PDF file.\n\n#version one using PyMuPDF \ndef extract_text_from_pdf(filename):\n    text = \"\"\n    try:\n        doc = fitz.open(filename)\n        for page_num in range(doc.page_count):\n            page = doc.load_page(page_num)\n            text += page.get_text()\n    except Exception as e:\n        print(f\"Error reading {filename}: {e}\")\n    return text\n\nThis function is just one of the data cleaning functions:\n\ndef preprocess_Dict(dct):\n    for k, v in dct.items():\n        if isinstance(v, list):\n            processed_list = []\n            for item in v:\n                item = item.lower()\n                item = re.sub(r'http\\S+|www\\S+|@\\S+', '', item)\n                item = re.sub(r'(?&lt;=\\w)-(?=\\w)', ' ', item)\n                item = re.sub(r'[^a-z0-9\\s\\n]', '', item)\n                item = re.sub(r'\\s+', ' ', item).strip()\n                item = re.sub(r'\\d+', '', item).strip()\n\n                # replacing abbreviations \n                item = item.replace('structural equation model', 'sem')\n                item = item.replace('technology acceptance model', 'tam')\n                item = item.replace('unified theory of acceptance and use of technology', 'utaut')\n                item = item.replace('diffusion of innovation', 'doi')\n                item = item.replace('partial least squares', 'pls')\n                item = item.replace('theory of planned behavior', 'tpb')\n                item = re.sub(\"perceived usefulness\", \"pu\", item)\n                item = re.sub(\"perceived ease of use\", \"peou\", item)\n                item = re.sub(\"perceived privacy\", \"priv\", item)\n                item = re.sub(\"perceived aesthetics\", \"p_aest\", item)\n                item = re.sub(\"perceived relative advantage\", \"p_rel_adv\", item)\n                item = re.sub(\"perceived risk\", \"prisk\", item)\n                item = re.sub(\"perceived enjoyment\",\"penjy\", item)\n                item = re.sub(\"perceived intelligence\",\"pintlj\", item)\n                item = re.sub(\"perceived security\",\"psec\", item)\n                item = re.sub(\"perceived trust\",\"ptrst\", item)\n                item = re.sub(\"perceived anthropomorphism\",\"panthro\", item)\n                item = re.sub(\"perceived value\",\"pval\", item)\n                item = re.sub(\"perceived compatibility\",\"pcompat\", item)\n                item = re.sub(\"perceived detterants\",\"pdet\", item)\n                item = re.sub(\"perceived behavioral control\", \"p_bhv_ctrl\", item)\n                item = re.sub(\"perceived credibility\",\"pcred\", item)\n                item = re.sub(\"perceived cost\",\"pcost\", item)\n                item = re.sub(\"perceived benefit\",\"pbenef\", item)\n                item = re.sub(\"perceived convenience\",\"pconv\", item)\n                item = re.sub(\"perceived usability\",\"pusbl\", item)\n                item = re.sub(\"perceived privacy concerns\", \"ppriv_cn\", item)\n                \n                # belief base \n                item = re.sub(\"performance expectancy\", \"peex\", item)\n                item = re.sub(\"convenience\", \"conv\", item)\n                item = re.sub(\"effort expectancy\",\"efex\", item)\n                item = re.sub(\"access convenience\",\"acc_conv\", item)\n                item = re.sub(\"reliability\", \"rely\", item)\n                item = re.sub(\"behavioral control\", \"bhv_ctrl\", item)\n                item = re.sub(\"compatibility\", \"compat\", item)\n                item = re.sub(\"normative beliefs\", \"norm_blf\", item)\n                item = re.sub(\"normative belief\", \"norm_blf\", item)\n                item = re.sub(\"transaction convenience\",\"trans_conv\", item)\n                item = re.sub(\"post use trust\", \"post_trst\", item)\n                item = re.sub(\"post-use trust\", \"post_trst\", item)\n                item = re.sub(\"benefit convenience\",\"ben_conv\", item)\n                item = re.sub(\"search convenience\", \"srch_conv\", item)\n                item = re.sub(\"utilitarian expectation\", \"util_exp\", item)\n                item = re.sub(\"evaluation convenience\", \"eval_conv\", item)\n                item = re.sub(\"expectation\", \"expect\", item)\n                item = re.sub(\"possession convenience\", \"poss_conv\", item)\n                item = re.sub(\"expected advantage\", \"exp_adv\", item)\n               \n                # intention \n                item = re.sub(\"intention\", \"intnt\", item)\n                item = re.sub(\"motivation\", \"motiv\", item)\n                item = re.sub(\"automative motivation\",\"auto_motiv\", item)\n                item = re.sub(\"behavioral intention\",\"bhv_intnt\", item)\n                item = re.sub(\"control motivation\",\"ctrl_motiv\", item)\n                item = re.sub(\"controlled motivation\", \"ctrl_motiv\",  item)\n                item = re.sub(\"hedonic motivation\",\"hed_motiv\", item)\n                item = re.sub(\"intention to use\",\"intnt_use\", item)\n                \n                # personal \n                item = re.sub(\"habit\", \"habt\", item)\n                item = re.sub(\"personality\",\"prsnl\",item)\n                item = re.sub(\"personal factors\",\"prsnl\",item)\n                item = re.sub(\"personal factor\", \"prsnl\", item)\n                item = re.sub(\"digital literacy\",\"dig_lit\",item)\n                item = re.sub(\"digital capability\",\"dig_cabl\",item)\n                item = re.sub(\"agreeableness\",\"agrbns\",item)\n                item = re.sub(\"financial literacy\",\"fin_lit\",item)\n                item = re.sub(\"previous experience\",\"prv_exp\",item)\n                item = re.sub(\"life compatibility\",\"life_compat\",item)\n                item = re.sub(\"lifestyle\", \"life\", item)\n                item = re.sub(\"knowledge\",\"know\",item)\n                item = re.sub(\"functional value\",\"fun_val\",item)\n                item = re.sub(\"fun value\", \"fun_val\", item)\n                item = re.sub(\"utalitarian value\",\"util_val\",item)\n                item = re.sub(\"epistemic value\",\"epi_val\",item)\n                item = re.sub(\"monetary value\",\"mon_val\",item)\n                item = re.sub(\"money value\",\"mon_val\",item)\n                item = re.sub(\"hedonic value\", \"hed_val\", item)\n                item = re.sub(\"emotional value\",\"emo_val\",item)\n                item = re.sub(\"quality value\",\"qual_val\",item)\n                item = re.sub(\"value barriers\", \"val_bar\", item)\n                item = re.sub(\"value barrier\",\"val_bar\",item)\n                item = re.sub(\"customer experience about usability\",\"exp_use\",item)\n                item = re.sub(\"experience\",\"exp\",item)\n                item = re.sub(\"self employment\",\"semp\",item)\n                item = re.sub(\"self-employment\",\"semp\",item)\n                item = re.sub(\"valence\",\"val\",item)\n                item = re.sub(\"religiosity\",\"religis\",item)\n                item = re.sub(\"task technology fit\",\"ttf\",item)\n                item = re.sub(\"lifestyle fit\",\"life_fit\",item)\n                    \n                # social \n                item = re.sub(\"social interactions on platforms\",\"soc_int_plt\", item)\n                item = re.sub(\"coercive pressures\", \"coe_prsr\",  item)\n                item = re.sub(\"coercive pressure\", \"coe_prsr\",  item)\n                item = re.sub(\"human human interaction\",\"hh_int\", item)\n                item = re.sub(\"human-human interaction\",\"hh_int\", item)\n                item = re.sub(\"social influence\", \"socinf\", item)\n                item = re.sub(\"collectivist cultural practices\", \"colcul\",  item)\n                item = re.sub(\"collectivist cultural practice\",\"colcul\", item)\n                item = re.sub(\"social media influence\",\"snsinf\", item)\n                item = re.sub(\"normative belief\",\"norm_blf\", item)\n                item = re.sub(\"interaction\",\"interac\", item)\n                item = re.sub(\"subjective norm\", \"sbj_nrm\",  item)\n                item = re.sub(\"subjective norms\", \"sbj_nrm\",  item)\n                item = re.sub(\"social factors\", \"soc_fac\",  item)\n                item = re.sub(\"social factor\" ,\"soc_fac\" , item)\n                item = re.sub(\"normative pressure\",\"nrm_prsr\", item)\n                item = re.sub(\"CSR economical responsibility\",\"csr_econ\", item)\n                item = re.sub(\"social norms\", \"soc_nrm\",  item)\n                item = re.sub(\"family influence\",\"fam_inf\", item)\n                item = re.sub(\"people\",\"people\", item)\n                item = re.sub(\"herd\",\"herd\", item)\n                item = re.sub(\"CSR social responsibility\",\"csr_soc\", item)\n                item = re.sub(\"mimetic pressure\", \"mim_prsr\",  item)\n                item = re.sub(\"CSR environmental responsibility\", \"csr_env\",  item)\n                item = re.sub(\"social value\",\"soc_val\", item)\n                item = re.sub(\"social values\",\"soc_val\", item)\n                item = re.sub(\"employee customer engagement\",\"engg_empcus\", item)\n                item = re.sub(\"employee-customer engagement\",\"engg_empcus\", item)\n                item = re.sub(\"social isolation\", \"soc_iso\",  item)\n                item = re.sub(\"normative pressures\",\"norm_prsr\", item)\n                item = re.sub(\"social proof social media\", \"soc_prf_sns\",  item)\n                item = re.sub(\"social proof\", \"soc_prf\",  item)\n                item = re.sub(\"social media\",\"sns\", item)\n                item = re.sub(\"word of mouth\", \"wom\",  item)\n                item = re.sub(\"wom\",\"wom\", item)\n                item = re.sub(\"word-of-mouth\", \"wom\",  item)\n                item = re.sub(\"w-o-m\", \"wom\",  item)\n                item = re.sub(\"wordmouth\", \"wom\",  item)\n                item = re.sub(\"environment\", \"env\",  item)\n                \n                \n                # psychological \n                item = re.sub(\"computer self efficacy\",\"self\",item)\n                item = re.sub(\"self efficacy\",\"self\",item)\n                item = re.sub(\"self-efficacy\", \"self\", item)\n                item = re.sub(\"attitude\", \"attd\", item)\n                item = re.sub(\"attitudes\",\"attd\",item)\n                item = re.sub(\"trust\",\"trst\",item)\n                item = re.sub(\"pragmatic\",\"prgt\",item)\n                item = re.sub(\"security concern\", \"sec_cn\",item)\n                item = re.sub(\"security concerns\",\"sec_cn\",item)\n                item = re.sub(\"self-image\", \"self_cong\", item)\n                item = re.sub(\"self image\",\"self_cong\",item)\n                item = re.sub(\"congruence\", \"cong\", item)\n                item = re.sub(\"self-congruence\",\"self_cong\",item)\n                item = re.sub(\"self congruence\", \"self_cong\", item)\n                item = re.sub(\"self-image congruence\",\"self_cong\",item)\n                item = re.sub(\"self image congruence\", \"self_cong\", item)\n                item = re.sub(\"selfimage congruence\", \"self_cong\", item)\n                item = re.sub(\"awareness\", \"awar\", item)\n                item = re.sub(\"satisfaction\",\"satis\",item)\n                item = re.sub(\"consumer satisfaction\",\"satis\",item)\n                item = re.sub(\"customer satisfaction\",\"satis\",item)\n                item = re.sub(\"restiant to change\",\"resist_chng\",item)\n                item = re.sub(\"resistance to change\",\"resist_chng\",item)\n                item = re.sub(\"risk aversion\", \"risk_avrs\", item)\n                item = re.sub(\"risk averse\",\"risk_avrs\",item)\n                item = re.sub(\"novelty\",\"new_seek\",item)\n                item = re.sub(\"novelty-seeking\", \"new_seek\", item)\n                item = re.sub(\"novelty seeking\", \"new_seek\", item)\n                item = re.sub(\"consciousnesnness\", \"conscn\", item)\n                item = re.sub(\"post-use trust\", \"post_trst\", item)\n                item = re.sub(\"post use trust\",\"post_trst\",item)\n                item = re.sub(\"postuse trust\",\"post_trst\",item)\n                item = re.sub(\"emotional experience\",\"emo_exp\",item)\n                item = re.sub(\"agreeableness\",\"agrbns\",item)\n                item = re.sub(\"privacy concerns\",\"priv_cn\",item)\n                item = re.sub(\"privacy concern\",\"priv_cn\",item)\n                item = re.sub(\"cognitive decline\",\"cog_dec\",item)\n                item = re.sub(\"benevolent convenince\",\"ben_conv\",item)\n                item = re.sub(\"enjoyment\",\"enjy\",item)\n                item = re.sub(\"enjoy\", \"enjy\", item)\n                item = re.sub(\"hedonic motivation\",\"hed_motiv\",item)\n                item = re.sub(\"oppenness\", \"open\", item)\n                item = re.sub(\"loyal\", \"loyal\", item)\n                item = re.sub(\"loyalty\",\"loyal\",item)\n                item = re.sub(\"confirmation\",\"confrm\",item)\n                item = re.sub(\"optimism\",\"optim\",item)\n                item = re.sub(\"safety concerns\",\"safe_cn\",item)\n                item = re.sub(\"safety concern\",\"safe_cn\",item)\n                item = re.sub(\"Covid-19 psychological distress\", \"dist_covid\", item)\n                item = re.sub(\"Covid19 psychological distress\", \"dist_covid\", item)\n                item = re.sub(\"Covid 19 psychological distress\",\"dist_covid\",item)\n                item = re.sub(\"psychological distress\",\"dist_covid\",item)\n                item = re.sub(\"green concerns\",\"green_cn\",item)\n                item = re.sub(\"technology anxiety\", \"anxiety\", item)\n                item = re.sub(\"anxiety\",\"anxiety\",item)\n                item = re.sub(\"obedience\", \"obed\", item)\n                item = re.sub(\"empathy\",\"empath\",item)\n                item = re.sub(\"decision comfort\",\"dec_comfrt\",item)\n                item = re.sub(\"confidence\",\"confdnc\",item)\n                item = re.sub(\"decision discomfort\", \"dec_dis_comfrt\", item)\n                item = re.sub(\"comfort\", \"cmfrt\", item)\n                item = re.sub(\"discomfort\",\"discmfrt\",item)\n                item = re.sub(\"insecurity\", \"insec\", item)\n                item = re.sub(\"insecurities\", \"insec\", item)\n                item = re.sub(\"benevolence\",\"benv\",item)\n                item = re.sub(\"technology stress\",\"tech_strss\",item)\n                item = re.sub(\"stress\", \"tech_strss\", item)\n                item = re.sub(\"techno-stress\",\"tech_strss\",item)\n                item = re.sub(\"technostress\",\"tech_strss\",item)\n                item = re.sub(\"techno stress\", \"tech_strss\", item)\n                item = re.sub(\"cognitive resistence\",\"cog_resist\",item)\n                        \n                # demographic \n                item = re.sub(\"age\",\"age\",item)\n                item = re.sub(\"sex\",\"sex\",item)\n                item = re.sub(\"education\",\"edu\",item)\n                item = re.sub(\"income\",\"income\",item)\n                item = re.sub(\"islamic religiosity\",\"religios\",item)\n                item = re.sub(\"culture\",\"cltr\",item)\n                \n                item = \" \".join([word for word in item.split() if word not in stop_words])\n                item = \" \".join([lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in item.split()])\n                #item = \" \".join([stemmer.stem(word) for word in item.split()])\n                processed_list.append(item)\n            dct[k] = \" \".join(processed_list)\n        else:\n            v = v.lower()\n            v = re.sub(r'http\\S+|www\\S+|@\\S+', '', v)\n            v = re.sub(r'(?&lt;=\\w)-(?=\\w)', ' ', v)\n            v = re.sub(r'[^a-z0-9\\s\\n]', '', v)\n            v = re.sub(r'\\s+', ' ', v).strip()\n            v = re.sub(r'\\d+', '', v).strip()\n\n            # replacing abbreviations \n            v = v.replace('structural equation model', 'sem')\n            v = v.replace('technology acceptance model', 'tam')\n            v = v.replace('unified theory of acceptance and use of technology', 'utaut')\n            v = v.replace('diffusion of innovation', 'doi')\n            v = v.replace('partial least squares', 'pls')\n            v = v.replace('theory of planned behavior', 'tpb')\n            v = re.sub(\"perceived usefulness\", \"pu\", v)\n            v = re.sub(\"perceived ease of use\", \"peou\", v)\n            v = re.sub(\"perceived privacy\", \"priv\", v)\n            v = re.sub(\"perceived aesthetics\", \"p_aest\", v)\n            v = re.sub(\"perceived relative advantage\", \"p_rel_adv\", v)\n            v = re.sub(\"perceived risk\", \"prisk\", v)\n            v = re.sub(\"perceived enjoyment\",\"penjy\", v)\n            v = re.sub(\"perceived intelligence\",\"pintlj\", v)\n            v = re.sub(\"perceived security\",\"psec\", v)\n            v = re.sub(\"perceived trust\",\"ptrst\", v)\n            v = re.sub(\"perceived anthropomorphism\",\"panthro\", v)\n            v = re.sub(\"perceived value\",\"pval\", v)\n            v = re.sub(\"perceived compatibility\",\"pcompat\", v)\n            v = re.sub(\"perceived detterants\",\"pdet\", v)\n            v = re.sub(\"perceived behavioral control\", \"p_bhv_ctrl\", v)\n            v = re.sub(\"perceived credibility\",\"pcred\", v)\n            v = re.sub(\"perceived cost\",\"pcost\", v)\n            v = re.sub(\"perceived benefit\",\"pbenef\", v)\n            v = re.sub(\"perceived convenience\",\"pconv\", v)\n            v = re.sub(\"perceived usability\",\"pusbl\", v)\n            v = re.sub(\"perceived privacy concerns\", \"ppriv_cn\", v)\n                \n            v = re.sub(\"performance expectancy\", \"peex\", v)\n            v = re.sub(\"convenience\", \"conv\", v)\n            v = re.sub(\"effort expectancy\",\"efex\", v)\n            v = re.sub(\"access convenience\",\"acc_conv\", v)\n            v = re.sub(\"reliability\", \"rely\", v)\n            v = re.sub(\"behavioral control\", \"bhv_ctrl\", v)\n            v = re.sub(\"compatibility\", \"compat\", v)\n            v = re.sub(\"normative beliefs\", \"norm_blf\", v)\n            v = re.sub(\"normative belief\", \"norm_blf\", v)\n            v = re.sub(\"transaction convenience\",\"trans_conv\", v)\n            v = re.sub(\"post use trust\", \"post_trst\", v)\n            v = re.sub(\"post-use trust\", \"post_trst\", v)\n            v = re.sub(\"benefit convenience\",\"ben_conv\", v)\n            v = re.sub(\"search convenience\", \"srch_conv\", v)\n            v = re.sub(\"utilitarian expectation\", \"util_exp\", v)\n            v = re.sub(\"evaluation convenience\", \"eval_conv\", v)\n            v = re.sub(\"expectation\", \"expect\", v)\n            v = re.sub(\"possession convenience\", \"poss_conv\", v)\n            v = re.sub(\"expected advantage\", \"exp_adv\", v)\n            \n            # intention \n            v = re.sub(\"intention\", \"intnt\", v)\n            v = re.sub(\"motivation\", \"motiv\", v)\n            v = re.sub(\"automative motivation\",\"auto_motiv\", v)\n            v = re.sub(\"behavioral intention\",\"bhv_intnt\", v)\n            v = re.sub(\"control motivation\",\"ctrl_motiv\", v)\n            v = re.sub(\"controlled motivation\", \"ctrl_motiv\",  v)\n            v = re.sub(\"hedonic motivation\",\"hed_motiv\", v)\n            v = re.sub(\"intention to use\",\"intnt_use\", v)\n                \n                # personal \n            v = re.sub(\"habit\", \"habt\", v)\n            v = re.sub(\"personality\",\"prsnl\",v)\n            v = re.sub(\"personal factors\",\"prsnl\",v)\n            v = re.sub(\"personal factor\", \"prsnl\", v)\n            v = re.sub(\"digital literacy\",\"dig_lit\",v)\n            v = re.sub(\"digital capability\",\"dig_cabl\",v)\n            v = re.sub(\"agreeableness\",\"agrbns\",v)\n            v = re.sub(\"financial literacy\",\"fin_lit\",v)\n            v = re.sub(\"previous experience\",\"prv_exp\",v)\n            v = re.sub(\"life compatibility\",\"life_compat\",v)\n            v = re.sub(\"lifestyle\", \"life\", v)\n            v = re.sub(\"knowledge\",\"know\",v)\n            v = re.sub(\"functional value\",\"fun_val\",v)\n            v = re.sub(\"fun value\", \"fun_val\", v)\n            v = re.sub(\"utalitarian value\",\"util_val\",v)\n            v = re.sub(\"epistemic value\",\"epi_val\",v)\n            v = re.sub(\"monetary value\",\"mon_val\",v)\n            v = re.sub(\"money value\",\"mon_val\",v)\n            v = re.sub(\"hedonic value\", \"hed_val\", v)\n            v = re.sub(\"emotional value\",\"emo_val\",v)\n            v = re.sub(\"quality value\",\"qual_val\",v)\n            v = re.sub(\"value barriers\", \"val_bar\", v)\n            v = re.sub(\"value barrier\",\"val_bar\",v)\n            v = re.sub(\"customer experience about usability\",\"exp_use\",v)\n            v = re.sub(\"experience\",\"exp\",v)\n            v = re.sub(\"self employment\",\"semp\",v)\n            v = re.sub(\"self-employment\",\"semp\",v)\n            v = re.sub(\"valence\",\"val\",v)\n            v = re.sub(\"religiosity\",\"religis\",v)\n            v = re.sub(\"task technology fit\",\"ttf\",v)\n            v = re.sub(\"lifestyle fit\",\"life_fit\",v)\n                    \n                # social \n            v = re.sub(\"social interactions on platforms\",\"soc_int_plt\", v)\n            v = re.sub(\"coercive pressures\", \"coe_prsr\",  v)\n            v = re.sub(\"coercive pressure\", \"coe_prsr\",  v)\n            v = re.sub(\"human human interaction\",\"hh_int\", v)\n            v = re.sub(\"human-human interaction\",\"hh_int\", v)\n            v = re.sub(\"social influence\", \"socinf\", v)\n            v = re.sub(\"collectivist cultural practices\", \"colcul\",  v)\n            v = re.sub(\"collectivist cultural practice\",\"colcul\", v)\n            v = re.sub(\"social media influence\",\"snsinf\", v)\n            v = re.sub(\"normative belief\",\"norm_blf\", v)\n            v = re.sub(\"interaction\",\"interac\", v)\n            v = re.sub(\"subjective norm\", \"sbj_nrm\",  v)\n            v = re.sub(\"subjective norms\", \"sbj_nrm\",  v)\n            v = re.sub(\"social factors\", \"soc_fac\",  v)\n            v = re.sub(\"social factor\" ,\"soc_fac\" , v)\n            v = re.sub(\"normative pressure\",\"nrm_prsr\", v)\n            v = re.sub(\"CSR economical responsibility\",\"csr_econ\", v)\n            v = re.sub(\"social norms\", \"soc_nrm\",  v)\n            v = re.sub(\"family influence\",\"fam_inf\", v)\n            v = re.sub(\"people\",\"people\", v)\n            v = re.sub(\"herd\",\"herd\", v)\n            v = re.sub(\"CSR social responsibility\",\"csr_soc\", v)\n            v = re.sub(\"mimetic pressure\", \"mim_prsr\",  v)\n            v = re.sub(\"CSR environmental responsibility\", \"csr_env\",  v)\n            v = re.sub(\"social value\",\"soc_val\", v)\n            v = re.sub(\"social values\",\"soc_val\", v)\n            v = re.sub(\"employee customer engagement\",\"engg_empcus\", v)\n            v = re.sub(\"employee-customer engagement\",\"engg_empcus\", v)\n            v = re.sub(\"social isolation\", \"soc_iso\",  v)\n            v = re.sub(\"normative pressures\",\"norm_prsr\", v)\n            v = re.sub(\"social proof social media\", \"soc_prf_sns\",  v)\n            v = re.sub(\"social proof\", \"soc_prf\",  v)\n            v = re.sub(\"social media\",\"sns\", v)\n            v = re.sub(\"word of mouth\", \"wom\",  v)\n            v = re.sub(\"wom\",\"wom\", v)\n            v = re.sub(\"word-of-mouth\", \"wom\",  v)\n            v = re.sub(\"w-o-m\", \"wom\",  v)\n            v = re.sub(\"wordmouth\", \"wom\",  v)\n            v = re.sub(\"environment\", \"env\",  v)\n                \n                \n                # psychological \n            v = re.sub(\"computer self efficacy\",\"self\",v)\n            v = re.sub(\"self efficacy\",\"self\",v)\n            v = re.sub(\"self-efficacy\", \"self\", v)\n            v = re.sub(\"attitude\", \"attd\", v)\n            v = re.sub(\"attitudes\",\"attd\",v)\n            v = re.sub(\"trust\",\"trst\",v)\n            v = re.sub(\"pragmatic\",\"prgt\",v)\n            v = re.sub(\"security concern\", \"sec_cn\",v)\n            v = re.sub(\"security concerns\",\"sec_cn\",v)\n            v = re.sub(\"self-image\", \"self_cong\", v)\n            v = re.sub(\"self image\",\"self_cong\",v)\n            v = re.sub(\"congruence\", \"cong\", v)\n            v = re.sub(\"self-congruence\",\"self_cong\",v)\n            v = re.sub(\"self congruence\", \"self_cong\", v)\n            v = re.sub(\"self-image congruence\",\"self_cong\",v)\n            v = re.sub(\"self image congruence\", \"self_cong\", v)\n            v = re.sub(\"selfimage congruence\", \"self_cong\", v)\n            v = re.sub(\"awareness\", \"awar\", v)\n            v = re.sub(\"satisfaction\",\"satis\",v)\n            v = re.sub(\"consumer satisfaction\",\"satis\",v)\n            v = re.sub(\"customer satisfaction\",\"satis\",v)\n            v = re.sub(\"restiant to change\",\"resist_chng\",v)\n            v = re.sub(\"resistance to change\",\"resist_chng\",v)\n            v = re.sub(\"risk aversion\", \"risk_avrs\", v)\n            v = re.sub(\"risk averse\",\"risk_avrs\",v)\n            v = re.sub(\"novelty\",\"new_seek\",v)\n            v = re.sub(\"novelty-seeking\", \"new_seek\", v)\n            v = re.sub(\"novelty seeking\", \"new_seek\", v)\n            v = re.sub(\"consciousnesnness\", \"conscn\", v)\n            v = re.sub(\"post-use trust\", \"post_trst\", v)\n            v = re.sub(\"post use trust\",\"post_trst\",v)\n            v = re.sub(\"postuse trust\",\"post_trst\",v)\n            v = re.sub(\"emotional experience\",\"emo_exp\",v)\n            v = re.sub(\"agreeableness\",\"agrbns\",v)\n            v = re.sub(\"privacy concerns\",\"priv_cn\",v)\n            v = re.sub(\"privacy concern\",\"priv_cn\",v)\n            v = re.sub(\"cognitive decline\",\"cog_dec\",v)\n            v = re.sub(\"benevolent convenince\",\"ben_conv\",v)\n            v = re.sub(\"enjoyment\",\"enjy\",v)\n            v = re.sub(\"enjoy\", \"enjy\", v)\n            v = re.sub(\"hedonic motivation\",\"hed_motiv\",v)\n            v = re.sub(\"oppenness\", \"open\", v)\n            v = re.sub(\"loyal\", \"loyal\", v)\n            v = re.sub(\"loyalty\",\"loyal\",v)\n            v = re.sub(\"confirmation\",\"confrm\",v)\n            v = re.sub(\"optimism\",\"optim\",v)\n            v = re.sub(\"safety concerns\",\"safe_cn\",v)\n            v = re.sub(\"safety concern\",\"safe_cn\",v)\n            v = re.sub(\"Covid-19 psychological distress\", \"dist_covid\", v)\n            v = re.sub(\"Covid19 psychological distress\", \"dist_covid\", v)\n            v = re.sub(\"Covid 19 psychological distress\",\"dist_covid\",v)\n            v = re.sub(\"psychological distress\",\"dist_covid\",v)\n            v = re.sub(\"green concerns\",\"green_cn\",v)\n            v = re.sub(\"technology anxiety\", \"anxiety\", v)\n            v = re.sub(\"anxiety\",\"anxiety\",v)\n            v = re.sub(\"obedience\", \"obed\", v)\n            v = re.sub(\"empathy\",\"empath\",v)\n            v = re.sub(\"decision comfort\",\"dec_comfrt\",v)\n            v = re.sub(\"confidence\",\"confdnc\",v)\n            v = re.sub(\"decision discomfort\", \"dec_dis_comfrt\", v)\n            v = re.sub(\"comfort\", \"cmfrt\", v)\n            v = re.sub(\"discomfort\",\"discmfrt\",v)\n            v = re.sub(\"insecurity\", \"insec\", v)\n            v = re.sub(\"insecurities\", \"insec\", v)\n            v = re.sub(\"benevolence\",\"benv\",v)\n            v = re.sub(\"technology stress\",\"tech_strss\",v)\n            v = re.sub(\"stress\", \"tech_strss\", v)\n            v = re.sub(\"techno-stress\",\"tech_strss\",v)\n            v = re.sub(\"technostress\",\"tech_strss\",v)\n            v = re.sub(\"techno stress\", \"tech_strss\", v)\n            v = re.sub(\"cognitive resistence\",\"cog_resist\",v)\n                    \n            # demographic \n            v = re.sub(\"age\",\"age\",v)\n            v = re.sub(\"sex\",\"sex\",v)\n            v = re.sub(\"education\",\"edu\",v)\n            v = re.sub(\"income\",\"income\",v)\n            v = re.sub(\"islamic religiosity\",\"religios\",v)\n            v = re.sub(\"culture\",\"cltr\",v)\n                \n            v = \" \".join([word for word in v.split() if word not in stop_words])\n            v = \" \".join([lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in v.split()])\n            #v = \" \".join([stemmer.stem(word) for word in v.split()])\n            dct[k] = v\n    return dct\n\ndef tokenizeToSentences(doc):\n    for k, v in doc.items():\n        \n        if isinstance(v, bytes):\n            v = v.decode('utf-8')\n          \n        v = v.lower()\n        v = v.replace('\\n', ' ')\n        v = re.sub(r'http\\S+www\\S+@\\S+', '', v)\n        #v = \" \".join([str(s) for s in v])\n\n        v = sent_tokenize(v)\n        doc[k] = v\n        \n    return doc\n\nFor Topic modeling, I write a function to generate dictionaries and save them in a .mm file format.\n\ndef generate_dictionary(text, name):\n    \"\"\" \n    As input takes in the text to build the dictionary for and the name of a .mm file\n    \"\"\" \n    \n    dictionary = Dictionary(text)\n    \n    corpus = [dictionary.doc2bow(review) for review in text] \n    \n    filename = f\"{name}.mm\"\n    \n    MmCorpus.serialize(filename, corpus)\n\n    return dictionary, corpus\n\nAdditionally, I want a function that prints the top 50 most frequently appearing words in the corpus:\n\n# ---------------------- START OF CHATGPT CODE\ndef print_top_50_words(corpus, dictionary):\n    total_word_count = defaultdict(int)\n    word_weights = defaultdict(float)\n\n    for word_id, word_count in itertools.chain.from_iterable(corpus):\n        total_word_count[word_id] += word_count\n\n    sorted_tota_words_count = sorted(total_word_count.items(), key = lambda w: w[1], reverse = True)\n\n    tfidf = TfidfModel(corpus)\n\n    for doc in corpus:\n        tfidf_weights = tfidf[doc]  # Calculate TF-IDF for the review\n        for term_id, weight in tfidf_weights:\n            word_weights[term_id] += weight  # Aggregate the weight for the term\n\n    sorted_word_weights = sorted(word_weights.items(), key=lambda x: x[1], reverse=True)\n\n    # Print the top 50 terms with their weights\n    top_50_words = [(dictionary.get(term_id), weight) for term_id, weight in sorted_word_weights[:50]]\n\n    for word, weight in top_50_words:\n        print(word, weight)\n\n# ---------------------- END OF CHATGPT CODE \n\nI also plan on seeing how python clusters the words (as in, finds similar words) vs me:\n\ndef print_clusters(n_clusters, list_of_words):\n    clusters = {i: [] for i in range(n_clusters)}\n    for word, label in zip(list_of_words, labels):\n        clusters[label].append(word)\n\n    for label, words in clusters.items():\n        print(f\"Cluster {label}:\")\n        for word in words:\n            print(f\"  {word}\")\n        print(\"\\n\")\n\n    # Explain clusters\n    print(\"Cluster explanations based on semantics and ideas:\")\n    for label, words in clusters.items():\n        print(f\"Cluster {label} might be related to:\")\n        for word in words:\n            print(f\"  {word}\")\n        print(\"\\n\")\n\nThis is a function for if you want to use a word embedding (requires some effort, time and machine power!):\n\ndef get_embedding(text):\n    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n    model_bert = BertModel.from_pretrained('bert-base-uncased')\n    \n    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=20)\n    with torch.no_grad():\n        outputs = model_bert(**inputs)\n    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n\nAnd then you use this to get semantically similar words:\n\ndef get_semantically_similar_words(words, threshold=0.7):\n    similar_words = set(words)\n    for word in words:\n        token = nlp(word)\n        for vocab_word in nlp.vocab:\n            if vocab_word.has_vector and vocab_word.is_alpha:\n                similarity = token.similarity(nlp(vocab_word.text))\n                if similarity &gt;= threshold:\n                    similar_words.add(vocab_word.text)\n    return similar_words\n\nSo, how do I find the themes? Essentially, I just tweaked TF-IDF:\n\nclass CustomTfidfVectorizer(TfidfVectorizer):\n    def __init__(self, vocabulary=None, **kwargs):\n        super().__init__(vocabulary=vocabulary, **kwargs)\n        #self.general_keywords = set(general_keywords)\n        \n    def build_analyzer(self):\n        analyzer = super().build_analyzer()\n        return lambda doc: [w for w in analyzer(doc)] #if w not in self.general_keywords]\n    \n    def fit(self, raw_documents, y=None):\n        self.fit_transform(raw_documents, y)\n        return self\n\n    def fit_transform(self, raw_documents, y=None):\n        X = super().fit_transform(raw_documents, y)\n        self.max_frequencies = self._compute_max_frequencies(X, raw_documents)\n        return X\n\n    def transform(self, raw_documents):\n        X = super().transform(raw_documents)\n\n        # Calculate augmented term frequency\n        max_frequencies = self.max_frequencies\n        max_frequencies[max_frequencies == 0] = 1  # Avoid division by zero\n        augmented_tf = 0.5 + 0.5 * (X.toarray() / max_frequencies[:, None])\n        \n        # Penalize general keywords\n        #penalized_idf = self.idf_ * (1 - 0.8 * np.isin(self.get_feature_names_out(), list(self.general_keywords)))\n        \n        # Apply penalized IDF\n        augmented_tfidf = augmented_tf * penalized_idf\n\n        return csr_matrix(augmented_tfidf)\n\n    def _compute_max_frequencies(self, X, raw_documents):\n        max_frequencies = np.zeros(X.shape[0])\n        for i, doc in enumerate(raw_documents):\n            term_freq = {}\n            for term in doc.split():\n                if term in term_freq:\n                    term_freq[term] += 1\n                else:\n                    term_freq[term] = 1\n            max_frequencies[i] = max(term_freq.values())\n        return max_frequencies\n\n\n\nKeyword Analytics\n\ntry:\n    df = pd.read_csv(\"P2_AR_04.csv\", encoding='utf-8')\nexcept UnicodeDecodeError:\n    try:\n        df = pd.read_csv(\"P2_AR_04.csv\", encoding='latin-1')\n    except Exception as e:\n        error_message = str(e)\n        df = None\n\nClean all the data you’ve gathered the same way the PDF’s have been cleaned (the preprocess_text() function looks very similar to the cleaning function above!):\n\ndf2 = df.copy()\n\ncolumns_to_preprocess = ['Man_Theme',\n                        'K1','K2','K3','K4','K5','K6','K7','K8','K9','K10',\n                        'F1','F2','F3','F4','F5','F6','F7','F8','F9',\n                        'FNS1','FNS2','FNS3','FNS4',\n                        'METHOD1','METHOD2','METHOD3','METHOD4',\n                        'THEORY1','THEORY2','THEORY3','THEORY4',\n                        'LIMIT1' ,'LIMIT2' ,'LIMIT3', 'Abstract'\n                        ]\n\nfor col in columns_to_preprocess:\n    df2[col] = df2[col].apply(preprocess_text)\n\n\npapers = {}\n\nfor paper_id, filename in name_of_pdfs.items():\n    text = extract_text_from_pdf(filename)\n    papers[paper_id] = text\n\npapers_df = pd.DataFrame.from_dict(papers, orient = 'index', columns = ['paperText'])\npapers_df = papers_df.reset_index(names = ['paperID'])\npapers_df.to_csv('papers_unclean.csv')\npapers_df.head()\n\nThese look like this: \n\n# keep a copy (a habit of mine)\npapers_uncleaned = papers.copy() \ntheme_of_words_uncleaned = theme_of_words.copy() \n\n# clean up all papers \npapers_cleaned = preprocess_Dict(papers)\npapersClean_df = pd.DataFrame.from_dict(papers_cleaned, orient = 'index', columns = ['paperText'])\npapersClean_df = papersClean_df.reset_index(names = ['paperID'])\npapersClean_df.to_csv('papers_clean3.csv')\npapersClean_df.head()\n\n\nClean the theme of words dictionary so the words match:\n\ntheme_of_words_cleaned = {}\n\nfor k, v in theme_of_words.items():\n    theme_of_words_cleaned[k] = preprocess_list(v)\n\ntheme_of_words_cleaned['psychological'][:10]\n\n\n\n\n[‘initial trst’,‘simcong’,‘hedmotiv’,‘selfefficacy’,‘strss’,‘controlled motiv’,‘cong’,‘selfcong cong’,‘trst systems’,‘custom loyalti’]\n\n\n\nMake sure to drop all NA’s and empty values:\n\nfor k, v in theme_of_words_cleaned.items():\n    theme_of_words_cleaned[k] = [x for x in v if x not in [None, \"\", ' ', 'NaN'] and not (isinstance(x, float) and math.isnan(x))]\n\n\n\nThemes Based on Count of Words in Each Group\nI will skip the parts on BERT and word embeddings that’s in the jupyter notebook as these were not used for my project. The reason is that it was taking so long and my computer simply did not have the capacity to handle it. I also didn’t have the time to find a fix for it, but there are a bunch of commented-out code from chatGPT that I was playing around with.\nSo, we now have a dictionary with the factors and their theme, and a corpus of text of all papers in the dataset. What I’m going to do here is: * Count all the words total_words in each paper * Count the instances of each word (factor) in theme_of_words_cleaned in each paper * Each word’s count adds 1 point to its corresponding theme’s “weight score” * Take for example the word emotion and I said the theme for this word is psychological. If emotion shows up 10 times in paper 1, paper 1’s dictionary of weights has a weight of 10 for (divided by the total number of words) psychological.\n\nresults = []\ncount_words_df = pd.DataFrame(results)\n\n\nfor doc_id, text in papers_cleaned.items():\n    doc = nlp(text)\n    word_counts = defaultdict(int)\n\n    for token in doc:\n        for group, keywords in theme_of_words_cleaned.items():\n            if token.text.lower() in keywords:\n                word_counts[group] += 1\n\n    total_words = len(doc)\n    group_weights = {f\"{group}_w\": count / total_words for group, count in word_counts.items()}\n    max_weight = max(group_weights.values(), default=0)\n    theme = max(group_weights, key=group_weights.get).replace(\"_w\", \"\") if max_weight &gt; 0 else None\n\n    result = {\"doc_id\": doc_id, **word_counts, **group_weights, \"theme\": theme, \"max_weight\": max_weight}\n    results.append(result)\n\n\nfor group in theme_of_words_cleaned.keys():\n    if group not in count_words_df.columns:\n        count_words_df[group] = 0\n    if f\"{group}_w\" not in count_words_df.columns:\n        count_words_df[f\"{group}_w\"] = 0.0\n\ncount_words_df.head()\n\n\nThis is still very basic, though because it’s only based on the factors which may not be fully representative. So, I do this again using keywords in addition to the factors. These are keywords that were selected by the authors as well as information extracted from Web of Science \\BibTeX file.\n\ncols_toPick = ['K1','K2','K3','K4','K5','K6','K7','K8','K9','K10','F1','F2','F3','F4','F5','F6','F7','F8','F9']\n\nkeywordsDf = df2.loc[:,cols_toPick]\n\n# flatten the dataframe to a list \nkeywords_across_db = keywordsDf.values.flatten().tolist()\n\n# there are 2,717 words here \nprint(\"number of words (factors and keywords) in total \", len(keywords_across_db))\n\n# making sure there are no empty/NaN/Null values \nkeywords_across_db = [x for x in keywords_across_db if x not in [None, \"\", ' ', 'NaN'] and not (isinstance(x, float) and math.isnan(x))]\n\n# making sure there are no duplicates (set takes care of this)\nkeywords_across_db_nodup_cleaned = list(set(keywords_across_db))\n\n# convert the list into a dictionary temporarily, then convert it to a dataframe \ntemp = {'Keyword': keywords_across_db_nodup_cleaned}\nkeywords_themes_df = pd.DataFrame(temp, columns=['Keyword'])\n\n# go back to the theme_of_words_cleaned and find each keyword's theme \nkeywords_themes_dic = {keyword: theme for theme, keywords in theme_of_words_cleaned.items() for keyword in keywords}\n\n# This is just a dataframe view of the keywords with their respective theme \nkeywords_themes_df['Theme'] = keywords_themes_df['Keyword'].map(keywords_themes_dic)\n\n# if the theme is empty, give it \"Generic\" - that means these keywords weren't in the list of important words that we picked themes for \nkeywords_themes_df['Theme'] = keywords_themes_df['Theme'].apply(lambda x: 'Generic' if pd.isna(x) or x == ' ' else x)\n\nkeywords_themes_df.head()",
    "crumbs": [
      "Home",
      "Projects",
      "Project 1. SLR"
    ]
  },
  {
    "objectID": "study1.html#part-0.-jupyter-notebook",
    "href": "study1.html#part-0.-jupyter-notebook",
    "title": "The Inner Workings of Mobile Banking Adoption: A Systematic Literature Review of Intrinsic Factors",
    "section": "",
    "text": "If you want to run the entire code, use the Jupyter notebook on my github page.",
    "crumbs": [
      "Home",
      "Projects",
      "Project 1. SLR"
    ]
  },
  {
    "objectID": "study1_theme.html",
    "href": "study1_theme.html",
    "title": "Algorithmic Approach to Finding Themes",
    "section": "",
    "text": "If you want to run the entire code, use the Jupyter notebook on my github page.",
    "crumbs": [
      "Home",
      "Projects",
      "Project 1. SLR",
      "Algorithmic Approach to Finding Themes"
    ]
  },
  {
    "objectID": "study1_theme.html#part-0.-jupyter-notebook",
    "href": "study1_theme.html#part-0.-jupyter-notebook",
    "title": "Algorithmic Approach to Finding Themes",
    "section": "",
    "text": "If you want to run the entire code, use the Jupyter notebook on my github page.",
    "crumbs": [
      "Home",
      "Projects",
      "Project 1. SLR",
      "Algorithmic Approach to Finding Themes"
    ]
  },
  {
    "objectID": "study1_theme.html#part-1.-data-collection",
    "href": "study1_theme.html#part-1.-data-collection",
    "title": "Algorithmic Approach to Finding Themes",
    "section": "Part 1. Data Collection",
    "text": "Part 1. Data Collection\nI downloaded the pdf of all the papers (143), reading them and extracting meta data based on the following:\n\nimport numpy as np \n\ndatabase = np.array([\n    {\n        'id': 'string', # unique identifier for the paper following convention P2_#number \n        'title': 'string', # title of the paper\n        'AffiliationCountry': 'string' , #name of country the study was conducted in,\n        'year': 2018-2024, # year of publication a value between 2018 and 2024\n        'journal': 'string', # name of the journal the paper was published in\n        'citations': 0-1000, # number of citations the paper has received - not reported in the paper \n        'year_since': 3, # number of years since publication - not reported in the paper \n        'cpy': 0, # number of citations per year - not reported in the paper \n        'keywords': ['TAM', 'mbanking', 'awareness'], # list of keywords, broken into K1-K10\n        'abstract': 'string', # abstract of the paper \n        'F': ['perceived usefulness'], # factors significant in the study, broken into F1-F9 \n        'FN': ['another factor'], # factors not significant in the study, broken into FNS1-FNS4 \n        'limit': ['geographical context'], # limitations of the study, broken into LIMIT1-LIMIT3 \n        'typeofResearch': 'string', # type of research conducted in the study \n        'methods': ['regression analysis'], # methods used in the study, broken into METHOD1-METHOD4\n        'theory': ['TAM'] # theories used in the study, broken into THEORY1-THEORY4\n        'sampleSize': 100, # sample size of the study \n        'tech': 'string', # main technology studied \n        'man_theme': 'string', # Theme manually assigned by me \n        'algo_theme': 'string', # Theme assigned by the algorithm \n        'decision_Theme': 'string', # Final theme of the paper  \n        'Score_Sig': 0.0, # % of significance for factors \n        'Score_NOT_Sig': 0.0, # % of non-significance for factors\n    }\n])\n\n\n\nIdea for future\n\n🤖 Build an Agentic AI application that automates this process.\n\n\nPart 1.1 Finding Out Themes\nFirst, install the following Python modules. I saved the pdf files’ name in a dictionary like this:\n\nname_of_pdfs = {\n    'p2_01': \"Lonkani et al_2020_A comparative study of trust in mobile banking.pdf\", \n    'p2_02': \"Saprikis et al_2022_A comparative study of users versus non-users' behavioral intention towards.pdf\", \n    'p2_03': \"Malaquias et al_2021_A cross-country study on intention to use mobile banking.pdf\", \n    'p2_04': \"Merhi et al_2019_A cross-cultural study of the intention to use mobile banking between Lebanese.pdf\", \n    'p2_05': \"Frimpong et al. - 2020 - A cross‐national investigation of trait antecedent.pdf\", \n    # and so on ... \n}\n\nAdditionally, I defined a dictionary “look up” for all the factors in the dataset with their related theme that looks like this (shortened for this presentation):\n\ntheme_of_words = {\n    'demographic': \n        list(set(['women', 'woman', 'female', 'men', 'man', 'male', 'sex', 'gender', 'age', 'income', \n            'demographic variables', 'elderly', 'education', 'gender differences', 'generation y', 'millennial generation',\n            'millennial', 'gen y', 'gen Z', 'gen alpha', 'gen X', 'boomer', 'babyboomer', 'generation X', 'generation z',\n            'young consumers', \n            # A lot more factors ...\n            ])),\n    \n    #----------------------------------------------------------------------------------------------------------------------------------\n    'cultural': \n        list(set(['developing countries','malaysia','transition country','pakistan',\n            'zakat','developing country','ghana','USA','srilanka', 'sri lanka',\n            'india','maldives','saudi-arabia','saudi arabia', 'nigeria','thailand','united states',\n            'yemen','citizenship','zimbabwe','palestine','culture',\n            'Country perspective', \n            # ... \n            ])),\n    \n    #----------------------------------------------------------------------------------------------------------------------------------\n    'psychological':\n        list(set(['anxiety','satisfaction','behavior','behaviour','attitudes','attitude','awareness',\n            'technology anxiety','consumer-behavior','trust','benv','consumer behaviour',\n            'covid-19 related psychological distress','psychological distress','psychological','distress',\n            'behavioral','computer anxiety','customer satisfaction', 'cognitive resistance',\n            # A LOT more ... \n            ]))\n            , \n            # ... few other key value pairs corresponding to themes \n\n}\n\nI also needed to delete some stop words, and decided to add more words that I knew would be frequently repeated. I also define the lemmer and stemmer.\n\nstop_words = stopwords.words('english')\nstop_words.extend([\"bank\", \"banking\", \"banks\", \n                   \"mobile\", \"mbank\", \"mbanking\", \"m-bank\", \"m bank\",\n                   \"online\", \"e\", \"e-bank\", \"ebank\", \"mobile banking\", \"mobile bank\", \n                   \"adoption\", \"acceptance\", \"accept\", \"theory\", \n                   \"purpose\", \"result\", \"method\", #from abstracts \n                   \"journal\", \"volume\", \"pp\", \"no\", \"doi\", \"http\", \"https\", \"et al\", \"issue\",\n                   \"technology\", \"internet\", \"information system\", \"international information\",\n                   \"information technology\", \"computer human\", \"mis quarterly\", \"electornic commerce\",\n                   \"j market\", \"telematics and informatics\", \"telematics informatics\", \"retail consumer\",\n                   \"international volume\", \"international business\", \"global information\",\n                   \"et\", \"al\", \"al.\", \"tam\", \"sem\", \"pls\", \"utaut\", \"tpb\",\n                   \".com\", \"management\", \"marketing\", \"published\", \"study\",\n                   \"research\", \"literature\", \"model\", #from journal information \n                   \"app\", \"application\", \"usage\"])\n\nstemmer = PorterStemmer()\nlemmatizer = WordNetLemmatizer()\n\n\nSet up.\nSo, I need a few functions as set up for cleaning the text. Function extract_text_from_pdf() is using PyMuPDF to extract text from a PDF file.\n\n#version one using PyMuPDF \ndef extract_text_from_pdf(filename):\n    text = \"\"\n    try:\n        doc = fitz.open(filename)\n        for page_num in range(doc.page_count):\n            page = doc.load_page(page_num)\n            text += page.get_text()\n    except Exception as e:\n        print(f\"Error reading {filename}: {e}\")\n    return text\n\nThis function is just one of the data cleaning functions: For Topic modeling, I write a function to generate dictionaries and save them in a .mm file format.\n\ndef generate_dictionary(text, name):\n    \"\"\" \n    As input takes in the text to build the dictionary for and the name of a .mm file\n    \"\"\" \n    \n    dictionary = Dictionary(text)\n    \n    corpus = [dictionary.doc2bow(review) for review in text] \n    \n    filename = f\"{name}.mm\"\n    \n    MmCorpus.serialize(filename, corpus)\n\n    return dictionary, corpus\n\nAdditionally, I want a function that prints the top 50 most frequently appearing words in the corpus:\nI also plan on seeing how python clusters the words (as in, finds similar words) vs me: This is a function for if you want to use a word embedding (requires some effort, time and machine power!):\n\ndef get_embedding(text):\n    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n    model_bert = BertModel.from_pretrained('bert-base-uncased')\n    \n    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=20)\n    with torch.no_grad():\n        outputs = model_bert(**inputs)\n    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n\nAnd then you use this to get semantically similar words:\n\ndef get_semantically_similar_words(words, threshold=0.7):\n    similar_words = set(words)\n    for word in words:\n        token = nlp(word)\n        for vocab_word in nlp.vocab:\n            if vocab_word.has_vector and vocab_word.is_alpha:\n                similarity = token.similarity(nlp(vocab_word.text))\n                if similarity &gt;= threshold:\n                    similar_words.add(vocab_word.text)\n    return similar_words\n\nSo, how do I find the themes? Essentially, I just tweaked TF-IDF:\n\nclass CustomTfidfVectorizer(TfidfVectorizer):\n    def __init__(self, vocabulary=None, **kwargs):\n        super().__init__(vocabulary=vocabulary, **kwargs)\n        #self.general_keywords = set(general_keywords)\n        \n    def build_analyzer(self):\n        analyzer = super().build_analyzer()\n        return lambda doc: [w for w in analyzer(doc)] #if w not in self.general_keywords]\n    \n    def fit(self, raw_documents, y=None):\n        self.fit_transform(raw_documents, y)\n        return self\n\n    def fit_transform(self, raw_documents, y=None):\n        X = super().fit_transform(raw_documents, y)\n        self.max_frequencies = self._compute_max_frequencies(X, raw_documents)\n        return X\n\n    def transform(self, raw_documents):\n        X = super().transform(raw_documents)\n\n        # Calculate augmented term frequency\n        max_frequencies = self.max_frequencies\n        max_frequencies[max_frequencies == 0] = 1  # Avoid division by zero\n        augmented_tf = 0.5 + 0.5 * (X.toarray() / max_frequencies[:, None])\n        \n        # Penalize general keywords\n        #penalized_idf = self.idf_ * (1 - 0.8 * np.isin(self.get_feature_names_out(), list(self.general_keywords)))\n        \n        # Apply penalized IDF\n        augmented_tfidf = augmented_tf * penalized_idf\n\n        return csr_matrix(augmented_tfidf)\n\n    def _compute_max_frequencies(self, X, raw_documents):\n        max_frequencies = np.zeros(X.shape[0])\n        for i, doc in enumerate(raw_documents):\n            term_freq = {}\n            for term in doc.split():\n                if term in term_freq:\n                    term_freq[term] += 1\n                else:\n                    term_freq[term] = 1\n            max_frequencies[i] = max(term_freq.values())\n        return max_frequencies\n\n\n\nKeyword Analytics\n\ntry:\n    df = pd.read_csv(\"P2_AR_04.csv\", encoding='utf-8')\nexcept UnicodeDecodeError:\n    try:\n        df = pd.read_csv(\"P2_AR_04.csv\", encoding='latin-1')\n    except Exception as e:\n        error_message = str(e)\n        df = None\n\nClean all the data you’ve gathered the same way the PDF’s have been cleaned (the preprocess_text() function looks very similar to the cleaning function above!):\n\ndf2 = df.copy()\n\ncolumns_to_preprocess = ['Man_Theme',\n                        'K1','K2','K3','K4','K5','K6','K7','K8','K9','K10',\n                        'F1','F2','F3','F4','F5','F6','F7','F8','F9',\n                        'FNS1','FNS2','FNS3','FNS4',\n                        'METHOD1','METHOD2','METHOD3','METHOD4',\n                        'THEORY1','THEORY2','THEORY3','THEORY4',\n                        'LIMIT1' ,'LIMIT2' ,'LIMIT3', 'Abstract'\n                        ]\n\nfor col in columns_to_preprocess:\n    df2[col] = df2[col].apply(preprocess_text)\n\n\npapers = {}\n\nfor paper_id, filename in name_of_pdfs.items():\n    text = extract_text_from_pdf(filename)\n    papers[paper_id] = text\n\npapers_df = pd.DataFrame.from_dict(papers, orient = 'index', columns = ['paperText'])\npapers_df = papers_df.reset_index(names = ['paperID'])\npapers_df.to_csv('papers_unclean.csv')\npapers_df.head()\n\nThese look like this: \n\n# keep a copy (a habit of mine)\npapers_uncleaned = papers.copy() \ntheme_of_words_uncleaned = theme_of_words.copy() \n\n# clean up all papers \npapers_cleaned = preprocess_Dict(papers)\npapersClean_df = pd.DataFrame.from_dict(papers_cleaned, orient = 'index', columns = ['paperText'])\npapersClean_df = papersClean_df.reset_index(names = ['paperID'])\npapersClean_df.to_csv('papers_clean3.csv')\npapersClean_df.head()\n\n\nClean the theme of words dictionary so the words match:\n\ntheme_of_words_cleaned = {}\n\nfor k, v in theme_of_words.items():\n    theme_of_words_cleaned[k] = preprocess_list(v)\n\ntheme_of_words_cleaned['psychological'][:10]\n\n\n\n\n[‘initial trst’,‘simcong’,‘hedmotiv’,‘selfefficacy’,‘strss’,‘controlled motiv’,‘cong’,‘selfcong cong’,‘trst systems’,‘custom loyalti’]\n\n\n\nMake sure to drop all NA’s and empty values:\n\nfor k, v in theme_of_words_cleaned.items():\n    theme_of_words_cleaned[k] = [x for x in v if x not in [None, \"\", ' ', 'NaN'] and not (isinstance(x, float) and math.isnan(x))]\n\n\n\nThemes Based on Count of Words in Each Group\nI will skip the parts on BERT and word embeddings that’s in the jupyter notebook as these were not used for my project. The reason is that it was taking so long and my computer simply did not have the capacity to handle it. I also didn’t have the time to find a fix for it, but there are a bunch of commented-out code from chatGPT that I was playing around with.\nSo, we now have a dictionary with the factors and their theme, and a corpus of text of all papers in the dataset. What I’m going to do here is: * Count all the words total_words in each paper * Count the instances of each word (factor) in theme_of_words_cleaned in each paper * Each word’s count adds 1 point to its corresponding theme’s “weight score” * Take for example the word emotion and I said the theme for this word is psychological. If emotion shows up 10 times in paper 1, paper 1’s dictionary of weights has a weight of 10 for (divided by the total number of words) psychological.\n\nresults = []\ncount_words_df = pd.DataFrame(results)\n\n\nfor doc_id, text in papers_cleaned.items():\n    doc = nlp(text)\n    word_counts = defaultdict(int)\n\n    for token in doc:\n        for group, keywords in theme_of_words_cleaned.items():\n            if token.text.lower() in keywords:\n                word_counts[group] += 1\n\n    total_words = len(doc)\n    group_weights = {f\"{group}_w\": count / total_words for group, count in word_counts.items()}\n    max_weight = max(group_weights.values(), default=0)\n    theme = max(group_weights, key=group_weights.get).replace(\"_w\", \"\") if max_weight &gt; 0 else None\n\n    result = {\"doc_id\": doc_id, **word_counts, **group_weights, \"theme\": theme, \"max_weight\": max_weight}\n    results.append(result)\n\n\nfor group in theme_of_words_cleaned.keys():\n    if group not in count_words_df.columns:\n        count_words_df[group] = 0\n    if f\"{group}_w\" not in count_words_df.columns:\n        count_words_df[f\"{group}_w\"] = 0.0\n\ncount_words_df.head()\n\n\nThis is still very basic, though because it’s only based on the factors which may not be fully representative. So, I do this again using keywords in addition to the factors. These are keywords that were selected by the authors as well as information extracted from Web of Science \\BibTeX file.\n\ncols_toPick = ['K1','K2','K3','K4','K5','K6','K7','K8','K9','K10','F1','F2','F3','F4','F5','F6','F7','F8','F9']\n\nkeywordsDf = df2.loc[:,cols_toPick]\n\n# flatten the dataframe to a list \nkeywords_across_db = keywordsDf.values.flatten().tolist()\n\n# there are 2,717 words here \nprint(\"number of words (factors and keywords) in total \", len(keywords_across_db))\n\n# making sure there are no empty/NaN/Null values \nkeywords_across_db = [x for x in keywords_across_db if x not in [None, \"\", ' ', 'NaN'] and not (isinstance(x, float) and math.isnan(x))]\n\n# making sure there are no duplicates (set takes care of this)\nkeywords_across_db_nodup_cleaned = list(set(keywords_across_db))\n\n# convert the list into a dictionary temporarily, then convert it to a dataframe \ntemp = {'Keyword': keywords_across_db_nodup_cleaned}\nkeywords_themes_df = pd.DataFrame(temp, columns=['Keyword'])\n\n# go back to the theme_of_words_cleaned and find each keyword's theme \nkeywords_themes_dic = {keyword: theme for theme, keywords in theme_of_words_cleaned.items() for keyword in keywords}\n\n# This is just a dataframe view of the keywords with their respective theme \nkeywords_themes_df['Theme'] = keywords_themes_df['Keyword'].map(keywords_themes_dic)\n\n# if the theme is empty, give it \"Generic\" - that means these keywords weren't in the list of important words that we picked themes for \nkeywords_themes_df['Theme'] = keywords_themes_df['Theme'].apply(lambda x: 'Generic' if pd.isna(x) or x == ' ' else x)\n\nkeywords_themes_df.head()\n\n\nI’m gonna get rid of all the Generic keywords, so keeping a copy of this dataframe:\n\nkeywords_themes_df_withGenerics = keywords_themes_df.copy()\n\nkeywords_themes_df['Theme'] = keywords_themes_df['Theme'].apply(lambda x: 'Generic' if pd.isna(x) or x == ' ' else x)\n\n# everything but generic \nkeywords_themes_df = keywords_themes_df.loc[keywords_themes_df['Theme'] != 'Generic']\n\n# flatten it to build a vocabulary \nwords_acrossAll_nonGeneric = keywords_themes_df['Keyword'].values.flatten().tolist()\n\n# making sure no null values were generated \nwords_acrossAll_nonGeneric = [x for x in words_acrossAll_nonGeneric if x not in [None, \"\", ' ', 'NaN'] and not (isinstance(x, float) and math.isnan(x))]\n\n# making sure there are no dulicates (233 words total)\nwords_acrossAll_nonGeneric = list(set(words_acrossAll_nonGeneric))\n\n\n\nTheme Assignment\nI will now use the custom TF-IDF class to generate a TF-IDF matrix. This is similar to what I did by hand a bit further above. Basically, all TF-IDF is doing is counting the frequency of words across the document.\n\nvectorizer_keys = CustomTfidfVectorizer(vocabulary = words_acrossAll_nonGeneric)\n\ntfidf_matrix = vectorizer_keys.fit_transform(papers_cleaned.values())\n\ntfidf_df = pd.DataFrame(tfidf_matrix.toarray(), index = papers_cleaned.keys(), columns=vectorizer_keys.get_feature_names_out())\n\ntfidf_df.head()\n\n\nNow using TF-IDF scores, finding the weights for themes for each paper:\n\nkeyword_to_theme = {keyword: theme for theme, keywords in theme_of_words_cleaned.items() for keyword in keywords}\n\ntheme_weights = pd.DataFrame(0, index=tfidf_df.index, columns=theme_of_words_cleaned.keys())\n\nfor keyword, theme in keyword_to_theme.items():\n    if keyword in tfidf_df.columns:\n        theme_weights[theme] += tfidf_df[keyword]\n\n\nfor _, row in df2.iterrows():\n    paper_id = row['ID']\n    keywords = words_acrossAll_nonGeneric \n    \n    for keyword in keywords:\n        if keyword in tfidf_df.columns:\n            theme = keyword_to_theme.get(keyword, None)\n            if theme:\n                #if theme in tfidf_df.index:\n                theme_weights.at[paper_id, theme] += tfidf_df.at[paper_id, keyword] * 5  \n\n# this is picking just 1 theme per paper - find the theme with the maximum weight as the main theme of the paper \nmain_theme_df = theme_weights.apply(lambda row: (row == row.max()).astype(int), axis=1)\n\nmain_theme_df.head()\n\n\nVisualizing this:\n\nfig, axes = plt.subplots(nrows=2, ncols=1, figsize=(20, 10))\n\n# Heatmap for theme weights\nsns.heatmap(theme_weights.T, ax=axes[0], cmap=\"YlGnBu_r\", cbar_kws={'label': 'Weight'})\naxes[0].set_xlabel(\"Paper ID\")\naxes[0].set_ylabel(\"Theme\")\naxes[0].set_title(\"Theme Weights per Paper\")\n\n# Heatmap for main themes\nsns.heatmap(main_theme_df.T, ax=axes[1], cmap=\"YlGnBu_r\", cbar_kws={'label': 'Theme Presence'})\naxes[1].set_xlabel(\"Paper ID\")\naxes[1].set_ylabel(\"Theme\")\naxes[1].set_title(\"Main Theme per Paper\")\n\nplt.tight_layout()\nplt.show()\nplt.savefig('main_themes_heatmap_1.png')\n\n\nTo extract the themes easily:\n\n# Extract themes for each paper\nthemes_for_papers = {\n    paper_id: main_theme_df.columns[row.astype(bool)].tolist()\n    for paper_id, row in main_theme_df.iterrows()\n}\n\n# Print the themes for each paper\nfor paper_id, themes in themes_for_papers.items():\n    print(f\"Paper ID: {paper_id}, Themes: {', '.join(themes)}\")\n\n\nPaper ID: p2_01, Themes: psychological Paper ID: p2_02, Themes: psychological Paper ID: p2_03, Themes: psychological Paper ID: p2_04, Themes: psychological Paper ID: p2_05, Themes: psychological Paper ID: p2_06, Themes: perceptive Paper ID: p2_07, Themes: personal Paper ID: p2_08, Themes: personal Paper ID: p2_09, Themes: perceptive Paper ID: p2_10, Themes: psychological\n\nSince Topic Modeling is Multimembership, papers can have more than just 1 theme. Since I didn’t use a word embedding and may not get the best representatitve theme here, I decided to allow for up to 3 themes. To do this, I changed the CustomTfidfVectorizer class.\n\n\n\nALLOWING FOR MULTIPLE THEMES\n\nclass CustomTfidfVectorizerUpdateClass(TfidfVectorizer):\n    def __init__(self, theme_keywords, threshold=0.8, **kwargs):\n        # Generate vocabulary from theme_keywords\n        vocabulary = list(set(word for words in theme_keywords.values() for word in words))\n        super().__init__(vocabulary=vocabulary, **kwargs)\n        self.threshold = threshold  # Threshold for determining multiple themes\n        self.theme_keywords = theme_keywords  # Store theme_keywords for later use\n\n    def build_analyzer(self):\n        analyzer = super().build_analyzer()\n        return lambda doc: [w for w in analyzer(doc)]\n\n    def fit(self, raw_documents, y=None):\n        self.fit_transform(raw_documents, y)\n        return self\n\n    def fit_transform(self, raw_documents, y=None):\n        X = super().fit_transform(raw_documents, y)\n        self.max_frequencies = self._compute_max_frequencies(X, raw_documents)\n        return X\n\n    def transform(self, raw_documents):\n        X = super().transform(raw_documents)\n\n        # Calculate augmented term frequency\n        max_frequencies = self.max_frequencies\n        max_frequencies[max_frequencies == 0] = 1  # Avoid division by zero\n        augmented_tf = 0.5 + 0.5 * (X.toarray() / max_frequencies[:, None])\n        \n        augmented_tfidf = augmented_tf  # No penalized IDF applied here\n\n        return csr_matrix(augmented_tfidf)\n\n    def determine_themes(self, documents_dict):\n        \"\"\"\n        Determines the themes for each document based on TF-IDF scores.\n        A paper can have multiple themes if the scores are within the threshold.\n\n        :param documents_dict: Dictionary of documents (keys: IDs, values: text)\n        :return: Dictionary where keys are document IDs and values are lists of themes\n        \"\"\"\n        document_ids = list(documents_dict.keys())\n        raw_documents = list(documents_dict.values())\n        \n        X = self.transform(raw_documents).toarray()\n        feature_name_to_index = {name: i for i, name in enumerate(self.get_feature_names_out())}\n\n        theme_scores = {}\n        for doc_index, doc_vector in enumerate(X):\n            doc_id = document_ids[doc_index]\n            # Calculate scores for each theme\n            scores = {\n                theme: sum(doc_vector[feature_name_to_index[word]]\n                           for word in keywords if word in feature_name_to_index)\n                for theme, keywords in self.theme_keywords.items()\n            }\n            max_score = max(scores.values()) if scores else 0\n            \n            # Determine themes within the threshold\n            selected_themes = [\n                theme for theme, score in scores.items()\n                if score &gt;= self.threshold * max_score\n            ]\n            theme_scores[doc_id] = selected_themes\n\n        return theme_scores\n\n    def _compute_max_frequencies(self, X, raw_documents):\n        max_frequencies = np.zeros(X.shape[0])\n        for i, doc in enumerate(raw_documents):\n            term_freq = {}\n            for term in doc.split():\n                if term in term_freq:\n                    term_freq[term] += 1\n                else:\n                    term_freq[term] = 1\n            max_frequencies[i] = max(term_freq.values())\n        return max_frequencies\n\nSimilar to the above task, I generate the vectorizer from theme_of_words_cleaned vocabulary, but this time, allow for a few more themes (threshold = 0.75).\n\nvectorizer_keys2 = CustomTfidfVectorizerUpdateClass(theme_keywords = theme_of_words_cleaned, threshold = 0.75) \n\ntfidf_matrix2 = vectorizer_keys2.fit_transform(papers_cleaned.values())\n\ntfidf_df2 = pd.DataFrame(tfidf_matrix2.toarray(), index=papers_cleaned.keys(), columns=vectorizer_keys2.get_feature_names_out())\ntfidf_df2.head()\n\n\nYou can still pick a main theme for each paper, but I want to see the top 3 themes:\n\nkeyword_to_theme2 = {keyword: theme for theme, keywords in theme_of_words_cleaned.items() for keyword in keywords}\n\ntheme_weights2 = pd.DataFrame(0, index=tfidf_df2.index, columns=theme_of_words_cleaned.keys())\n\nfor keyword, theme in keyword_to_theme2.items():\n    if keyword in tfidf_df2.columns:\n        theme_weights2[theme] += tfidf_df2[keyword]\n\nfor _, row in df2.iterrows():\n    paper_id = row['ID']\n    #for keyword in words_acrossAll_nonGeneric:\n    for keyword in vectorizer_keys2.get_feature_names_out():\n        if keyword in tfidf_df2.columns:\n            theme = keyword_to_theme2.get(keyword, None)\n            if theme:\n                # the 5 I'm adding here is just to make the weights a bit larger for visualization. It's a simple scaling and won't change the results \n                theme_weights2.at[paper_id, theme] += tfidf_df2.at[paper_id, keyword] * 5\n\n\ntop_3_themes_for_papers = {\n    paper_id: theme_weights2.loc[paper_id]\n    .sort_values(ascending=False)[:3]  \n    .index.tolist()\n    for paper_id in theme_weights2.index\n}\n\nfor paper_id, themes in top_3_themes_for_papers.items():\n    print(f\"Paper ID: {paper_id}, Top 3 Themes: {', '.join(themes)}\")\n\n\nPaper ID: p2_01, Top 3 Themes: psychological, cultural, demographic Paper ID: p2_02, Top 3 Themes: psychological, market, personal Paper ID: p2_03, Top 3 Themes: psychological, personal, perceptive Paper ID: p2_04, Top 3 Themes: psychological, personal, perceptive Paper ID: p2_05, Top 3 Themes: cultural, personal, psychological Paper ID: p2_06, Top 3 Themes: perceptive, cultural, psychological Paper ID: p2_07, Top 3 Themes: personal, psychological, demographic Paper ID: p2_08, Top 3 Themes: psychological, personal, perceptive Paper ID: p2_09, Top 3 Themes: perceptive, psychological, personal Paper ID: p2_10, Top 3 Themes: psychological, cultural, personal\n\nKeep a copy and save this to file:\n\ndf3 = df2.copy()\n\ntop_3_themes_for_papers = {\n    paper_id: \", \".join(theme_weights2.loc[paper_id]\n                         .sort_values(ascending=False)[:3]\n                         .index.tolist())\n    for paper_id in theme_weights2.index\n}\n\ndf3[\"Algo_Theme\"] = df2[\"ID\"].map(top_3_themes_for_papers)\n\ndf3.to_csv(\"NewWithThemes.csv\")",
    "crumbs": [
      "Home",
      "Projects",
      "Project 1. SLR",
      "Algorithmic Approach to Finding Themes"
    ]
  },
  {
    "objectID": "study1_DA.html",
    "href": "study1_DA.html",
    "title": "Data Analysis",
    "section": "",
    "text": "The R libraries used for data analysis are as follows:\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(purrr)\nlibrary(ggplot2)\nlibrary(psych)\nlibrary(tidyr)\nlibrary(stargazer)\nlibrary(forcats)\nlibrary(xtable)\nlibrary(ggraph)\nlibrary(igraph)\nlibrary(gt)\nlibrary(ggpubr)\n\nLooking at the data:\n\ndf &lt;- read.csv(\"data/P2_AR_07.csv\") \nglimpse(df)\n\nSummary statiscs\n\npsych::describe(df %&gt;% \n    dplyr::select(Year, Match, Num_Factors, NUM_FAC_NOTSIG, SampleSize)) %&gt;% \n    dplyr::select(vars, n, mean, sd, median, min, max) \n\nWoah! One paper has 25,000 and that is messing up the sample sizes. Remembering this study’s ID:\n\ndf %&gt;% filter(SampleSize == 25000) %&gt;% \n    dplyr::select(ID, Title, SampleSize)\n\nSetting aside the study with sample size of 25,000:\n\npsych::describe(\n    df %&gt;% dplyr::select(Year, Match, Num_Factors, NUM_FAC_NOTSIG, SampleSize) %&gt;% \n    filter(SampleSize != 25000)) %&gt;% \n    dplyr::select(vars, n, mean, sd, median, min, max) \n\nWelp! Another large study.\n\ndf %&gt;% filter(SampleSize == 21526) %&gt;% \n    dplyr::select(ID, Title, SampleSize)\n\nSetting aside the study with sample size of 25,000 and the one with 21,52 as they are outliers:\n\npsych::describe(\n    df %&gt;% filter(!ID %in% c('p2_59','p2_77')) %&gt;% \n    dplyr::select(ID, Year,Match, Num_Factors, NUM_FAC_NOTSIG, SampleSize)) %&gt;% \n    dplyr::select(vars, n, mean, sd, median, min, max) \n\nCounting the unique values for each of the columns:\n\nresults &lt;- c(\n  paste('Number of Unique Values in ID: ', n_distinct(df$ID)),\n  paste('Number of Unique Values in Title: ', n_distinct(df$Title)),\n  paste('Number of Unique Values in PublicationTitles: ', n_distinct(df$PublicationTitle)),\n  paste('Number of Unique Values in Publisher: ', n_distinct(df$Publisher)),\n  paste('Number of Unique Values in AffiliationCountry: ', n_distinct(df$AffiliationCountry)),\n  paste('Number of Unique Values in Factors: ', dplyr::n_distinct(df %&gt;% dplyr::select(F1:F9) %&gt;% unlist())),\n  paste('Number of Unique Values in Not Sig: ', dplyr::n_distinct(df %&gt;% dplyr::select(FNS1:FNS4) %&gt;% unlist())),\n  paste('Number of Unique Values in Methods: ', dplyr::n_distinct(df %&gt;% dplyr::select(METHOD1:METHOD4) %&gt;% unlist())),\n  paste('Number of Unique Values in Theory: ', dplyr::n_distinct(df %&gt;% dplyr::select(THEORY1:THEORY4) %&gt;% unlist())),\n  paste('Number of Unique Values in Limits: ', dplyr::n_distinct(df %&gt;% dplyr::select(LIMIT1:LIMIT3) %&gt;% unlist())),\n  paste('Number of Unique Values in ResearchType: ', n_distinct(df$ResearchType)),\n  paste('Number of Unique Values in Authors: ', n_distinct(df$Creators)),\n  paste('Number of Unique Values in Keywords: ', dplyr::n_distinct(df %&gt;% dplyr::select(K1:K10) %&gt;% unlist())),\n  paste('Number of Unique Values in Tech: ', n_distinct(df$Tech)),\n  paste('Number of Unique Values in Themes: ', n_distinct(df$DecisionTheme))\n)\n\ncat(results, sep = \"\\n\")\n\nChecking the sample sizes Without the outliers:\n\npsych::describe(\n    df %&gt;% filter(!ID %in% c('p2_59','p2_77')) %&gt;% \n    dplyr::select(SampleSize)) %&gt;% \n    dplyr::select(n, mean, sd, median, min, max) \n\n\nnoOutliers &lt;- df %&gt;% filter(!ID %in% c('p2_59','p2_77'))\n\nquantiles &lt;- quantile(noOutliers$SampleSize, na.rm = T)\n\nquantile_binned &lt;- cut(df$SampleSize, \n                breaks = quantiles, \n                labels = c(\"SQ1\", \"SQ2\", \"SQ3\", \"SQ4\"), \n                include.lowest = TRUE)\n\ndf$SampleSizeBin &lt;- quantile_binned\n\ndf &lt;- df %&gt;% mutate(\n    SampleSizeBin = if_else(\n        is.na(SampleSizeBin),\n        \"NotStated\",\n        SampleSizeBin\n    )\n)\n\ndf %&gt;% count(SampleSizeBin)\n\nLet’s calculate the scores for factors that are significant and non-significant:\n\nF_counts &lt;- df %&gt;%\n  dplyr::select(F1:F9) %&gt;%\n  unlist() %&gt;%\n  table() %&gt;%\n  as.data.frame() %&gt;%\n  rename(FAC = \".\", F_count = \"Freq\")\n\nFNS_counts &lt;- df %&gt;%\n  dplyr::select(FNS1:FNS4) %&gt;%\n  unlist() %&gt;%\n  table() %&gt;%\n  as.data.frame() %&gt;%\n  rename(FAC = \".\", FNS_counts = \"Freq\")\n\n# Count occurrences of each factor in all columns (F1 to F9 + FNS1 to FNS4)\nTotal_counts &lt;- df %&gt;%\n  dplyr::select(c(F1:F9, FNS1:FNS4)) %&gt;%\n  unlist() %&gt;%\n  table() %&gt;%\n  as.data.frame() %&gt;%\n  rename(FAC = \".\", Total_count = \"Freq\")\n\n# Merge the two count tables\nfactor_scores &lt;- merge(F_counts,FNS_counts, by = \"FAC\", all = TRUE)\nfactor_scores &lt;- merge(factor_scores, Total_counts, by = \"FAC\", all = TRUE)\n\n\n# Replace NAs with 0 for cases where factors appear in some but not all sections\nfactor_scores[is.na(factor_scores)] &lt;- 0\n\nfactor_scores &lt;- factor_scores %&gt;%\n  mutate(Score_Sig = round(F_count / Total_count, 2),\n         Score_NOT_Sig = round(FNS_counts / Total_count, 2)) %&gt;% filter(FAC != \"\")\n\n\nhead(factor_scores)",
    "crumbs": [
      "Home",
      "Projects",
      "Project 1. SLR",
      "Data Analysis"
    ]
  },
  {
    "objectID": "study1_DA.html#part-3.-results",
    "href": "study1_DA.html#part-3.-results",
    "title": "Data Analysis",
    "section": "Part 3. Results",
    "text": "Part 3. Results",
    "crumbs": [
      "Home",
      "Projects",
      "Project 1. SLR",
      "Data Analysis"
    ]
  },
  {
    "objectID": "study1_DA.html#part-1-data-cleaning-and-prep",
    "href": "study1_DA.html#part-1-data-cleaning-and-prep",
    "title": "Data Analysis",
    "section": "",
    "text": "The R libraries used for data analysis are as follows:\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(purrr)\nlibrary(ggplot2)\nlibrary(psych)\nlibrary(tidyr)\nlibrary(stargazer)\nlibrary(forcats)\nlibrary(xtable)\nlibrary(ggraph)\nlibrary(igraph)\nlibrary(gt)\nlibrary(ggpubr)\n\nLooking at the data:\n\ndf &lt;- read.csv(\"data/P2_AR_07.csv\") \nglimpse(df)\n\nRows: 143\nColumns: 62\n$ Reason_Theme       &lt;chr&gt; \"comparative\", \"behavioral intention\", \"keywords\", …\n$ CiteKey            &lt;chr&gt; \"LonkaniAcomparativeStudyOfTrust2020\", \"SaprikisACo…\n$ ID                 &lt;chr&gt; \"p2_01\", \"p2_02\", \"p2_03\", \"p2_04\", \"p2_05\", \"p2_06…\n$ Algo_Theme         &lt;chr&gt; \"psychological, cultural, demographic\", \"psychologi…\n$ DecisionTheme      &lt;chr&gt; \"cultural\", \"psychological\", \"psychological\", \"cult…\n$ Man_Theme          &lt;chr&gt; \"cultural, psychological\", \"social, psychological\",…\n$ Match              &lt;int&gt; 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ Title              &lt;chr&gt; \"a comparative study of trust in mobile banking: an…\n$ Year               &lt;int&gt; 2020, 2022, 2021, 2019, 2020, 2020, 2024, 2020, 201…\n$ PublicationTitle   &lt;chr&gt; \"journal of global information management\", \"inform…\n$ Creators           &lt;chr&gt; \"ravi lonkani, chuleeporn changchit, tim klaus, jom…\n$ Publisher          &lt;chr&gt; \"igi global\", \"mdpi\", \"igi global\", \"elsevier\", \"wi…\n$ AffiliationCountry &lt;chr&gt; \"thailand\", \"greece\", \"brazil, south korea, usa\", \"…\n$ K1                 &lt;chr&gt; \"technology acceptance model\", \"word-of-mouth\", \"em…\n$ K2                 &lt;chr&gt; \"initial trust\", \"mobile-banking\", \"consumer adopti…\n$ K3                 &lt;chr&gt; \"consumer acceptance\", \"acceptance model\", \"initial…\n$ K4                 &lt;chr&gt; \"gender-differences\", \"information-technology\", \"in…\n$ K5                 &lt;chr&gt; \"normative beliefs\", \"consumer adoption\", \"anxiety\"…\n$ K6                 &lt;chr&gt; \"national culture\", \"moderating role\", \"acceptance\"…\n$ K7                 &lt;chr&gt; \"usage intentions\", \"utaut model\", \"commerce\", \"emp…\n$ K8                 &lt;chr&gt; \"privacy concerns\", \"services\", \"traits\", \"acceptan…\n$ K9                 &lt;chr&gt; \"internet users\", \"internet\", \"usage\", \"mediating r…\n$ K10                &lt;chr&gt; \"online trust\", \"determinants\", \"satisfaction\", \"ad…\n$ Num_Factors        &lt;int&gt; 5, 8, 5, 6, 4, 6, 4, 5, 4, 4, 8, 4, 5, 3, 6, 8, 6, …\n$ F1                 &lt;chr&gt; \"cltr\", \"peex\", \"self\", \"habt\", \"attd\", \"pu\", \"prgt…\n$ F2                 &lt;chr&gt; \"norm_blf\", \"socinf\", \"trst\", \"psec\", \"intrac\", \"pe…\n$ F3                 &lt;chr&gt; \"prv_exp\", \"fac_cond\", \"peou\", \"ppriv\", \"cltr\", \"so…\n$ F4                 &lt;chr&gt; \"competnc\", \"risk\", \"pu\", \"trst\", \"innov\", \"fac_con…\n$ F5                 &lt;chr&gt; \"trst\", \"anxiety\", \"intnt_use\", \"peex\", \"\", \"trst\",…\n$ F6                 &lt;chr&gt; \"\", \"rwrd\", \"\", \"price\", \"\", \"prisk\", \"\", \"\", \"\", \"…\n$ F7                 &lt;chr&gt; \"\", \"sec\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"habt\", …\n$ F8                 &lt;chr&gt; \"\", \"recom\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"price…\n$ F9                 &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\",…\n$ F1_THEME           &lt;chr&gt; \"social\", \"personal\", \"psychological\", \"personal\", …\n$ F2_THEME           &lt;chr&gt; \"social\", \"social\", \"psychological\", \"perceptive\", …\n$ F3_THEME           &lt;chr&gt; \"personal\", \"external\", \"perceptive\", \"perceptive\",…\n$ F4_THEME           &lt;chr&gt; \"external\", \"external\", \"perceptive\", \"psychologica…\n$ F5_THEME           &lt;chr&gt; \"psychological\", \"psychological\", \"psychological\", …\n$ F6_THEME           &lt;chr&gt; \"\", \"external\", \"\", \"external\", \"\", \"perceptive\", \"…\n$ F7_THEME           &lt;chr&gt; \"\", \"external\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"pe…\n$ F8_THEME           &lt;chr&gt; \"\", \"external\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"ex…\n$ F9_THEME           &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\",…\n$ NUM_FAC_NOTSIG     &lt;int&gt; 2, 2, 0, 4, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, …\n$ FNS1               &lt;chr&gt; \"age\", \"efex\", \"\", \"peex\", \"\", \"\", \"\", \"\", \"\", \"soc…\n$ FNS2               &lt;chr&gt; \"sex\", \"fac_cond\", \"\", \"pval\", \"\", \"\", \"\", \"\", \"\", …\n$ FNS3               &lt;chr&gt; \"\", \"\", \"\", \"socinf\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"…\n$ FNS4               &lt;chr&gt; \"\", \"\", \"\", \"hed_motiv\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"…\n$ ResearchType       &lt;chr&gt; \"comparative\", \"empirical\", \"empirical\", \"empirical…\n$ SampleSize         &lt;int&gt; 560, 837, 458, 901, 1340, 755, 418, 203, 384, 127, …\n$ METHOD1            &lt;chr&gt; \"regression\", \"cfa\", \"cfa\", \"sem\", \"multigroup sem\"…\n$ METHOD2            &lt;chr&gt; \"\", \"sem\", \"sem\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"…\n$ METHOD3            &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\",…\n$ METHOD4            &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\",…\n$ Tech               &lt;chr&gt; \"mobile banking\", \"mobile banking\", \"mobile banking…\n$ THEORY1            &lt;chr&gt; \"hcd\", \"utaut\", \"tam\", \"utaut2\", \"tam\", \"tam\", \"dct…\n$ THEORY2            &lt;chr&gt; \"\", \"\", \"\", \"\", \"tpb\", \"\", \"\", \"\", \"tpb\", \"\", \"utau…\n$ THEORY3            &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\",…\n$ THEORY4            &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\",…\n$ LIMIT1             &lt;chr&gt; \"sample\", \"sample\", \"sample\", \"sample\", \"no_long\", …\n$ LIMIT2             &lt;chr&gt; \"\", \"cross_cult\", \"bias\", \"no_mod\", \"bias\", \"sample…\n$ LIMIT3             &lt;chr&gt; \"\", \"incomp_fac\", \"question\", \"incomp_fac\", \"no_mod…\n$ Abstract           &lt;chr&gt; \"with the rapid growth of mobile phone usage, mobil…\n\n\nSummary statiscs\n\npsych::describe(df %&gt;% \n    dplyr::select(Year, Match, Num_Factors, NUM_FAC_NOTSIG, SampleSize)) %&gt;% \n    dplyr::select(vars, n, mean, sd, median, min, max) \n\nWoah! One paper has 25,000 and that is messing up the sample sizes. Remembering this study’s ID:\n\ndf %&gt;% filter(SampleSize == 25000) %&gt;% \n    dplyr::select(ID, Title, SampleSize)\n\nSetting aside the study with sample size of 25,000:\n\npsych::describe(\n    df %&gt;% dplyr::select(Year, Match, Num_Factors, NUM_FAC_NOTSIG, SampleSize) %&gt;% \n    filter(SampleSize != 25000)) %&gt;% \n    dplyr::select(vars, n, mean, sd, median, min, max) \n\nWelp! Another large study.\n\ndf %&gt;% filter(SampleSize == 21526) %&gt;% \n    dplyr::select(ID, Title, SampleSize)\n\nSetting aside the study with sample size of 25,000 and the one with 21,52 as they are outliers:\n\npsych::describe(\n    df %&gt;% filter(!ID %in% c('p2_59','p2_77')) %&gt;% \n    dplyr::select(ID, Year,Match, Num_Factors, NUM_FAC_NOTSIG, SampleSize)) %&gt;% \n    dplyr::select(vars, n, mean, sd, median, min, max) \n\nCounting the unique values for each of the columns:\n\nresults &lt;- c(\n  paste('Number of Unique Values in ID: ', n_distinct(df$ID)),\n  paste('Number of Unique Values in Title: ', n_distinct(df$Title)),\n  paste('Number of Unique Values in PublicationTitles: ', n_distinct(df$PublicationTitle)),\n  paste('Number of Unique Values in Publisher: ', n_distinct(df$Publisher)),\n  paste('Number of Unique Values in AffiliationCountry: ', n_distinct(df$AffiliationCountry)),\n  paste('Number of Unique Values in Factors: ', dplyr::n_distinct(df %&gt;% dplyr::select(F1:F9) %&gt;% unlist())),\n  paste('Number of Unique Values in Not Sig: ', dplyr::n_distinct(df %&gt;% dplyr::select(FNS1:FNS4) %&gt;% unlist())),\n  paste('Number of Unique Values in Methods: ', dplyr::n_distinct(df %&gt;% dplyr::select(METHOD1:METHOD4) %&gt;% unlist())),\n  paste('Number of Unique Values in Theory: ', dplyr::n_distinct(df %&gt;% dplyr::select(THEORY1:THEORY4) %&gt;% unlist())),\n  paste('Number of Unique Values in Limits: ', dplyr::n_distinct(df %&gt;% dplyr::select(LIMIT1:LIMIT3) %&gt;% unlist())),\n  paste('Number of Unique Values in ResearchType: ', n_distinct(df$ResearchType)),\n  paste('Number of Unique Values in Authors: ', n_distinct(df$Creators)),\n  paste('Number of Unique Values in Keywords: ', dplyr::n_distinct(df %&gt;% dplyr::select(K1:K10) %&gt;% unlist())),\n  paste('Number of Unique Values in Tech: ', n_distinct(df$Tech)),\n  paste('Number of Unique Values in Themes: ', n_distinct(df$DecisionTheme))\n)\n\ncat(results, sep = \"\\n\")\n\nChecking the sample sizes Without the outliers:\n\npsych::describe(\n    df %&gt;% filter(!ID %in% c('p2_59','p2_77')) %&gt;% \n    dplyr::select(SampleSize)) %&gt;% \n    dplyr::select(n, mean, sd, median, min, max) \n\n\nnoOutliers &lt;- df %&gt;% filter(!ID %in% c('p2_59','p2_77'))\n\nquantiles &lt;- quantile(noOutliers$SampleSize, na.rm = T)\n\nquantile_binned &lt;- cut(df$SampleSize, \n                breaks = quantiles, \n                labels = c(\"SQ1\", \"SQ2\", \"SQ3\", \"SQ4\"), \n                include.lowest = TRUE)\n\ndf$SampleSizeBin &lt;- quantile_binned\n\ndf &lt;- df %&gt;% mutate(\n    SampleSizeBin = if_else(\n        is.na(SampleSizeBin),\n        \"NotStated\",\n        SampleSizeBin\n    )\n)\n\ndf %&gt;% count(SampleSizeBin)\n\nLet’s calculate the scores for factors that are significant and non-significant:\n\nF_counts &lt;- df %&gt;%\n  dplyr::select(F1:F9) %&gt;%\n  unlist() %&gt;%\n  table() %&gt;%\n  as.data.frame() %&gt;%\n  rename(FAC = \".\", F_count = \"Freq\")\n\nFNS_counts &lt;- df %&gt;%\n  dplyr::select(FNS1:FNS4) %&gt;%\n  unlist() %&gt;%\n  table() %&gt;%\n  as.data.frame() %&gt;%\n  rename(FAC = \".\", FNS_counts = \"Freq\")\n\n# Count occurrences of each factor in all columns (F1 to F9 + FNS1 to FNS4)\nTotal_counts &lt;- df %&gt;%\n  dplyr::select(c(F1:F9, FNS1:FNS4)) %&gt;%\n  unlist() %&gt;%\n  table() %&gt;%\n  as.data.frame() %&gt;%\n  rename(FAC = \".\", Total_count = \"Freq\")\n\n# Merge the two count tables\nfactor_scores &lt;- merge(F_counts,FNS_counts, by = \"FAC\", all = TRUE)\nfactor_scores &lt;- merge(factor_scores, Total_counts, by = \"FAC\", all = TRUE)\n\n\n# Replace NAs with 0 for cases where factors appear in some but not all sections\nfactor_scores[is.na(factor_scores)] &lt;- 0\n\nfactor_scores &lt;- factor_scores %&gt;%\n  mutate(Score_Sig = round(F_count / Total_count, 2),\n         Score_NOT_Sig = round(FNS_counts / Total_count, 2)) %&gt;% filter(FAC != \"\")\n\n\nhead(factor_scores)\n\n\n\nNow let’s actually do some analysis. Let’s visualize how the themes of the papers have changed across the years. I will first generate a bar plot that fills the bars at each year (as a categorical factor) with proportions of themes in that year. This is an aggregation that happens under the hood, and using position = \"fill\" will actually make sure all the bars consider things relative to eachother, filling the full 100% of the bar.\n\nggplot(df, aes(x = as.factor(Year), fill = DecisionTheme)) +\n  geom_bar(position = \"fill\") +\n  theme_minimal() +\n  labs(fill = \"Theme\",\n       x = \"Year\",\n       y = \"Total Count\") +\n  fill_palette(\"Set3\")\n\nTo see how things move/flow over the years, a line chart is a great idea:\n\ndf %&gt;%\n    dplyr::count(DecisionTheme, Year) %&gt;%\n    ggplot(aes(x = as.factor(Year), y = n, color = DecisionTheme, group = DecisionTheme)) +\n  geom_line() +\n  geom_point() +\n  theme_minimal() +\n  labs(fill = \"Theme\",\n       x = \"Year\",\n       y = \"Total Count\") +\n  fill_palette(\"Dark2\")\n\nFor analysis, I will need to convert the data to long format. Since I want to avoid making it too big, I’ll do this separately for each key variable.\n\ntheory_long &lt;- df %&gt;% \n    pivot_longer(\n        cols = THEORY1:THEORY4,\n        names_to = \"THEORY_NAME\", \n        values_to = \"THEORY\"\n    ) \n\nmethod_long &lt;- df %&gt;% \n    pivot_longer(\n        cols = METHOD1:METHOD4,\n        names_to = \"METHODNAME\", \n        values_to = \"METHOD\"\n    ) \n\nlimit_long &lt;- df %&gt;% \n    pivot_longer(\n        cols = LIMIT1:LIMIT3,\n        names_to = \"LIMITNAME\", \n        values_to = \"LIMIT\"\n    ) \n\nfac_long &lt;- df %&gt;% \n    pivot_longer(\n        cols = F1:F9,\n        names_to = \"FACNAME\",\n        values_to = \"FAC\"\n    )\n\nfac_NS_long &lt;- df %&gt;% \n    pivot_longer(\n        cols = FNS1:FNS4,\n        names_to = \"FAC_NS_NAME\",\n        values_to = \"FAC_NS\"\n    )\n\nfactors_based_on_themes &lt;- df %&gt;% pivot_longer(\n    cols = F1_THEME:F9_THEME,\n    names_to = \"FAC_THEMES_NAMES\",\n    values_to = \"FACTHEME\"\n)\n\nRemove all the empty rows:\n\ntheory_long &lt;- theory_long %&gt;% filter(THEORY != \"\") #\nmethod_long &lt;- method_long %&gt;% filter(METHOD != \"\") #\nlimit_long &lt;- limit_long %&gt;% filter(LIMIT != \"\") #\nfac_long &lt;- fac_long %&gt;% filter(FAC != \"\")\nfac_NS_long &lt;- fac_NS_long %&gt;% filter(FAC_NS != \"\")\n\nAdd factor scores to the long factors and non-signficant factors’ data:\n\nfac_long &lt;- merge(fac_long, factor_scores, by = \"FAC\", all = T)\n\n\nfactor_scores &lt;- factor_scores %&gt;% mutate(FAC_NS = FAC) %&gt;% dplyr::select(FAC_NS,Score_Sig, Score_NOT_Sig)\nfac_NS_long &lt;- merge(fac_NS_long, factor_scores, by = \"FAC_NS\", all = T)\n\n\n\n\nNow, I want to explore the interactions between the following properties: themes, theories, methodologies, limitations, factors, years, research types, sample sizes, technologies, and non-significant factors. Some questions that can be answered from such an analysis are:\n\nAre there notable differences in the distribution of themes, theories, methodologies, limitations, factors, research types, sample sizes, technologies, and non-significant factors.across years?\nAre themes, theories, methodologies, limitations, factors, years, research types, sample sizes, and non-significant factors significantly associated with specific technologies?\nAre there significant differences in sample sizes across themes, theories, methodologies, limitations, factors, years, research types, technologies, and non-significant factors?\nDo research types vary significantly among different themes, theories, methodologies, limitations, factors, years, sample sizes, technologies, and non-significant factors?\nAre there significant differences in methods used across themes, theories, limitations, factors, years, research types, sample sizes, technologies, and non-significant factors?\nAre the significant factors identified notably different among themes, theories, methodologies, limitations, years, research types, sample sizes, technologies, and non-significant factors?\nAre the non-significant factors identified notably different among themes, theories, methodologies, limitations, factors, years, research types, sample sizes, and technologies?\n\nTo do this, I will first decide if further investigation is even worthwhile. First, I will use ANOVA to figure out if there are significant differences between groups of the same variable. That is, are themes, theories, methodologies, limitations, technologies, factors, years, research types, sample sizes, and non-significant factors actually different across the dataset?\n\nbuild_anova &lt;- function(nameOfCol){\n    counts_df &lt;- df %&gt;% count({{nameOfCol}}) %&gt;% arrange(desc(n))\n\n    counts_df_long &lt;- data.frame(\n        Group = rep(as.character(counts_df[[1]]), times = counts_df$n),\n        Value = unlist(lapply(counts_df$n, function(x) seq_len(x)))\n    )\n\n    anova_result &lt;- aov(Value ~ Group, data = counts_df_long)\n    return(summary(anova_result))\n}\n\nThemes are significantly different.\n\nbuild_anova(DecisionTheme)\n\nSo, let’s see how they differ across other factors - starting with the ones that do not require pivoting the dataframe! (Year, Tech, SampleSizeBin, ResearchType). This time, I will use a \\chi^2 test of independence.\n\nbuild_contingency_table &lt;- function(nameOfCol){\n    data_combine &lt;- df %&gt;% group_by(DecisionTheme) %&gt;% count({{nameOfCol}}) \n\n    contingency_table &lt;- xtabs(n ~ DecisionTheme + {{nameOfCol}}, data = data_combine)\n    chi_sq_result &lt;- chisq.test(contingency_table)\n    chi_sq_result\n}\n\n\n#build_contingency_table(SampleSizeBin)\n\nYou can also calculate the Cramer V:\n\ntable(is.na(df$SampleSizeBin))\n\n\n#cramerV(build_contingency_table(SampleSizeBin))",
    "crumbs": [
      "Home",
      "Projects",
      "Project 1. SLR",
      "Data Analysis"
    ]
  },
  {
    "objectID": "study1_DA.html#part-1.-data-cleaning-and-prep",
    "href": "study1_DA.html#part-1.-data-cleaning-and-prep",
    "title": "Data Analysis",
    "section": "",
    "text": "The R libraries used for data analysis are as follows:\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(purrr)\nlibrary(ggplot2)\nlibrary(psych)\nlibrary(tidyr)\nlibrary(stargazer)\nlibrary(forcats)\nlibrary(xtable)\nlibrary(ggraph)\nlibrary(igraph)\nlibrary(gt)\nlibrary(ggpubr)\nlibrary(ggcorrplot)\nlibrary(ggnewscale)\nlibrary(ggeffects)\nlibrary(ggeffects)\nlibrary(pheatmap)\nlibrary(gridExtra)\nlibrary(grid)\nlibrary(treemapify)\n\nLooking at the data:\n\ndf &lt;- read.csv(\"data/P2_AR_07.csv\") \nglimpse(df)\n\nRows: 143\nColumns: 62\n$ Reason_Theme       &lt;chr&gt; \"comparative\", \"behavioral intention\", \"keywords\", …\n$ CiteKey            &lt;chr&gt; \"LonkaniAcomparativeStudyOfTrust2020\", \"SaprikisACo…\n$ ID                 &lt;chr&gt; \"p2_01\", \"p2_02\", \"p2_03\", \"p2_04\", \"p2_05\", \"p2_06…\n$ Algo_Theme         &lt;chr&gt; \"psychological, cultural, demographic\", \"psychologi…\n$ DecisionTheme      &lt;chr&gt; \"cultural\", \"psychological\", \"psychological\", \"cult…\n$ Man_Theme          &lt;chr&gt; \"cultural, psychological\", \"social, psychological\",…\n$ Match              &lt;int&gt; 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ Title              &lt;chr&gt; \"a comparative study of trust in mobile banking: an…\n$ Year               &lt;int&gt; 2020, 2022, 2021, 2019, 2020, 2020, 2024, 2020, 201…\n$ PublicationTitle   &lt;chr&gt; \"journal of global information management\", \"inform…\n$ Creators           &lt;chr&gt; \"ravi lonkani, chuleeporn changchit, tim klaus, jom…\n$ Publisher          &lt;chr&gt; \"igi global\", \"mdpi\", \"igi global\", \"elsevier\", \"wi…\n$ AffiliationCountry &lt;chr&gt; \"thailand\", \"greece\", \"brazil, south korea, usa\", \"…\n$ K1                 &lt;chr&gt; \"technology acceptance model\", \"word-of-mouth\", \"em…\n$ K2                 &lt;chr&gt; \"initial trust\", \"mobile-banking\", \"consumer adopti…\n$ K3                 &lt;chr&gt; \"consumer acceptance\", \"acceptance model\", \"initial…\n$ K4                 &lt;chr&gt; \"gender-differences\", \"information-technology\", \"in…\n$ K5                 &lt;chr&gt; \"normative beliefs\", \"consumer adoption\", \"anxiety\"…\n$ K6                 &lt;chr&gt; \"national culture\", \"moderating role\", \"acceptance\"…\n$ K7                 &lt;chr&gt; \"usage intentions\", \"utaut model\", \"commerce\", \"emp…\n$ K8                 &lt;chr&gt; \"privacy concerns\", \"services\", \"traits\", \"acceptan…\n$ K9                 &lt;chr&gt; \"internet users\", \"internet\", \"usage\", \"mediating r…\n$ K10                &lt;chr&gt; \"online trust\", \"determinants\", \"satisfaction\", \"ad…\n$ Num_Factors        &lt;int&gt; 5, 8, 5, 6, 4, 6, 4, 5, 4, 4, 8, 4, 5, 3, 6, 8, 6, …\n$ F1                 &lt;chr&gt; \"cltr\", \"peex\", \"self\", \"habt\", \"attd\", \"pu\", \"prgt…\n$ F2                 &lt;chr&gt; \"norm_blf\", \"socinf\", \"trst\", \"psec\", \"intrac\", \"pe…\n$ F3                 &lt;chr&gt; \"prv_exp\", \"fac_cond\", \"peou\", \"ppriv\", \"cltr\", \"so…\n$ F4                 &lt;chr&gt; \"competnc\", \"risk\", \"pu\", \"trst\", \"innov\", \"fac_con…\n$ F5                 &lt;chr&gt; \"trst\", \"anxiety\", \"intnt_use\", \"peex\", \"\", \"trst\",…\n$ F6                 &lt;chr&gt; \"\", \"rwrd\", \"\", \"price\", \"\", \"prisk\", \"\", \"\", \"\", \"…\n$ F7                 &lt;chr&gt; \"\", \"sec\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"habt\", …\n$ F8                 &lt;chr&gt; \"\", \"recom\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"price…\n$ F9                 &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\",…\n$ F1_THEME           &lt;chr&gt; \"social\", \"personal\", \"psychological\", \"personal\", …\n$ F2_THEME           &lt;chr&gt; \"social\", \"social\", \"psychological\", \"perceptive\", …\n$ F3_THEME           &lt;chr&gt; \"personal\", \"external\", \"perceptive\", \"perceptive\",…\n$ F4_THEME           &lt;chr&gt; \"external\", \"external\", \"perceptive\", \"psychologica…\n$ F5_THEME           &lt;chr&gt; \"psychological\", \"psychological\", \"psychological\", …\n$ F6_THEME           &lt;chr&gt; \"\", \"external\", \"\", \"external\", \"\", \"perceptive\", \"…\n$ F7_THEME           &lt;chr&gt; \"\", \"external\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"pe…\n$ F8_THEME           &lt;chr&gt; \"\", \"external\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"ex…\n$ F9_THEME           &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\",…\n$ NUM_FAC_NOTSIG     &lt;int&gt; 2, 2, 0, 4, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, …\n$ FNS1               &lt;chr&gt; \"age\", \"efex\", \"\", \"peex\", \"\", \"\", \"\", \"\", \"\", \"soc…\n$ FNS2               &lt;chr&gt; \"sex\", \"fac_cond\", \"\", \"pval\", \"\", \"\", \"\", \"\", \"\", …\n$ FNS3               &lt;chr&gt; \"\", \"\", \"\", \"socinf\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"…\n$ FNS4               &lt;chr&gt; \"\", \"\", \"\", \"hed_motiv\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"…\n$ ResearchType       &lt;chr&gt; \"comparative\", \"empirical\", \"empirical\", \"empirical…\n$ SampleSize         &lt;int&gt; 560, 837, 458, 901, 1340, 755, 418, 203, 384, 127, …\n$ METHOD1            &lt;chr&gt; \"regression\", \"cfa\", \"cfa\", \"sem\", \"multigroup sem\"…\n$ METHOD2            &lt;chr&gt; \"\", \"sem\", \"sem\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"…\n$ METHOD3            &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\",…\n$ METHOD4            &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\",…\n$ Tech               &lt;chr&gt; \"mobile banking\", \"mobile banking\", \"mobile banking…\n$ THEORY1            &lt;chr&gt; \"hcd\", \"utaut\", \"tam\", \"utaut2\", \"tam\", \"tam\", \"dct…\n$ THEORY2            &lt;chr&gt; \"\", \"\", \"\", \"\", \"tpb\", \"\", \"\", \"\", \"tpb\", \"\", \"utau…\n$ THEORY3            &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\",…\n$ THEORY4            &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\",…\n$ LIMIT1             &lt;chr&gt; \"sample\", \"sample\", \"sample\", \"sample\", \"no_long\", …\n$ LIMIT2             &lt;chr&gt; \"\", \"cross_cult\", \"bias\", \"no_mod\", \"bias\", \"sample…\n$ LIMIT3             &lt;chr&gt; \"\", \"incomp_fac\", \"question\", \"incomp_fac\", \"no_mod…\n$ Abstract           &lt;chr&gt; \"with the rapid growth of mobile phone usage, mobil…\n\n\nSummary statiscs\n\npsych::describe(df %&gt;% \n    dplyr::select(Year, Match, Num_Factors, NUM_FAC_NOTSIG, SampleSize)) %&gt;% \n    dplyr::select(vars, n, mean, sd, median, min, max) \n\n               vars   n    mean      sd median  min   max\nYear              1 143 2021.18    1.84   2021 2018  2024\nMatch             2 143    0.94    0.23      1    0     1\nNum_Factors       3 143    5.31    1.60      5    2     9\nNUM_FAC_NOTSIG    4 143    0.46    0.87      0    0     4\nSampleSize        5 103  894.50 3189.24    384   26 25000\n\n\nWoah! One paper has 25,000 and that is messing up the sample sizes. Remembering this study’s ID:\n\ndf %&gt;% filter(SampleSize == 25000) %&gt;% \n    dplyr::select(ID, Title, SampleSize)\n\n     ID\n1 p2_77\n                                                                            Title\n1 financial literacy, behavioral traits, and epayment adoption and usage in japan\n  SampleSize\n1      25000\n\n\nSetting aside the study with sample size of 25,000:\n\npsych::describe(\n    df %&gt;% dplyr::select(Year, Match, Num_Factors, NUM_FAC_NOTSIG, SampleSize) %&gt;% \n    filter(SampleSize != 25000)) %&gt;% \n    dplyr::select(vars, n, mean, sd, median, min, max) \n\n               vars   n    mean      sd median  min   max\nYear              1 102 2021.14    1.86   2021 2018  2024\nMatch             2 102    0.94    0.24      1    0     1\nNum_Factors       3 102    5.15    1.65      5    2     9\nNUM_FAC_NOTSIG    4 102    0.54    0.95      0    0     4\nSampleSize        5 102  658.18 2112.42    384   26 21526\n\n\nWelp! Another large study.\n\ndf %&gt;% filter(SampleSize == 21526) %&gt;% \n    dplyr::select(ID, Title, SampleSize)\n\n     ID\n1 p2_59\n                                                                                                                                                                     Title\n1 exploring mobile banking adoption and service quality features through user-generated content: the application of a topic modeling approach to google play store reviews\n  SampleSize\n1      21526\n\n\nSetting aside the study with sample size of 25,000 and the one with 21,52 as they are outliers:\n\npsych::describe(\n    df %&gt;% filter(!ID %in% c('p2_59','p2_77')) %&gt;% \n    dplyr::select(ID, Year,Match, Num_Factors, NUM_FAC_NOTSIG, SampleSize)) %&gt;% \n    dplyr::select(vars, n, mean, sd, median, min, max) \n\n               vars   n    mean     sd median  min  max\nID*               1 141   71.00  40.85     71    1  141\nYear              2 141 2021.16   1.85   2021 2018 2024\nMatch             3 141    0.94   0.23      1    0    1\nNum_Factors       4 141    5.33   1.60      5    2    9\nNUM_FAC_NOTSIG    5 141    0.47   0.87      0    0    4\nSampleSize        6 101  451.56 330.40    384   26 2202\n\n\nCounting the unique values for each of the columns:\n\nresults &lt;- c(\n  paste('Number of Unique Values in ID: ', n_distinct(df$ID)),\n  paste('Number of Unique Values in Title: ', n_distinct(df$Title)),\n  paste('Number of Unique Values in PublicationTitles: ', n_distinct(df$PublicationTitle)),\n  paste('Number of Unique Values in Publisher: ', n_distinct(df$Publisher)),\n  paste('Number of Unique Values in AffiliationCountry: ', n_distinct(df$AffiliationCountry)),\n  paste('Number of Unique Values in Factors: ', dplyr::n_distinct(df %&gt;% dplyr::select(F1:F9) %&gt;% unlist())),\n  paste('Number of Unique Values in Not Sig: ', dplyr::n_distinct(df %&gt;% dplyr::select(FNS1:FNS4) %&gt;% unlist())),\n  paste('Number of Unique Values in Methods: ', dplyr::n_distinct(df %&gt;% dplyr::select(METHOD1:METHOD4) %&gt;% unlist())),\n  paste('Number of Unique Values in Theory: ', dplyr::n_distinct(df %&gt;% dplyr::select(THEORY1:THEORY4) %&gt;% unlist())),\n  paste('Number of Unique Values in Limits: ', dplyr::n_distinct(df %&gt;% dplyr::select(LIMIT1:LIMIT3) %&gt;% unlist())),\n  paste('Number of Unique Values in ResearchType: ', n_distinct(df$ResearchType)),\n  paste('Number of Unique Values in Authors: ', n_distinct(df$Creators)),\n  paste('Number of Unique Values in Keywords: ', dplyr::n_distinct(df %&gt;% dplyr::select(K1:K10) %&gt;% unlist())),\n  paste('Number of Unique Values in Tech: ', n_distinct(df$Tech)),\n  paste('Number of Unique Values in Themes: ', n_distinct(df$DecisionTheme))\n)\n\ncat(results, sep = \"\\n\")\n\nNumber of Unique Values in ID:  143\nNumber of Unique Values in Title:  143\nNumber of Unique Values in PublicationTitles:  54\nNumber of Unique Values in Publisher:  15\nNumber of Unique Values in AffiliationCountry:  43\nNumber of Unique Values in Factors:  233\nNumber of Unique Values in Not Sig:  32\nNumber of Unique Values in Methods:  42\nNumber of Unique Values in Theory:  44\nNumber of Unique Values in Limits:  16\nNumber of Unique Values in ResearchType:  16\nNumber of Unique Values in Authors:  137\nNumber of Unique Values in Keywords:  413\nNumber of Unique Values in Tech:  9\nNumber of Unique Values in Themes:  7\n\n\nChecking the sample sizes Without the outliers:\n\npsych::describe(\n    df %&gt;% filter(!ID %in% c('p2_59','p2_77')) %&gt;% \n    dplyr::select(SampleSize)) %&gt;% \n    dplyr::select(n, mean, sd, median, min, max) \n\n             n   mean    sd median min  max\nSampleSize 101 451.56 330.4    384  26 2202\n\n\n\nnoOutliers &lt;- df %&gt;% filter(!ID %in% c('p2_59','p2_77'))\n\nquantiles &lt;- quantile(noOutliers$SampleSize, na.rm = T)\n\nquantile_binned &lt;- cut(df$SampleSize, \n                breaks = quantiles, \n                labels = c(\"SQ1\", \"SQ2\", \"SQ3\", \"SQ4\"), \n                include.lowest = TRUE)\n\ndf$SampleSizeBin &lt;- quantile_binned\n\ndf &lt;- df %&gt;% mutate(\n    SampleSizeBin = if_else(\n        is.na(SampleSizeBin),\n        \"NotStated\",\n        SampleSizeBin\n    )\n)\n\ndf %&gt;% count(SampleSizeBin)\n\n  SampleSizeBin  n\n1     NotStated 42\n2           SQ1 26\n3           SQ2 28\n4           SQ3 23\n5           SQ4 24\n\n\nLet’s calculate the scores for factors that are significant and non-significant:\n\nF_counts &lt;- df %&gt;%\n  dplyr::select(F1:F9) %&gt;%\n  unlist() %&gt;%\n  table() %&gt;%\n  as.data.frame() %&gt;%\n  rename(FAC = \".\", F_count = \"Freq\")\n\nFNS_counts &lt;- df %&gt;%\n  dplyr::select(FNS1:FNS4) %&gt;%\n  unlist() %&gt;%\n  table() %&gt;%\n  as.data.frame() %&gt;%\n  rename(FAC = \".\", FNS_counts = \"Freq\")\n\n# Count occurrences of each factor in all columns (F1 to F9 + FNS1 to FNS4)\nTotal_counts &lt;- df %&gt;%\n  dplyr::select(c(F1:F9, FNS1:FNS4)) %&gt;%\n  unlist() %&gt;%\n  table() %&gt;%\n  as.data.frame() %&gt;%\n  rename(FAC = \".\", Total_count = \"Freq\")\n\n# Merge the two count tables\nfactor_scores &lt;- merge(F_counts,FNS_counts, by = \"FAC\", all = TRUE)\nfactor_scores &lt;- merge(factor_scores, Total_counts, by = \"FAC\", all = TRUE)\n\n\n# Replace NAs with 0 for cases where factors appear in some but not all sections\nfactor_scores[is.na(factor_scores)] &lt;- 0\n\nfactor_scores &lt;- factor_scores %&gt;%\n  mutate(Score_Sig = round(F_count / Total_count, 2),\n         Score_NOT_Sig = round(FNS_counts / Total_count, 2)) %&gt;% filter(FAC != \"\")\n\n\nhead(factor_scores)\n\n        FAC F_count FNS_counts Total_count Score_Sig Score_NOT_Sig\n1       acc       1          0           1       1.0           0.0\n2  acc_conv       2          0           2       1.0           0.0\n3 accbility       3          0           3       1.0           0.0\n4       age       3          3           6       0.5           0.5\n5    agrbns       1          0           1       1.0           0.0\n6      alts       1          0           1       1.0           0.0",
    "crumbs": [
      "Home",
      "Projects",
      "Project 1. SLR",
      "Data Analysis"
    ]
  },
  {
    "objectID": "study1_DA.html#part-2.-analysis",
    "href": "study1_DA.html#part-2.-analysis",
    "title": "Data Analysis",
    "section": "Part 2. Analysis",
    "text": "Part 2. Analysis\nNow let’s actually do some analysis. Let’s visualize how the themes of the papers have changed across the years. I will first generate a bar plot that fills the bars at each year (as a categorical factor) with proportions of themes in that year. This is an aggregation that happens under the hood, and using position = \"fill\" will actually make sure all the bars consider things relative to eachother, filling the full 100% of the bar.\n\nggplot(df, aes(x = as.factor(Year), fill = DecisionTheme)) +\n  geom_bar(position = \"fill\") +\n  theme_minimal() +\n  labs(fill = \"Theme\",\n       x = \"Year\",\n       y = \"Total Count\") +\n  fill_palette(\"Set3\")\n\n\n\n\n\n\n\n\nTo see how things move/flow over the years, a line chart is a great idea:\n\ndf %&gt;%\n    dplyr::count(DecisionTheme, Year) %&gt;%\n    ggplot(aes(x = as.factor(Year), y = n, color = DecisionTheme, group = DecisionTheme)) +\n  geom_line() +\n  geom_point() +\n  theme_minimal() +\n  labs(fill = \"Theme\",\n       x = \"Year\",\n       y = \"Total Count\") +\n  fill_palette(\"Dark2\")\n\n\n\n\n\n\n\n\nFor analysis, I will need to convert the data to long format. Since I want to avoid making it too big, I’ll do this separately for each key variable.\n\ntheory_long &lt;- df %&gt;% \n    pivot_longer(\n        cols = THEORY1:THEORY4,\n        names_to = \"THEORY_NAME\", \n        values_to = \"THEORY\"\n    ) \n\nmethod_long &lt;- df %&gt;% \n    pivot_longer(\n        cols = METHOD1:METHOD4,\n        names_to = \"METHODNAME\", \n        values_to = \"METHOD\"\n    ) \n\nlimit_long &lt;- df %&gt;% \n    pivot_longer(\n        cols = LIMIT1:LIMIT3,\n        names_to = \"LIMITNAME\", \n        values_to = \"LIMIT\"\n    ) \n\nfac_long &lt;- df %&gt;% \n    pivot_longer(\n        cols = F1:F9,\n        names_to = \"FACNAME\",\n        values_to = \"FAC\"\n    )\n\nfac_NS_long &lt;- df %&gt;% \n    pivot_longer(\n        cols = FNS1:FNS4,\n        names_to = \"FAC_NS_NAME\",\n        values_to = \"FAC_NS\"\n    )\n\nfactors_based_on_themes &lt;- df %&gt;% pivot_longer(\n    cols = F1_THEME:F9_THEME,\n    names_to = \"FAC_THEMES_NAMES\",\n    values_to = \"FACTHEME\"\n)\n\nRemove all the empty rows:\n\ntheory_long &lt;- theory_long %&gt;% filter(THEORY != \"\") #\nmethod_long &lt;- method_long %&gt;% filter(METHOD != \"\") #\nlimit_long &lt;- limit_long %&gt;% filter(LIMIT != \"\") #\nfac_long &lt;- fac_long %&gt;% filter(FAC != \"\")\nfac_NS_long &lt;- fac_NS_long %&gt;% filter(FAC_NS != \"\")\n\nAdd factor scores to the long factors and non-signficant factors’ data:\n\nfac_long &lt;- merge(fac_long, factor_scores, by = \"FAC\", all = T)\n\n\nfactor_scores &lt;- factor_scores %&gt;% mutate(FAC_NS = FAC) %&gt;% dplyr::select(FAC_NS,Score_Sig, Score_NOT_Sig)\nfac_NS_long &lt;- merge(fac_NS_long, factor_scores, by = \"FAC_NS\", all = T)",
    "crumbs": [
      "Home",
      "Projects",
      "Project 1. SLR",
      "Data Analysis"
    ]
  },
  {
    "objectID": "study1_DA.html#part-3-statistical-analysis",
    "href": "study1_DA.html#part-3-statistical-analysis",
    "title": "Data Analysis",
    "section": "Part 3 Statistical Analysis",
    "text": "Part 3 Statistical Analysis\nNow, I want to explore the interactions between the following properties: themes, theories, methodologies, limitations, factors, years, research types, sample sizes, technologies, and non-significant factors. Some questions that can be answered from such an analysis are:\n\nAre there notable differences in the distribution of themes, theories, methodologies, limitations, factors, research types, sample sizes, technologies, and non-significant factors.across years?\nAre themes, theories, methodologies, limitations, factors, years, research types, sample sizes, and non-significant factors significantly associated with specific technologies?\nAre there significant differences in sample sizes across themes, theories, methodologies, limitations, factors, years, research types, technologies, and non-significant factors?\nDo research types vary significantly among different themes, theories, methodologies, limitations, factors, years, sample sizes, technologies, and non-significant factors?\nAre there significant differences in methods used across themes, theories, limitations, factors, years, research types, sample sizes, technologies, and non-significant factors?\nAre the significant factors identified notably different among themes, theories, methodologies, limitations, years, research types, sample sizes, technologies, and non-significant factors?\nAre the non-significant factors identified notably different among themes, theories, methodologies, limitations, factors, years, research types, sample sizes, and technologies?\n\nTo do this, I will first decide if further investigation is even worthwhile. First, I will use ANOVA to figure out if there are significant differences between groups of the same variable. That is, are themes, theories, methodologies, limitations, technologies, factors, years, research types, sample sizes, and non-significant factors actually different across the dataset?\n\nbuild_anova &lt;- function(nameOfCol){\n    counts_df &lt;- df %&gt;% count({{nameOfCol}}) %&gt;% arrange(desc(n))\n\n    counts_df_long &lt;- data.frame(\n        Group = rep(as.character(counts_df[[1]]), times = counts_df$n),\n        Value = unlist(lapply(counts_df$n, function(x) seq_len(x)))\n    )\n\n    anova_result &lt;- aov(Value ~ Group, data = counts_df_long)\n    return(summary(anova_result))\n}\n\n\nPaper Themes\nThemes are significantly different.\n\nbuild_anova(DecisionTheme)\n\n             Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nGroup         6   7489  1248.2   15.05 3.69e-13 ***\nResiduals   136  11280    82.9                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nSo, let’s see how they differ across other factors - starting with the ones that do not require pivoting the dataframe! (Year, Tech, SampleSizeBin, ResearchType). This time, I will use a \\chi^2 test of independence.\n\ncombine_data &lt;- function(nameOfCol){\n    data_combine &lt;- df %&gt;% \n    group_by(DecisionTheme) %&gt;% \n    count({{nameOfCol}}) \n\n    return(data_combine)\n}\n\n\nchisq.test(\n    xtabs(n ~ DecisionTheme + Year, \n         data = combine_data(Year))\n)\n\n\n    Pearson's Chi-squared test\n\ndata:  xtabs(n ~ DecisionTheme + Year, data = combine_data(Year))\nX-squared = 44.414, df = 36, p-value = 0.1585\n\n\nI do this for SampleSizeBin, ResearchType, and Tech too. The results are:\n\n\n\nVariable\n\\chi^2\ndf\np-value\n\n\n\n\nYear\n44.414\n36\n0.159\n\n\nTech\n102.59\n48\n0.002^{**}\n\n\nSampleSizeBin\n8.6582\n18\n0.967\n\n\nResearchType\n127.06\n90\n0.006^{**}\n\n\n\nFor the variables that have multiple columns, I first pivot the dataframe and then do the test. For example, this is how I do it for non-significant factors:\n\ncombine_data_longFormat &lt;- function(startCol, stopCol){\n    longlong &lt;- df %&gt;% pivot_longer(\n        cols = {{startCol}}:{{stopCol}},\n        names_to = \"NAMES\",\n        values_to = \"THINGS\") %&gt;% \n    filter(THINGS != \"\" & THINGS != \" \") # no null values \n\n    combined_df &lt;- longlong %&gt;% \n        group_by(DecisionTheme) %&gt;%\n        count(THINGS) \n\n    return(combined_df)\n}\n\ncombined_df &lt;- combine_data_longFormat(FNS1, FNS4)\nchisq.test(xtabs(n ~ DecisionTheme + THINGS,\n           data = combined_df))\n\n\n    Pearson's Chi-squared test\n\ndata:  xtabs(n ~ DecisionTheme + THINGS, data = combined_df)\nX-squared = 207.87, df = 180, p-value = 0.0758\n\n\nDoing this for Theory, Methods, Limitatiosn, Factors, as well as factor Themes I have:\n\n\n\nVariable\n\\chi^2\ndf\np-value\n\n\n\n\nTheories\n251.84\n252\n0.491\n\n\nMethods\n272.02\n240\n0.076\n\n\nLimits\n89.327\n84\n0.325\n\n\nFactors\n1461.4\n1386\n0.078\n\n\nFactorThemes\n139.26\n36\n&lt; 0.001^{***}\n\n\nNS.Factors\n207.87\n180\n0.076\n\n\n\nWhich basically means only technology, research type and factor themes are kind of related to overall paper themes. Let’s dig deeper.\n\nPaper Themes vs Technology\nI first make sure the rows where Tech is empty are filled with mobile banking (as the most frequent technology, because the papers were specifically m-banking related).\n\ndf %&gt;% group_by(DecisionTheme) %&gt;% \n    mutate(Tech = ifelse(\n        Tech == \"\",\n        \"mobile banking\",\n        Tech\n    )) %&gt;% count(Tech) %&gt;% ggplot(aes(x = as.factor(DecisionTheme), fill = Tech)) +\n  geom_bar(position = \"fill\") +\n  theme_minimal() +\n  labs(fill = \"Tech\",\n       x = \"Themes\",\n       y = \" \") +\n  fill_palette(\"Set3\") + \n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1, face = \"bold\"), \n  ) \n\n\n\n\n\n\n\n\nMarket studies cover the broadest range of technologies. This is because studies on variations of m-banking like m-wallets and smart wearables fall under the Market category, influencing the theme distribution across technologies. To confirm that this effect is driven by Market studies, I exclude them and rerun the above test:\n\ntemp_combine_data &lt;- df %&gt;% \n    filter(DecisionTheme != 'market') %&gt;% \n    group_by(DecisionTheme) %&gt;% \n    count(Tech) \n\nchisq.test(xtabs(\n    n ~ DecisionTheme + Tech,\n    data = temp_combine_data\n))\n\n\n    Pearson's Chi-squared test\n\ndata:  xtabs(n ~ DecisionTheme + Tech, data = temp_combine_data)\nX-squared = 19.629, df = 20, p-value = 0.4813\n\n\nThe p-value of 0.481 means we cannot reject the null hypothesis of \\chi^2 test of independence. That means, Paper Themes and Technology studied are independent when we set aside Market studies. How about research type? First, a little house keeping for the data with respect to research type: I simply rename a few columns for easier visualization.\n\n\nPaper Themes vs Research Type\n\ndf %&gt;% group_by(DecisionTheme) %&gt;% \n    mutate(\n        ResearchType = case_when(\n               ResearchType == \"survey quantitative\" ~ \"survey analysis\",\n               ResearchType == \"survey, empirical\" ~ \"survey analysis\",\n               ResearchType == \"survey\" ~ \"survey analysis\",\n               ResearchType == \"conceptual qualitative\" ~ \"conceptual\",\n               ResearchType == \"mixed method qualquant\" ~ \"mixed method\",\n               .default = ResearchType\n    )) %&gt;% \n    filter(!is.na(ResearchType) & ResearchType != \"\") %&gt;% \n    count(ResearchType) %&gt;% \n    ggplot(aes(x = as.factor(DecisionTheme), fill = ResearchType)) +\n          geom_bar(position = \"fill\") +\n          theme_minimal() +\n          labs(fill = \"Research Type\",\n               x = \"Themes\",\n               y = \" \") +\n          fill_palette(\"Set3\") + \n          theme(\n            axis.text.x = element_text(angle = 45, hjust = 1, face = \"bold\"), \n          )\n\n\n\n\n\n\n\n\nPatterns between research types and themes are less distinct. Market and Personal studies show the least research type diversity, while Psychological studies exhibit the most. Comparative research appears only in Cultural and Demographic studies. For themes of factors, I will use a different type of visualization (not filling the bars, but wanting to see the difference separately):\n\n\nPaper Themes vs Sig/Non-Significant Factors\n\nlonglong &lt;- df %&gt;% pivot_longer(\n        cols = F1_THEME:F9_THEME,\n        names_to = \"NAMES\",\n        values_to = \"THINGS\") %&gt;% \n        # don't include external variables \n    filter(THINGS != \"\" & THINGS != \" \" & THINGS != \"external\") %&gt;% # these changes are for visualization \n    mutate(THINGS = str_to_title(THINGS), DecisionTheme = str_to_title(DecisionTheme))\n\nlonglongGroup &lt;- longlong %&gt;% group_by(DecisionTheme) %&gt;% count(THINGS) %&gt;% arrange(desc(n))\n\nggplot(longlongGroup, aes(x = as.factor(DecisionTheme), y = n, fill = as.factor(THINGS))) +\n  geom_bar(stat = \"identity\", position = \"dodge\", color = \"black\") +\n  labs(fill = \"Theme of Factors\", x = \"Theme\", y = \"Proportions\") +\n  fill_palette(\"Accent\") + \n  theme(axis.text.y = element_text(angle = 0, hjust = 1, face = \"bold\"),\n        axis.text.x = element_text(angle = 45, hjust = 1, face = \"bold\")) \n\n\n\n\n\n\n\n\nMarket studies contain the most diverse set of factors influencing m-banking, likely due to device differences. This suggests that variations of the same technology (e.g., m-wallets, m-payment devices) introduce different influencing factors. A comparative study between devices could provide valuable insights. Market-specific factors appear only in Social and Perceptive themes (aside from Market itself). This suggests that Social and Perceptive studies are the only studies that are considering device differences. Therefore, future research should expand the understanding of device differences in m-banking adoption by conducting thematically different studies. Cultural and Personal themes have the least diverse factor sets. Within Cultural studies, personal and perceptive factors dominate, indicating that cultural influences shape individual perceptions (opinions) and values (personal factors). In personal studies, social factors are overshadowed by others, suggesting that individual preferences take precedence over group opinions. Most studies on individual differences do not consider societal influences, signaling an unexplored area for future research. Although no significant relationship exists between factors and themes, it is still useful to look at some patterns.\nOk, I’m not interested in seeing if there are differences in how many times factors appear significant and non-significant across themes. First, let’s merge the long data on factor and non-significant factors with factor_scores datasets. The factor scores were the counts of how many times each factor appeared across the literature as significant/non-significant.\n\nlonglong_fac &lt;- df %&gt;% pivot_longer(\n    cols = F1:F9,\n    names_to = \"NAMES\",\n    values_to = \"FAC_NS\"\n) %&gt;% filter(FAC_NS != \"\" & FAC_NS != \" \" & FAC_NS != \"external\")\n\nlonglong_nonsigfac &lt;- df %&gt;% pivot_longer(\n    cols = FNS1:FNS4,\n    names_to = \"NAMES\",\n    values_to = \"FAC_NS\"\n) %&gt;% filter(FAC_NS != \"\" & FAC_NS != \" \" & FAC_NS != \"external\") \n\ndf_merged_fac &lt;- longlong_fac %&gt;% left_join(factor_scores, by = \"FAC_NS\")\ndf_merged_nonsigfac &lt;- longlong_nonsigfac %&gt;% left_join(factor_scores, by = \"FAC_NS\")\n\nThe significance score across themes:\n\nggplot(df_merged_fac, \n    aes(x = DecisionTheme, y = Score_Sig, \n        fill = DecisionTheme)) +\n  geom_boxplot() +\n  labs(x = \"Theme\",\n       y = \"Factor's Significance Score\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1,\n   face = \"bold\"))\n\n\n\n\n\n\n\n\nMost themes have a high median significant score. The median for all themes is close to 1.0, meaning most factors are frequently significant, at least relative to their total occurrences. Variation exists, but it’s minor. Some themes (e.g., market, cultural and demographic) have slightly lower median than others. The IQR is small, indicating that most factor scores are clustered near the top. Some data points fall below 0.5, suggesting a few factors appear non-significant more so than they are significant. These outliers might indicate less relevant factors for certain themes. Since most medians are high and IQR’s are similar, no theme strongly dominates in terms of factor scores. Market studies might have slightly lower scores than others, indicating device and technology specific factors may be needed. I can also find out which factors have a low significance score for each theme:\n\nhead(df_merged_fac %&gt;% filter(Score_Sig &lt;= 0.5) %&gt;% dplyr::select(FAC_NS, DecisionTheme))\n\n# A tibble: 6 × 2\n  FAC_NS  DecisionTheme\n  &lt;chr&gt;   &lt;chr&gt;        \n1 age     psychological\n2 soc_val demographic  \n3 edu     market       \n4 trial   social       \n5 age     demographic  \n6 edu     demographic  \n\n\nThe outliers (Score_Sig \\leq 0.5) and the related theme they appeared as an outlier in are: Age (Psychological), Education (Market), Triability (Social), Hedonic and Social Value (Perceptive), and Age, Education and Social Value (Demographic). The reason for these outliers are:\n\nthese factors either do not appear frequently in those themes, or\nthey do appear, but they are not frequently found to be significant.\n\nFor example, I expected age and education to appear in many Demographic studies. Therefore, it is surprising that these factors have low scores. This indicates that age and education are either not frequently used in Demographic studies or they are frequently found to be non-significant variables in m-banking adoption.\nThe non-significance score across themes:\n\nggplot(df_merged_nonsigfac,\n    aes(x = DecisionTheme, y = Score_NOT_Sig, \n        fill = DecisionTheme)) +\n  geom_boxplot() +\n  labs(x = \"Theme\",\n       y = \"Factor's Non Significance Score\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1, \n  face = \"bold\"))\n\n\n\n\n\n\n\n\nPerceptive has the widest range of non-significant scores, with a high median (around 0.75). Demographic also has a relatively high median, meaning many factors in demographic-related studies were found to be non-significant. Some themes (e.g., Perceptive and Demographic) show high variability in non-significant factor scores, while some report very low variability. The presence of outliers suggests that certain studies had unusually high numbers of non-significant factors. The outliers are factors with non-significant scores of higher than 0.5 (appearing more than half of their total frequency as non-significant).\n\nhead(df_merged_nonsigfac %&gt;% filter(Score_NOT_Sig &gt; 0.5) %&gt;% dplyr::select(FAC_NS, DecisionTheme))\n\n# A tibble: 6 × 2\n  FAC_NS   DecisionTheme\n  &lt;chr&gt;    &lt;chr&gt;        \n1 soc_val  demographic  \n2 cond_val demographic  \n3 trial    demographic  \n4 soc_val  demographic  \n5 pu_atm   perceptive   \n6 soc_val  psychological\n\n\nThese are: Social Value, Conditional Value, Triability (Demographic), Perceived Usefulness of ATM devices, Search Convenience and Evaluation Convenience (Perceptive), Social Value and Hedonic Experience (Psychological), Triability (Personal) and Convenience barriers (Market).\nAn interesting observation is social values being an outlier in demographic studies for both significance and non-significane score.\n\nglimpse(df %&gt;% filter(F1 == \"soc_val\" | F2 == \"soc_val\" | F3 == \"soc_val\" | F4 == \"soc_val\" | F5 == \"soc_val\" | F6 == \"soc_val\" | F7 == \"soc_val\" | F8 == \"soc_val\" | F9 == \"soc_val\"))\n\nRows: 2\nColumns: 63\n$ Reason_Theme       &lt;chr&gt; \"In agreement\", \"\"\n$ CiteKey            &lt;chr&gt; \"\", \"\"\n$ ID                 &lt;chr&gt; \"p2_37\", \"p2_143\"\n$ Algo_Theme         &lt;chr&gt; \"psychological, market, perceptive\", \"perceptive, p…\n$ DecisionTheme      &lt;chr&gt; \"demographic\", \"perceptive\"\n$ Man_Theme          &lt;chr&gt; \"personal, demographic\", \"perceptive\"\n$ Match              &lt;int&gt; 0, 1\n$ Title              &lt;chr&gt; \"consumption values and mobile banking services: un…\n$ Year               &lt;int&gt; 2021, 2024\n$ PublicationTitle   &lt;chr&gt; \"international journal of bank marketing\", \"spanish…\n$ Creators           &lt;chr&gt; \"heikki karjaluoto, richard glavee-geo, dineshwar r…\n$ Publisher          &lt;chr&gt; \"emerald\", \"emerald\"\n$ AffiliationCountry &lt;chr&gt; \"mauritius\", \"spain, macedonia\"\n$ K1                 &lt;chr&gt; \"technology acceptance model\", \"perceived value\"\n$ K2                 &lt;chr&gt; \"perceived value\", \"mobile\"\n$ K3                 &lt;chr&gt; \"discriminant validity\", \"services\"\n$ K4                 &lt;chr&gt; \"behavioral intention\", \"adoption\"\n$ K5                 &lt;chr&gt; \"initial trust\", \"acceptance\"\n$ K6                 &lt;chr&gt; \"social value\", \"commerce\"\n$ K7                 &lt;chr&gt; \"method bias\", \"drives\"\n$ K8                 &lt;chr&gt; \"adoption\", NA\n$ K9                 &lt;chr&gt; \"recommendations\", NA\n$ K10                &lt;chr&gt; \"identification\", NA\n$ Num_Factors        &lt;int&gt; 4, 5\n$ F1                 &lt;chr&gt; \"fun_val\", \"util_val\"\n$ F2                 &lt;chr&gt; \"epi_val\", \"hed_val\"\n$ F3                 &lt;chr&gt; \"emo_val\", \"soc_val\"\n$ F4                 &lt;chr&gt; \"soc_val\", \"epi_val\"\n$ F5                 &lt;chr&gt; \"\", \"pval\"\n$ F6                 &lt;chr&gt; \"\", \"\"\n$ F7                 &lt;chr&gt; \"\", \"\"\n$ F8                 &lt;chr&gt; \"\", \"\"\n$ F9                 &lt;chr&gt; \"\", \"\"\n$ F1_THEME           &lt;chr&gt; \"personal\", \"personal\"\n$ F2_THEME           &lt;chr&gt; \"personal\", \"personal\"\n$ F3_THEME           &lt;chr&gt; \"psychological\", \"social\"\n$ F4_THEME           &lt;chr&gt; \"social\", \"personal\"\n$ F5_THEME           &lt;chr&gt; \"\", \"perceptive\"\n$ F6_THEME           &lt;chr&gt; \"\", \"\"\n$ F7_THEME           &lt;chr&gt; \"\", \"\"\n$ F8_THEME           &lt;chr&gt; \"\", \"\"\n$ F9_THEME           &lt;chr&gt; \"\", \"\"\n$ NUM_FAC_NOTSIG     &lt;int&gt; 1, 1\n$ FNS1               &lt;chr&gt; \"soc_val\", \"hed_val\"\n$ FNS2               &lt;chr&gt; \"\", \"\"\n$ FNS3               &lt;chr&gt; \"\", \"\"\n$ FNS4               &lt;chr&gt; \"\", \"\"\n$ ResearchType       &lt;chr&gt; \"quantitative\", \"quantitative\"\n$ SampleSize         &lt;int&gt; 246, 252\n$ METHOD1            &lt;chr&gt; \"pls sem\", \"pls sem\"\n$ METHOD2            &lt;chr&gt; \"\", \"\"\n$ METHOD3            &lt;chr&gt; \"\", \"\"\n$ METHOD4            &lt;chr&gt; \"\", \"\"\n$ Tech               &lt;chr&gt; \"mobile banking\", \"mobile banking\"\n$ THEORY1            &lt;chr&gt; \"tcv\", \"sor\"\n$ THEORY2            &lt;chr&gt; \"\", \"tcv\"\n$ THEORY3            &lt;chr&gt; \"\", \"\"\n$ THEORY4            &lt;chr&gt; \"\", \"\"\n$ LIMIT1             &lt;chr&gt; \"sample_size\", \"loc_specific\"\n$ LIMIT2             &lt;chr&gt; \"loc_specific\", \"\"\n$ LIMIT3             &lt;chr&gt; \"\", \"\"\n$ Abstract           &lt;chr&gt; \"purpose this study develops a theoretical model of…\n$ SampleSizeBin      &lt;chr&gt; \"SQ1\", \"SQ1\"\n\n\nThis is because of the study of Karjaluoto @karjaluotoConsumptionValuesMobile2021b where they found that social values did not influence trust in m-banking directly. However, social value is a significant moderator between functional value and intention to m-bank. Other insights from this analysis are: Perceptive studies may either not be reliable or may be useful to frameworks to assess the significance of factors in. Since another reason for low scores of factors are due to them not appearing as frequently across the literature, we also highlight several of these overlooked factors across themes. Identifying these gaps can provide valuable insights for future research and broaden the understanding of m-banking adoption.\nI can test whether certain themes are more likely to have non-significant factors:\n\nkruskal.test(Score_NOT_Sig ~ DecisionTheme, data = df_merged_nonsigfac)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  Score_NOT_Sig by DecisionTheme\nKruskal-Wallis chi-squared = 15.829, df = 6, p-value = 0.0147\n\n\nIf p &lt; 0.05, it confirms that certain themes are more likely to have non-significant factors.\nI can also identify which specific factors within perceptive or demographic themes are most non-significant. Check interaction effects (e.g., Does the study’s research method influence significance?).\n\nlonglongfactors &lt;- df %&gt;% pivot_longer(\n    cols = F1:F9,\n    names_to = \"NAMES\",\n    values_to = \"FAC\"\n) %&gt;% filter(FAC != \"\" & FAC != \" \")\n\nprint(\"Unique Decision Themes: \\n\") \n\n[1] \"Unique Decision Themes: \\n\"\n\nunique(df$DecisionTheme)\n\n[1] \"cultural\"      \"psychological\" \"demographic\"   \"perceptive\"   \n[5] \"social\"        \"market\"        \"personal\"     \n\n\n\nhead(longlongfactors %&gt;% \n    group_by(DecisionTheme) %&gt;%\n    count(FAC) %&gt;% arrange(desc(n)) %&gt;% \n    filter(DecisionTheme == \"personal\"))\n\n# A tibble: 6 × 3\n# Groups:   DecisionTheme [1]\n  DecisionTheme FAC           n\n  &lt;chr&gt;         &lt;chr&gt;     &lt;int&gt;\n1 personal      peou          4\n2 personal      pu            4\n3 personal      efex          3\n4 personal      habt          3\n5 personal      hed_motiv     3\n6 personal      peex          3\n\n\nIn the paper, I actually wanted to find the factors that appeared the least number of times. So, just take out the desc() arrangement and then look for the papers that have used certain factors (only once). For example, agreeableness was used only once. Let’s see which personal-themed paper used it:\n\ndf %&gt;% pivot_longer(\n    cols = F1:F9,\n    names_to = \"NAMES\",\n    values_to = \"THINGS\") %&gt;%\n    filter(DecisionTheme == \"personal\" &\n           THINGS == \"agrbns\") %&gt;%\n    dplyr::select(ID)\n\n# A tibble: 1 × 1\n  ID    \n  &lt;chr&gt; \n1 p2_106\n\n\nThis was then referenced in the paper I wrote.\nThe same entire process was repeated for Theory, Methods, Limitations, Factors and Non-significant Factors.\n\n\n\nFoundational Theories\n\ncounts_dflong &lt;- theory_long %&gt;% \n        filter(THEORY != \"\" & THEORY != \" \"\n            & !is.na(THEORY)) %&gt;% \n        count(THEORY) %&gt;% arrange(desc(n))\n    \ncounts_df_long &lt;- data.frame(\n    Group = rep(counts_dflong$THEORY, times = counts_dflong$n),\n    Value = unlist(lapply(counts_dflong$n, function(x) seq_len(x))))\n    \nanova_result &lt;- aov(Value ~ Group, data = counts_df_long)\nsummary(anova_result)\n\n             Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nGroup        42  18420   438.6   3.632 1.82e-08 ***\nResiduals   120  14492   120.8                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nGreat, There’s statistically significant differences in the distribution of Theories in the data. I follow the same idea above, except this time, use the theory_long in building the combined data. For example, for Year:\n\nbuild_combine_long &lt;- function(df, colName1, colName2){\n    data_combine &lt;- df %&gt;% group_by({{colName1}}) %&gt;% count({{colName2}}) \n\n    return(data_combine)\n}\n\n\nchisq.test(xtabs(\n    n ~ THEORY + Year, \n    data = build_combine_long(theory_long, THEORY, Year)))\n\n\n    Pearson's Chi-squared test\n\ndata:  xtabs(n ~ THEORY + Year, data = build_combine_long(theory_long,     THEORY, Year))\nX-squared = 262.92, df = 252, p-value = 0.3054\n\n\nI do this for SampleSizeBin, ResearchType, and Tech too. The results are:\n\n\n\nVariable\n\\chi^2\ndf\np-value\n\n\n\n\nYear\n262.92\n252\n0.305\n\n\nTech\n309.21\n294\n0.260\n\n\nSampleSizeBin\n112.35\n108\n0.368\n\n\nResearchType\n421.38\n$ 630$\n0.999\n\n\n\nFor the multiple column variables, I get:\n\n\n\nVariable\n\\chi^2\ndf\np-value\n\n\n\n\nMethods\n315.09\n168\n&lt;.001^{***}\n\n\nLimits\n133.81\n78\n&lt;.001^{***}\n\n\nFactors\n1745.5\n1134\n&lt;.001^{***}\n\n\nNS.Factors\n247.63\n162\n&lt;.001^{***}\n\n\n\nLet’s dig deeper.\n\nTheory vs Method\nThere are a large number of unique methods in the data, 41 to be exact:\n\nn_distinct(method_long$METHOD)\n\n[1] 41\n\n\nLet’s visualize this.\n\nlonglong_theorymethod &lt;- theory_long %&gt;%\n    pivot_longer(\n        cols = METHOD1:METHOD4,\n        names_to = \"NAMES\",\n        values_to = \"THINGS\") %&gt;% \n        filter(THINGS != \"\" & THINGS != \" \" & \n               THEORY != \" \" & THEORY != \"\") %&gt;% \n    mutate(THINGS = str_to_upper(THINGS),\n           THEORY = str_to_upper(THEORY))\n\nlonglong_theorymethodGroup &lt;- longlong_theorymethod %&gt;% \n    group_by(THEORY) %&gt;% count(THINGS) %&gt;%\n    arrange(desc(n))\n\nggplot(longlong_theorymethodGroup, \n    aes(area = n, fill = as.factor(THEORY), \n       label = paste(THEORY, THINGS, sep = \"\\n\"))) +\n  geom_treemap() +\n  geom_treemap_text(\n    aes(color = \"black\"), \n    place = \"centre\",\n    reflow = TRUE,\n    min.size = 0) +\n  #scale_fill_viridis_d(option = \"turbo\") + \n  scale_color_identity() +     \n  labs(fill = \"THEORY\") +\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\", \n    legend.text = element_text(size = 7),  \n    legend.spacing.y = unit(0.1, \"cm\"), # row spaces \n    legend.key.height = unit(0.2, \"cm\"), \n  ) + guides(color = guide_legend(ncol = 7)) \n\n\n\n\n\n\n\n\nThe guides() pipe helps with the guild legend on the bottom of the figure (same color as the cells). This doesn’t help much, so I’ll set aside the top methods and the top theories:\n\nlonglongGroup2 &lt;- longlong_theorymethod %&gt;% \n    group_by(THINGS) %&gt;% filter(\n        THINGS != \"SEM\" & THINGS != \"PLS SEM\" & THINGS != \"PLS\"& \n        THEORY != \"TAM\" & THEORY != \"UTAUT\"\n    ) %&gt;% count(THEORY) %&gt;% arrange(desc(n)) \n\nggplot(longlongGroup2, \n       aes(area = n, fill = as.factor(THINGS),\n           label = paste(THEORY, THINGS, sep = \"\\n\"))) +\n  geom_treemap() +\n  geom_treemap_text(\n    aes(color = \"black\"), \n    place = \"centre\",\n    reflow = TRUE,\n    min.size = 0\n  ) +\n  #scale_fill_viridis_d(option = \"rocket\") + \n  scale_color_identity() +  \n  labs(fill = \"THEORY\") +\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\", \n    legend.text = element_text(size = 7), \n    legend.spacing.y = unit(0.1, \"cm\"),   \n    legend.key.height = unit(0.2, \"cm\"),  \n  ) + guides(color = guide_legend(ncol = 7)) \n\n\n\n\n\n\n\n\nMuch more useful information from this. For example, we see that newer model theories like Elaborate Likelihood Method (ELM) incorporate moderation and mediation analysis. Whereas older models like TAM, DOI, TPB were the ones using confirmatory factor analysis and SEM.\n\n\nTheory vs Limits\nFor limits, I’m setting aside the least referenced (ttest) and the limit in scope that is captured in other limits. Additionally, let’s set aside the two top theories TAM and UTAUT.\n\nlonglonglimittheory &lt;- theory_long %&gt;% pivot_longer(\n        cols = LIMIT1:LIMIT3,\n        names_to = \"NAMES\",\n        values_to = \"THINGS\") %&gt;% \n        filter(THINGS != \"\" & THINGS != \" \"&\n               THEORY != \" \" & THEORY != \"\" & \n               THEORY != \"tam\" & THEORY != \"utaut\" &\n               THINGS != \"ttest\" & THINGS != \"limit_scope\") %&gt;% \n    mutate(THINGS = str_to_upper(THINGS), \n           THEORY = str_to_upper(THEORY))\n\nlonglonglimittheoryGroup &lt;- longlonglimittheory %&gt;% \n    group_by(THEORY) %&gt;%\n    count(THINGS) %&gt;% arrange(desc(n))\n\n\nggplot(longlonglimittheoryGroup, \n    aes(x = THEORY, fill = as.factor(THINGS))) + \n    geom_bar(position = \"fill\", color = \"black\") +\n    theme_minimal() +\n    labs(fill = \"Limits\", x = \" \", y = \"Proportions\") +\n    fill_palette(\"Paired\") + \n    theme(\n        axis.text.y = element_text(angle = 0, hjust = 1, face = \"bold\"),\n        axis.text.x = element_text(angle = 45, hjust = 1)) \n\n\n\n\n\n\n\n\nLet’s see how many times which theories are used in papers which have limitations because of sample bias:\n\ntheory_long %&gt;% pivot_longer(\n        cols = LIMIT1:LIMIT3,\n        names_to = \"NAMES\",\n        values_to = \"THINGS\") %&gt;% \n        filter(THINGS != \"\" & THINGS != \" \"&\n               THEORY != \" \" & THEORY != \"\") %&gt;%\n    mutate(THINGS = str_to_upper(THINGS), \n           THEORY = str_to_upper(THEORY)) %&gt;% \n        group_by(THINGS) %&gt;% \n        count(THEORY) %&gt;% arrange(desc(n)) %&gt;%\n        filter(THINGS == \"BIAS\") \n\n# A tibble: 5 × 3\n# Groups:   THINGS [1]\n  THINGS THEORY         n\n  &lt;chr&gt;  &lt;chr&gt;      &lt;int&gt;\n1 BIAS   TAM            3\n2 BIAS   IRT            1\n3 BIAS   META_UTAUT     1\n4 BIAS   TPB            1\n5 BIAS   TT             1\n\n\n\n\nTheory vs Factors\nThere are way too many factors for even visualizations to have any meaningful results. So, it’s better to look at the factor themes and maybe get more specific only look up really interesting patterns.\n\nlonglongfactortheory &lt;- theory_long %&gt;% pivot_longer(\n    cols = F1_THEME:F9_THEME,\n    names_to = \"NAMES\",\n    values_to = \"THINGS\"\n) %&gt;% filter(THINGS != \"\" & THINGS != \" \" &\n             THEORY != \" \" & THEORY != \"\") %&gt;% \n    mutate(THINGS = str_to_upper(THINGS),\n           THEORY = str_to_upper(THEORY))\n\nlonglongGroupfactortheory &lt;- longlongfactortheory %&gt;% \n    filter(THEORY != \"TAM\" & THEORY != \"UTAUT\" &\n           THINGS != \"EXTERNAL\") %&gt;% \n    group_by(THEORY) %&gt;% count(THINGS) %&gt;% arrange(desc(n))\n\n\nggplot(longlongGroupfactortheory, \n    aes(area = n, fill = as.factor(THINGS),\n       label = paste(THEORY, THINGS, sep = \"\\n\"))) +\n  geom_treemap() +\n  geom_treemap_text(\n    aes(color = \"black\"), \n    place = \"centre\",\n    reflow = TRUE,\n    min.size = 0\n  ) +\n  #scale_fill_viridis_d(option = \"plasma\") + \n  scale_color_identity() +  \n  labs(fill = \"THEORY\") +\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\", \n    legend.text = element_text(size = 7),  \n    legend.spacing.y = unit(0.1, \"cm\"), \n    legend.key.height = unit(0.2, \"cm\")\n  ) \n\n\n\n\n\n\n\n\n\nggplot(longlongGroupfactortheory, \n    aes(x = as.factor(THEORY), fill = THINGS)) +\n    geom_bar(position = \"fill\") +\n    theme_minimal() +\n    labs(fill = \"Theme of Factors\", x = \"Theory\", y = \"Proportions\") +\n    fill_palette(\"Set1\") +\n    theme(\n        axis.text.y = element_text(angle = 0, hjust = 1, face = \"bold\"),\n        axis.text.x = element_text(angle = 45, hjust = 1, face = \"bold\"))\n\n\n\n\n\n\n\n\nLet’s see for example what type of theories are routinely used with market factors:\n\nlonglongfactortheory %&gt;% group_by(THINGS) %&gt;% \n    count(THEORY) %&gt;% arrange(desc(n)) %&gt;%\n    filter(THINGS == \"MARKET\")\n\n# A tibble: 3 × 3\n# Groups:   THINGS [1]\n  THINGS THEORY     n\n  &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt;\n1 MARKET TAM        1\n2 MARKET TPB        1\n3 MARKET UTAUT2     1\n\n\n\n\nTheory vs Non-Significant Factors\n\nlonglongfactornstheory &lt;- theory_long %&gt;% pivot_longer(\n    cols = FNS1:FNS4,\n    names_to = \"NAMES\",\n    values_to = \"THINGS\"\n) %&gt;% filter(THINGS != \"\" & THINGS != \" \" &\n             THEORY != \" \" & THEORY != \"\") %&gt;% \n    mutate(THINGS = str_to_upper(THINGS),\n           THEORY = str_to_upper(THEORY))\n\nlonglongGroupfactorNStheory &lt;- longlongfactornstheory %&gt;%\n    group_by(THEORY) %&gt;% count(THINGS) %&gt;% \n    arrange(desc(n))\n\n\nggplot(longlongGroupfactorNStheory, \n    aes(area = n, fill = THEORY, \n        label = paste(THEORY, THINGS, sep = \"\\n\"))) +\n  geom_treemap() +\n  geom_treemap_text(\n    aes(color = \"black\"), \n    place = \"centre\",\n    reflow = TRUE,\n    min.size = 0\n  ) +\n  scale_fill_viridis_d(option = \"plasma\") + \n  scale_color_identity() +   \n  labs(fill = \"THEORY\") +\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\", \n    legend.text = element_text(size = 7), \n    legend.spacing.y = unit(0.1, \"cm\"),\n    legend.key.height = unit(0.2, \"cm\")\n  ) \n\n\n\n\n\n\n\n\n\nlonglongfactornstheory %&gt;% group_by(THINGS) %&gt;% \n    count(THEORY) %&gt;% arrange(desc(n)) %&gt;% \n    filter(THINGS == \"PERSONAL\")\n\n# A tibble: 0 × 3\n# Groups:   THINGS [0]\n# ℹ 3 variables: THINGS &lt;chr&gt;, THEORY &lt;chr&gt;, n &lt;int&gt;",
    "crumbs": [
      "Home",
      "Projects",
      "Project 1. SLR",
      "Data Analysis"
    ]
  },
  {
    "objectID": "study1_tm.html",
    "href": "study1_tm.html",
    "title": "Topic Modeling",
    "section": "",
    "text": "If you want to run the entire code, use the Jupyter notebook on my github page.",
    "crumbs": [
      "Home",
      "Projects",
      "Project 1. SLR",
      "Topic Modeling"
    ]
  },
  {
    "objectID": "study1_tm.html#part-0.-jupyter-notebook",
    "href": "study1_tm.html#part-0.-jupyter-notebook",
    "title": "Topic Modeling",
    "section": "",
    "text": "If you want to run the entire code, use the Jupyter notebook on my github page.",
    "crumbs": [
      "Home",
      "Projects",
      "Project 1. SLR",
      "Topic Modeling"
    ]
  },
  {
    "objectID": "study1_tm.html#part-1.",
    "href": "study1_tm.html#part-1.",
    "title": "Topic Modeling",
    "section": "Part 1.",
    "text": "Part 1.\nFirst things first, we need a bunch of libraries. Since I am not familiar with Docker, I couldn’t resolve the package dependencies. This took so much time for me and I finally managed to fix it with this specific configuration.\n\n__requires__= 'scipy==1.12.0'\nimport scipy\n\nprint(scipy.__version__)\n\nThe imports look scary, but a lot of them I won’t even use, just added them because I wanted to try things:\n\n# general python imports \nimport string\nimport os\nimport re\nimport pandas as pd\nimport numpy as np \nimport scipy\nimport itertools\nimport textract\n\n# NLT imports \nimport nltk\nfrom nltk import pos_tag\n#from nltk.tokenize import regexp_tokenize\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tokenize import sent_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.util import ngrams\nfrom nltk.tokenize import RegexpTokenizer\n\n# SKLEARN \nfrom sklearn import metrics\nfrom sklearn import neighbors\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\n# from sklearn.ensemble import RandomForestClassifier\n# from sklearn.metrics import classification_report\n# from sklearn.naive_bayes import MultinomialNB\n# from sklearn.neighbors import NearestNeighbors\n# from sklearn.preprocessing import LabelEncoder\n# from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n# from sklearn.naive_bayes import (\n#     BernoulliNB,\n#     ComplementNB,\n#     MultinomialNB,\n# )\n# from sklearn.neighbors import KNeighborsClassifier\n# from sklearn.tree import DecisionTreeClassifier\n# from sklearn.ensemble import RandomForestClassifier\n# from sklearn.linear_model import LogisticRegression\n# from sklearn.neural_network import MLPClassifier\n# from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n# from sklearn.decomposition import LatentDirichletAllocation\n\n\n# GENSIM imports \nimport gensim\nfrom gensim.models import Phrases\nfrom gensim.models.phrases import Phraser\nfrom gensim.corpora.dictionary import Dictionary\nfrom gensim.corpora import MmCorpus\nfrom gensim.models.tfidfmodel import TfidfModel\nfrom gensim.models import CoherenceModel\nfrom gensim.models import KeyedVectors\n\n\n# PyLDAvis imports \n# import pyLDAvis\n# import pyLDAvis.gensim_models as gensimvis\n# import pyLDAvis.gensim\n# import pyLDAvis.gensim_models\n\n\n\n# MISC imports \nfrom collections import Counter\nfrom collections import defaultdict\nfrom string import punctuation\nfrom pprint import pprint\nfrom numpy import triu\n#from scipy.linalg.special_matrices import triu\nfrom scipy.sparse import csr_matrix\n\n\n\n# TRANSFORMERS \n#import torch\n#import tensorflow as tf\n#from transformers import BertTokenizer, BertModel\n#from tensorflow.keras.models import Sequential\n#from tensorflow.keras.preprocessing.text import Tokenizer\n#from tensorflow.keras.preprocessing.sequence import pad_sequences\n#from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n#from tensorflow.keras.layers import LeakyReLU\n\nimport fitz  # PyMuPDF\n\n\n# MATPLOT \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n#%matplotlib inline # do this if you're in jupyter, I still don't know why tho \n\n\n# only run once\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('wordnet')\nnltk.download('punkt_tab')\n#nltk.download('omw-1.4')  # Optional for better language support\n#nltk.download('averaged_perceptron_tagger')  # For POS tagging\n#nltk.download('averaged_perceptron_tagger_eng')\n\n\nCLEANING AND PRE-PROCESSING DATA\nI downloaded the pdf of all the papers (143), reading them and extracting meta data based on the following:\n\nimport numpy as np \n\ndatabase = np.array([\n    {\n        'id': 'string', # unique identifier for the paper following convention P2_#number \n        'title': 'string', # title of the paper\n        'AffiliationCountry': 'string' , #name of country the study was conducted in,\n        'year': 2018-2024, # year of publication a value between 2018 and 2024\n        'journal': 'string', # name of the journal the paper was published in\n        'citations': 0-1000, # number of citations the paper has received - not reported in the paper \n        'year_since': 3, # number of years since publication - not reported in the paper \n        'cpy': 0, # number of citations per year - not reported in the paper \n        'keywords': ['TAM', 'mbanking', 'awareness'], # list of keywords, broken into K1-K10\n        'abstract': 'string', # abstract of the paper \n        'F': ['perceived usefulness'], # factors significant in the study, broken into F1-F9 \n        'FN': ['another factor'], # factors not significant in the study, broken into FNS1-FNS4 \n        'limit': ['geographical context'], # limitations of the study, broken into LIMIT1-LIMIT3 \n        'typeofResearch': 'string', # type of research conducted in the study \n        'methods': ['regression analysis'], # methods used in the study, broken into METHOD1-METHOD4\n        'theory': ['TAM'] # theories used in the study, broken into THEORY1-THEORY4\n        'sampleSize': 100, # sample size of the study \n        'tech': 'string', # main technology studied \n        'man_theme': 'string', # Theme manually assigned by me \n        'algo_theme': 'string', # Theme assigned by the algorithm \n        'decision_Theme': 'string', # Final theme of the paper  \n        'Score_Sig': 0.0, # % of significance for factors \n        'Score_NOT_Sig': 0.0, # % of non-significance for factors\n    }\n])\n\n\n\nIdea for future\n\n🤖 Build an Agentic AI application that automates this process.\n\nThe following procedures are implemented for Data Cleaning:\n\nTurn everything into lower case\nRemove stopwords + additional stopwords such as “bank”, “banking”, “banks”, “mobile”, “mbank”, “mbanking”, “m-bank”, “online”, “digital”, “adoption”, “theory”, “app”, “application”\nRemove punctuation\nLemming/Stemming\n\nGrabbing the names of the pdf files (you can also do this from the terminal, and have the results be written to a .txt file).\n\npdf_directory = \"./pdfs/\"\nall_files = os.listdir(pdf_directory)\npdf_files = [file for file in all_files if file.endswith('.pdf')]\n\noutput_file = \"pdf_file_names.txt\"\nwith open(output_file, \"w\") as f:\n    for pdf in pdf_files:\n        f.write(pdf + \"\\n\")\n\nprint(f\"PDF file names have been saved to {output_file}\")\n\nThen saving them in a python dictionary:\n\nname_of_pdfs = {\n    'p2_101': \"Okocha and Awele Adibi - 2020 - Mobile banking adoption by business executives in.pdf\",\n    # ... \n}\n\nExtract text:\n\n#version one using PyMuPDF - there's also textract \ndef extract_text_from_pdf(filename):\n    text = \"\"\n    try:\n        doc = fitz.open(filename)\n        for page_num in range(doc.page_count):\n            page = doc.load_page(page_num)\n            text += page.get_text()\n    except Exception as e:\n        print(f\"Error reading {filename}: {e}\")\n    return text\n\n\ntext_of_pdfs_v1 = {}\n\nfor paper_id, filename in name_of_pdfs.items():\n    text = extract_text_from_pdf(filename)\n    text_of_pdfs_v1[paper_id] = text\n\n# Example: Print the extracted text from the first PDF\nfor paper_id, text in text_of_pdfs_v1.items():\n    print(f\"Text from {paper_id} ({name_of_pdfs[paper_id]}):\")\n    print(text[:500])  # Print the first 500 characters of the text\n    break\n\n\nText from p2_101 (Okocha and Awele Adibi - 2020 - Mobile banking adoption by business executives in .pdf): Mobile banking adoption by business executives in Nigeria Foluke Olabisi Okocha1* and Vera Awele Adibi2 1Centre for Learning Resources, Landmark University, Nigeria 2Doctoral student, University of Ibadan, Nigeria *Corresponding author email: dada.foluke@lmu.edu.ng, folukedada@yahoo.com Challenges with the adoption of mobile banking technologies are best understood by studies on adoption. This however requires understanding the factors that inﬂuence its adoption in a given region. Technology Acc\n\nClean text:\n\nstop_words = stopwords.words('english')\nstop_words.extend([\"bank\", \"banking\", \"banks\", \n                   \"mobile\", \"mbank\", \"mbanking\", \"m-bank\", \"m bank\",\n                   \"adoption\", \"acceptance\", \"accept\", \"theory\", \"technology\", \n                   \"purpose\", \"result\", \"method\", #from abstracts \n                   \"journal\", \"volume\", \"pp\", \"no\", #from journal information \n                   \"theory\", \"app\", \"application\", \"usage\", \"model\"])\n\nstemmer = PorterStemmer()\nlemmatizer = WordNetLemmatizer()\n\nThis is just one of the cleaning functions:\n\ndef preprocess_Dict(dct):\n    for k, v in dct.items():\n        if isinstance(v, list):\n            processed_list = []\n            for item in v:\n                item = item.lower()\n                item = re.sub(r'http\\S+www\\S+@\\S+', '', item)\n                item = re.sub(r'(?&lt;=\\w)-(?=\\w)', ' ', item)\n                item = re.sub(r'[^a-z0-9\\s\\n]', '', item)\n                item = re.sub(r'\\s+', ' ', item).strip()\n                item = re.sub(r'\\d+', '', item).strip()\n                item = \" \".join([word for word in item.split() if word not in stop_words])\n                item = \" \".join([stemmer.stem(word) for word in item.split()])\n                processed_list.append(item)\n            dct[k] = processed_list\n        else:\n            v = v.lower()\n            v = re.sub(r'http\\S+www\\S+@\\S+', '', v)\n            v = re.sub(r'(?&lt;=\\w)-(?=\\w)', ' ', v)\n            v = re.sub(r'[^a-z0-9\\s\\n]', '', v)\n            v = re.sub(r'\\s+', ' ', v).strip()\n            v = re.sub(r'\\d+', '', v).strip()\n            v = \" \".join([word for word in v.split() if word not in stop_words])\n            v = \" \".join([stemmer.stem(word) for word in v.split()])\n            dct[k] = v\n    return dct\n\nSentence Tokenizer:\n\ndef tokenizeToSentences(doc):\n    for k, v in doc.items():\n        \n        if isinstance(v, bytes):\n            v = v.decode('utf-8')\n          \n        v = v.lower()\n        v = v.replace('\\n', ' ')\n        v = re.sub(r'http\\S+www\\S+@\\S+', '', v)\n        #v = \" \".join([str(s) for s in v])\n\n        v = sent_tokenize(v)\n        doc[k] = v\n        \n    return doc\n\n\ntext_of_pdfs_uncleaned_tokenizedSentences_v1 = tokenizeToSentences(text_of_pdfs_v1)\n\nBuild uni and bi-grams:\n\ntext_of_pdfs_uncleaned_tokenize_words_v1 = {}\ntext_of_pdfs_uncleaned_tokenize_bigrams_v1 = {}\n\n\nfor k, v in text_of_pdfs_uncleaned_tokenizedSentences_v1.items():\n    #v is a list of sentences \n    text_of_pdfs_uncleaned_tokenize_words_v1[k] = [word_tokenize(s) for s in v] #list of lists \n    text_of_pdfs_uncleaned_tokenize_bigrams_v1[k] = [list(ngrams(sentence, 2)) for sentence in text_of_pdfs_uncleaned_tokenize_words_v1[k]] \n\nClean:\n\ntext_of_pdfs_cleaned_tokenize_words_v1 = {}\n\n\nfor k, v in text_of_pdfs_uncleaned_tokenize_words_v1.items():\n    # v is a list of lists - where each outer list is a sentence, and the inner list is the words in that sentence. \n    text_of_pdfs_cleaned_tokenize_words_v1[k] = preprocess_listOfLists(v)\n    \n\ntext_of_pdfs_cleaned_tokenize_bigrams_v1 = {}\n\nfor k, v in text_of_pdfs_cleaned_tokenize_words_v1.items():\n    text_of_pdfs_cleaned_tokenize_bigrams_v1[k] = [list(ngrams(sentence, 2)) for sentence in v]\n\n\ntext_of_pdfs_cleaned_tokenize_words_v1['p2_01'][0][:3]\n\n\n[‘doi’, ‘jgim’, ‘global’]\n\n\ntext_of_pdfs_cleaned_tokenize_bigrams_v1['p2_01'][0][:3]\n\n\n[(‘doi’, ‘jgim’), (‘jgim’, ‘global’), (‘global’, ‘inform’)]\n\nStich the bi-grams together:\n\ntext_of_pdfs_cleaned_tokenize_bigrams_combined_v1 = {}\n\nfor k, v in text_of_pdfs_cleaned_tokenize_bigrams_v1.items():\n    text_of_pdfs_cleaned_tokenize_bigrams_combined_v1[k] = [[f\"{a} {b}\" for a, b in sublist] for sublist in v]\n    \ntext_of_pdfs_cleaned_tokenize_bigrams_combined_v1['p2_01'][0][:3]\n\n\n[‘doi jgim’, ‘jgim global’, ‘global inform’]\n\nGenerate Dictionary and Corpuses for unigrams and bigrams, and save them to file (you can read these files in later runs of the program):\n\ndef generate_dictionary(text, name):\n    \"\"\" \n    As input takes in the text to build the dictionary for and the name of a .mm file\n    \"\"\" \n    \n    dictionary = Dictionary(text)\n    \n    corpus = [dictionary.doc2bow(review) for review in text] \n    \n    filename = f\"{name}.mm\"\n    \n    MmCorpus.serialize(filename, corpus)\n    \n    return dictionary, corpus\n\n\npapers_dictionary_unigrams_v1 = {}\npapers_corpus_unigrams_v1 = {}\n\nfor k, v in text_of_pdfs_cleaned_tokenize_words_v1.items():\n    papers_dictionary_unigrams_v1[k] = generate_dictionary(v, 'mmcorpus_unigrams')[0]\n    papers_corpus_unigrams_v1[k] = generate_dictionary(v, 'mmcorpus_unigrams')[1]\n\n\npapers_dictionary_bigrams_v1 = {}\npapers_corpus_bigrams_v1 = {}\n\nfor k, v in text_of_pdfs_cleaned_tokenize_bigrams_combined_v1.items():\n    papers_dictionary_bigrams_v1[k] = generate_dictionary(v, 'mmcorpus_bigrams')[0]\n    papers_corpus_bigrams_v1[k] = generate_dictionary(v, 'mmcorpus_bigrams')[1]\n\nAdditionally, I combine all the PDFs and run this for the entire Database.\n\nentire_database_listoflists_unigrams_v1 = []\n\nfor value in text_of_pdfs_cleaned_tokenize_words_v1.values():\n    entire_database_listoflists_unigrams_v1.extend(value)\n\nentire_database_listoflists_bigrams_v1 = []\n\nfor value in text_of_pdfs_cleaned_tokenize_bigrams_combined_v1.values():\n    entire_database_listoflists_bigrams_v1.extend(value)\n\n\n# database_dictionary_unigrams = {}\n# database_corpus_unigrams = {}\n\ndatabase_dictionary_unigrams_v1 = generate_dictionary(entire_database_listoflists_unigrams_v1, 'mmcorpus_Database_unigrams_v1')[0]\ndatabase_corpus_unigrams_v1 = generate_dictionary(entire_database_listoflists_unigrams_v1, 'mmcorpus_Database_unigrams_v1')[1]\n\n\ndatabase_dictionary_bigrams_v1 = generate_dictionary(entire_database_listoflists_bigrams_v1, 'mmcorpus_Database_bigrams_v1')[0]\ndatabase_corpus_bigrams_v1 = generate_dictionary(entire_database_listoflists_bigrams_v1, 'mmcorpus_Database_bigrams_v1')[1]\n\nPrinting top 50 words across the corpus:\n\n# ---------------------- START OF CHATGPT CODE ----------------------\ndef print_top_50_words(corpus, dictionary):\n    total_word_count = defaultdict(int)\n    word_weights = defaultdict(float)\n\n    for word_id, word_count in itertools.chain.from_iterable(corpus):\n        total_word_count[word_id] += word_count\n\n    sorted_tota_words_count = sorted(total_word_count.items(), key = lambda w: w[1], reverse = True)\n\n    tfidf = TfidfModel(corpus)\n\n\n    for doc in corpus:\n        tfidf_weights = tfidf[doc]  # Calculate TF-IDF for the review\n        for term_id, weight in tfidf_weights:\n            word_weights[term_id] += weight  # Aggregate the weight for the term\n\n    sorted_word_weights = sorted(word_weights.items(), key=lambda x: x[1], reverse=True)\n\n    # Print the top 50 terms with their weights\n    top_50_words = [(dictionary.get(term_id), weight) for term_id, weight in sorted_word_weights[:50]]\n\n    for word, weight in top_50_words:\n        print(word, weight)\n\n# ---------------------- END OF CHATGPT CODE ----------------------\n\nUni-grams over the entire database:\n\nprint_top_50_words(database_corpus_unigrams_v1, database_dictionary_unigrams_v1)\n\n\nuse 1710.4405813502553\nal 1500.5918177863637 et 1495.2598944189729 studi 1254.889113401414 servic 1177.5025518831014 research 1155.4801330260996 model 1093.757883374598 intent 1085.622080362571 inform 1035.95718724093 market 1032.669725701611 manag 1020.5243612360091 custom 1011.465319080724 perceiv 975.0912634817644 consum 959.7309079460276 and many more\n\nBi-grams over the entire database:\n\nprint_top_50_words(database_corpus_bigrams_v1, database_dictionary_bigrams_v1)\n\n\net al 1065.4586868386625 intern market 424.5870797007975 inform manag 324.417783324221 http doiorg 272.8802285987675 inform system 259.07233958915 intent use 247.3467671477514 behavior intent 207.71672202444856 eas use 206.32538882113823 comput human 183.94284111390388 perceiv use 183.0881496709403 human behavior 179.3628870311971 and many more\n\nBuild an LDA model, but I want to test anywhere from 5 to 15 topic numbers, so I’ll leave this as a parameter to pass to this function. I left the other parameters as is. You of course need to pass in your corpus (text) and dictionary (you created above).\n\ndef build_lda_model(n_topic, corpus_, dictionary_):\n    lda_model = gensim.models.ldamodel.LdaModel(corpus = corpus_,\n                                              num_topics = n_topic,\n                                              id2word = dictionary_,\n                                              random_state = 100,\n                                              update_every = 1,\n                                              chunksize = 1000,\n                                              passes = 10,\n                                              alpha = 'auto',\n                                              per_word_topics = True)\n    return lda_model\n\nHere’s where I train 11 different models passing in different values for number of topics. I save each model in a list.\n\ndef train_models(corpus_, dictionary_):\n    list_to_hold_models = []\n    topic_n_to_try = [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n    \n    for i in range(len(topic_n_to_try)):\n        list_to_hold_models.insert(i, build_lda_model(topic_n_to_try[i], corpus_, dictionary_))\n        \n    return list_to_hold_models\n\nYou can calculate both perplexity and coherence. Coherence is more straightforward: It’s a measure of how correct your model is. Perplexity, not that difficult, it’s how off you are. So, you want higher coherence, and lower perplexity. It’s easier to just focus on one of them. But for practice, I did both!\n\ndef calculate_perplexity(model, corpus_):\n    perplexity = model.log_perplexity(corpus_)\n    return perplexity\n\ndef calculate_coherence(model, text, dictionary_):\n    coherence_model_lda = CoherenceModel(model = model, texts = text, dictionary = dictionary_, coherence='c_v')\n    coherence_lda = coherence_model_lda.get_coherence()\n\n    return coherence_lda\n\nHere’s how to compare models: build a table with columns Num_Topics and values of Coherence and Perplexity for each model.\n\ndef build_model_comparison_table(list_of_models, corpus_, dictionary_, data):\n    tracker = 5 \n    models_perplexity = []\n    models_coherence = []\n    models_topics = [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n    \n    \n    for model in list_of_models:\n        models_perplexity.append(calculate_perplexity(model, corpus_))\n        models_coherence.append(calculate_coherence(model, data, dictionary_))\n        tracker += 1\n        \n    if tracker == 10:\n        print(\"Successfully generated model comparison table.\") \n        \n    models_df = pd.DataFrame({\n        'Num_Topics': models_topics,\n        'Coherence': models_coherence,\n        'Perplexity': models_perplexity,\n    })\n\n    return models_df\n\nYou can either look at the values, or have this code find you the best. Here, I pick the maximum coherence and minimum perplexity. But, what if it’s two difference values? As in, for example, topic number 5 gives the highest coherence but also the highest perplexity, and topic number 10 gives the lowest perplexity but lower coherence. Which do you choose? Well, I decided they’re both equally as important, so I come up with a score for each topic number that’s just the weighted average of coherence and perplexity. Or, save yourself the headache and just use one metric.\n\ndef find_best_model(models_df):\n    print(\"Number of topics with the maximum Coherence is \", models_df.loc[models_df['Coherence'].idxmax(), 'Num_Topics'])\n    print(\"Number of topics with the minimum Perplexity is \", models_df.loc[models_df['Perplexity'].idxmin(), 'Num_Topics'])\n    \n    if models_df.loc[models_df['Coherence'].idxmax(), 'Num_Topics'] == models_df.loc[models_df['Perplexity'].idxmin(), 'Num_Topics']:\n        best_model_row = models_df.loc[models_df['Perplexity'].idxmin()]\n        best_number_of_topics = best_model_row['Num_Topics']\n    else:\n        models_df['Normalized_Perplexity'] = (models_df['Perplexity'] - models_df['Perplexity'].min()) / (models_df['Perplexity'].max() - models_df['Perplexity'].min())\n        models_df['Normalized_Coherence'] = (models_df['Coherence'] - models_df['Coherence'].min()) / (models_df['Coherence'].max() - models_df['Coherence'].min())\n\n        models_df['Inverted_Perplexity'] = 1 - models_df['Normalized_Perplexity'] # because smaller is better\n\n        weight_preplexity = 0.5\n        weight_coherence = 0.5\n\n        models_df['Score'] = weight_coherence * models_df['Normalized_Coherence'] + weight_preplexity * models_df['Inverted_Perplexity']\n\n        best_model_row = models_df.loc[models_df['Score'].idxmax()]\n        best_number_of_topics = best_model_row['Num_Topics']\n\n    print(best_model_row)\n                                       \n    return best_model_row, best_number_of_topics\n                                                                                                     \n                                                                                                \ndef pick_best_model(num, m):\n    \"\"\" \n    Model inputs are: \n        num = best number of topics found according to find_best_model()\n        m = list of models \n    \"\"\"\n    model_index = num - 5 \n    model_index = int(model_index)\n                                       \n    best_model = m[model_index]\n                                       \n    return best_model  \n                                                                  \ndef print_topics(model, corpus):\n    pprint(model.print_topics())\n    doc_lda = model[corpus]\n    \n    return doc_lda\n\n\nTopic Modeling - Unigrams\n\nunigram_models_v1 = train_models(database_corpus_unigrams_v1, database_dictionary_unigrams_v1)\n\nHere are the results for unigram models:\n\nunigram_model_comparison_v1 = build_model_comparison_table(unigram_models_v1, database_corpus_unigrams_v1, database_dictionary_unigrams_v1, entire_database_listoflists_unigrams_v1)\nunigram_model_comparison_v1\n\n\n\n\nNum_Topics\nCoherence\nPerplexity\n\n\n\n\n0\n5\n0.436565\n\n\n1\n6\n0.413618\n\n\n2\n7\n0.469700\n\n\n3\n8\n0.400105\n\n\n4\n9\n0.452116\n\n\n5\n10\n0.420971\n\n\n6\n11\n0.446276\n\n\n7\n12\n0.454530\n\n\n8\n13\n0.409933\n\n\n9\n14\n0.418211\n\n\n10\n15\n0.406770\n\n\n\n\nunigram_best_row_v1 = find_best_model(unigram_model_comparison_v1)[0]\nunigram_best_n_topics_v1 = find_best_model(unigram_model_comparison_v1)[1]\n\n\nNumber of topics with the maximum Coherence is 7 Number of topics with the minimum Perplexity is 15 Num_Topics 12.000000 Coherence 0.454530 Perplexity -9.011387 Normalized_Perplexity 0.636029 Normalized_Coherence 0.782020 Inverted_Perplexity 0.363971 Score 0.572996 Name: 7, dtype: float64 Number of topics with the maximum Coherence is 7 Number of topics with the minimum Perplexity is 15 Num_Topics 12.000000 Coherence 0.454530 Perplexity -9.011387 Normalized_Perplexity 0.636029 Normalized_Coherence 0.782020 Inverted_Perplexity 0.363971 Score 0.572996 Name: 7, dtype: float64\n\n\nunigram_best_model_v1 = pick_best_model(unigram_best_n_topics_v1, unigram_models_v1)\nprint(\"Best Unigram model is (V1):\", unigram_best_model_v1)\n\n\nBest Unigram model is (V1): LdaModel(num_terms=27200, num_topics=12, decay=0.5, chunksize=1000)\n\n\n\nTopic Modeling - Bigrams\n\nbigram_models_v1 = train_models(database_corpus_bigrams_v1, database_dictionary_bigrams_v1)\n\nAnd the bigrams:\n\nbigram_model_comparison_v1 = build_model_comparison_table(bigram_models_v1, database_corpus_bigrams_v1, database_dictionary_bigrams_v1, entire_database_listoflists_bigrams_v1)\nbigram_model_comparison_v1\n\n\n\n\nNum_Topics\nCoherence\nPerplexity\n\n\n\n\n0\n5\n0.558434\n\n\n1\n6\n0.535400\n\n\n2\n7\n0.542287\n\n\n3\n8\n0.515335\n\n\n4\n9\n0.523767\n\n\n5\n10\n0.526290\n\n\n6\n11\n0.523879\n\n\n7\n12\n0.513803\n\n\n8\n13\n0.510867\n\n\n9\n14\n0.554809\n\n\n10\n15\n0.582336\n\n\n\n\nbigram_best_row_v1 = find_best_model(bigram_model_comparison_v1)[0]\nbigram_best_n_topics_v1 = find_best_model(bigram_model_comparison_v1)[1]\n\n\nNumber of topics with the maximum Coherence is 15 Number of topics with the minimum Perplexity is 15 Num_Topics 15.000000 Coherence 0.582336 Perplexity -24.581214 Name: 10, dtype: float64 Number of topics with the maximum Coherence is 15 Number of topics with the minimum Perplexity is 15 Num_Topics 15.000000 Coherence 0.582336 Perplexity -24.581214 Name: 10, dtype: float64\n\n\nbigram_best_model_v1 = pick_best_model(bigram_best_n_topics_v1, bigram_models_v1)\nprint(\"Best Unigram model is (V1):\", bigram_best_model_v1)\n\n\nBest Unigram model is (V1): LdaModel(num_terms=306163, num_topics=15, decay=0.5, chunksize=1000)\n\n\n\nPick Best Model\nThis is different from my dissertation because I actually didn’t upload all the pdf’s here, and also I’m looking at both scores where I only looked at Coherence. Also, 15 topics is way too many.\n\ndef model_score(p, c, wp = 0.5, wc = 0.5):\n    \"\"\" Calculates model score with 0.5 weights as default\"\"\"\n    score = (1 - p) * wp + c * wc \n    return score\n\n\nprint(\"Best unigram model's score is (V1):\", model_score(-9.011387,0.454530))\nprint(\"Best bigram model's score is (V1):\", model_score(-24.581214,0.582336))\n\n\n\n\nNGRAM\nPerplexity\nCoherence\n# of topics\nScore\n\n\n\n\nUNI\n-9.011387\n0.454530\n12\n5.2329585\n\n\nBI\n-24.581214\n0.582336\n15\n13.081775\n\n\n\nThe best model overall is therefore bigram_best_model.\nI chose bigrams, but went with 8-9 topics.\n\nbest_topic_model_v1 = bigram_best_model_v1\nnumber_of_topics = 8\n\n\nprint_topics(best_topic_model_v1, database_corpus_bigrams_v1)\n\n\n\n\n\nTopic Modeling using Keywords\n\nkeywordsDf = df.loc[:,'K1':'K10']\n\nkeywords_across_db = keywordsDf.values.flatten().tolist()\nlen(keywords_across_db)\n\n\nimport math\n\nempty_or_na_count = sum(1 for x in keywords_across_db if x in [None, \"\", ' '] or (isinstance(x, float) and math.isnan(x)))\n\nprint(f\"Number of empty or NA values: {empty_or_na_count}\")\n\n\nkeywords_across_db = [x for x in keywords_across_db if x not in [None, \"\", ' '] and not (isinstance(x, float) and math.isnan(x))]\n\nkeywords_across_db_nodup = list(set(keywords_across_db))\n\n\nfrom transformers import BertTokenizer, BertModel\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel_bert = BertModel.from_pretrained('bert-base-uncased')\n\ndef get_embedding(text):\n    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=20)\n    with torch.no_grad():\n        outputs = model_bert(**inputs)\n    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n\n\ndef print_clusters(n_clusters, list_of_words):\n    clusters = {i: [] for i in range(n_clusters)}\n    for word, label in zip(list_of_words, labels):\n        clusters[label].append(word)\n\n    for label, words in clusters.items():\n        print(f\"Cluster {label}:\")\n        for word in words:\n            print(f\"  {word}\")\n        print(\"\\n\")\n\n    # Explain clusters\n    print(\"Cluster explanations based on semantics and ideas:\")\n    for label, words in clusters.items():\n        print(f\"Cluster {label} might be related to:\")\n        for word in words:\n            print(f\"  {word}\")\n        print(\"\\n\")\n\n\nimport torch \n\nkeyword_embeddings = np.array([get_embedding(phrase) for phrase in keywords_across_db_nodup])\n\nn_clusters = number_of_topics\nkmeans = KMeans(n_clusters = n_clusters, random_state = 0)\nkmeans.fit(keyword_embeddings)\nlabels = kmeans.labels_\n\n\nprint_clusters(n_clusters, keywords_across_db_nodup)",
    "crumbs": [
      "Home",
      "Projects",
      "Project 1. SLR",
      "Topic Modeling"
    ]
  },
  {
    "objectID": "study2_DA.html",
    "href": "study2_DA.html",
    "title": "Effect Of Mental Health on Mobile Banking",
    "section": "",
    "text": "In this project, I focused on analyzing how mental health relates to mobile banking adoption. I used data from the Canadian Internet Use Survey, which includes questions about digital habits, mental health, and demographics. You can find the dataset here. I built a fixed-effects logistic regression model, grouped by province, to control for regional differences as this was the sampling cluster. My main variable was self-reported mental health scores. I included factors like relationship satisfaction, smartphone dependency, and social media use. I also tested interaction effects to see if these variables change the way mental health influences mobile banking use. The following is a step-by-step on the coding and analysis of the project.\n\n\nNote that not all libraries may be utilized. The most important ones are dplyr, lme4, tidyr, ggplot2, psych, corrr, haven, marginaleffects and margins and any related libraries to these.\n\n\n\nI first started by reading the entire PUMF file available.\nThis gives you information on how the survey was set up, why, and how things were measured. Then, I looked at the individual survey questions to see the available data, and how they were measured. In general, questions are measured numerically were answeres follow as such: &gt; Yes : 1, No : 2, Valid Skip: 6, Don’t Know: 7, Refusal: 8, Not Stated: 9 Of course this differs question-by-question as some questions have other answer categories and some questions (which were note used in my study) asked for numerical input from the participants (like how much did you spend online last year). To help readers understand the data, I will include the question exactly as it appears in the CIUS 2020 PUMF Data Dictionary with corresponding answer choices and codes. These will be in a blue-bordered box, and will include the Variable name (on the PUMF file), Concept, Question Body and Answers. Then I will show you in R code how I’ve re-coded and used the question as a model variable. The Variables I need are as follows:\n\nMobile banking adoption (mBanking)\nAge Group (AGE)\nSmartphone Dependent (SD)\nIncome Quintile (INC)\nFriendship Satisfaction (FRISAT)\nMental Health (MH)\nFamily Relation Satisfaction (FAMSAT)\nEducation Level (EDU)\nImmigration Status (IMM)\nEmployment Status (EMP)\nFamily Type (FAM)\nGender (SEX)\nSocial Media Use (SNS)\nProvince (province)\n\nThe data is available in various formats. To avoid data loss, I decided to use the .dta format (SAS file). You need the haven package to read SAS files. However, this file is 150MB in size and since I am uploading the code in my GitHub repository, I am not able to use the SAS file. So I’ve saved the data in a .csv file with only the columns I will need. This is how you’d read a SAS file:\n\nds20 &lt;- read_dta(\"data/cius2020_2022nov18_en.dta\")\nds &lt;- ds20\n\nInstead, I run this:\n\nds10 &lt;- read.csv(\"ds.csv\")\nds &lt;- ds10 \n\nStep 1. Smartphone Users Select only those who use smartphones because the question is about “online banking” and not “mobile banking”.\n\n\n\nVariable Name: DV_010A\n\nConcept: Devices used\n\n\nQuestion Text:\nDuring the past three months, what devices did you use to access the Internet?\nDid you use:\nA smartphone\n\n\n\nAnswer Categories\nCode\n\n\n\n\nYes\n1\n\n\nNo\n2\n\n\nValid Skip\n6\n\n\nDon’t Know\n7\n\n\nRefusal\n8\n\n\nNot Stated\n9\n\n\n\n\n\nds &lt;- ds %&gt;% \n    mutate(\n        devSM = case_when(\n            dv_010a == 1 ~ 1, #yes\n            dv_010a == 2 ~ 0, #no\n        .default = -1, #any valid skip and not stated \n        )\n    )\n\nds &lt;- ds %&gt;% \n   filter(devSM == 1)\n\nStep 2. Mobile Banking Question Select the outcome variable, mobile banking:\n\n\n\nVariable Name: UI_050D\n\nConcept: Activities related to other online activities\n\n\nQuestion Text:\nDuring the past three months, which of the following other online activities, have you done over the Internet? Have you: Conducted online banking\n\n\n\nAnswer Categories\nCode\n\n\n\n\nYes\n1\n\n\nNo\n2\n\n\nValid Skip\n6\n\n\nDon’t Know\n7\n\n\nRefusal\n8\n\n\nNot Stated\n9\n\n\n\n\nStep 3. Select Other Model Variables Now I will move on to selecting the predictors. For Smartphone Dependency:\n\n\n\nVariable Name: SM_030A\n\nConcept: Frequency of use of smartphone\n\n\nQuestion Text:\nIn a typical day, how often do you check your smartphone?\n\n\n\nAnswer Categories\nCode\n\n\n\n\nAt least every 5 minutes\n01\n\n\nAt least every 15 minutes\n02\n\n\nAt least every 30 minutes\n03\n\n\nOne time per hour\n04\n\n\nOnce a day or a few times per day\n05\n\n\nLess than one time per day\n06\n\n\nValid skip\n96\n\n\nDon’t know\n97\n\n\nRefusal\n98\n\n\nNot stated\n99\n\n\n\n\nAs you can see, this one is no longer just a yes/no question, but has a few categories for answers. Since I’m thinking of tracking “dependence”, it makes sense that the more frequent checking gets a higher value. So, this is how I code this variable:\n\nds &lt;- ds %&gt;%\n    mutate(\n        #timeline : past 3 months \n        mBanking = case_when(\n            ui_050d == 1 ~ 1, # Yes \n            ui_050d == 2 ~ 0, # No \n            .default = -1 # valid skip, don't know, refused, not stated \n        ),\n        \n        SD = case_when(\n            sm_030a == 1 ~ 6, # At least every 5 minutes \n            sm_030a == 2 ~ 5, # At least every 15 minutes \n            sm_030a == 3 ~ 4, # At least every 30 minutes  \n            sm_030a == 4 ~ 3, # One time per hour  \n            sm_030a == 5 ~ 2, # Once a day or a few times per day  \n            sm_030a == 6 ~ 1, # Less than one time per day\n            .default = 96 # Valid skip 96 , Don’t know 97 , Refusal  98, Not stated  99\n        )\n    )\n\nds &lt;- ds %&gt;% filter(SD &lt; 10)\nds &lt;- ds %&gt;% filter(mBanking != -1) \n\nThe filters are just making sure that the skip’s, don’t know’s, refusal’s and not stated answers are dropped. This is not too big of a loss. There are 17,409 rows of data.\nFriendship Satisfaction:\n\n\n\nVariable Name: TS_010A\n\nConcept: Satisfaction with relationships\n\n\nQuestion Text:\nIn general, on a scale from 1 to 5 where 1 means «completely dissatisfied» and 5 means «completely satisfied», how satisfied are you with the relationships you have with the following people? Friends\n\n\n\nAnswer Categories\nCode\n\n\n\n\n1-Completely dissatisfied\n1\n\n\n2\n2\n\n\n3\n3\n\n\n4\n4\n\n\n5- Completely satisfied\n5\n\n\nValid skip\n6\n\n\nDon’t know\n7\n\n\nRefusal\n8\n\n\nNot stated\n9\n\n\n\n\nFamily Satisfaction:\n\n\n\nVariable Name: TS_010B\n\nConcept: Satisfaction with relationships\n\n\nQuestion Text:\nIn general, on a scale from 1 to 5 where 1 means «completely dissatisfied» and 5 means «completely satisfied», how satisfied are you with the relationships you have with the following people? Relatives or family members, excluding those you live with\n\n\n\nAnswer Categories\nCode\n\n\n\n\n1-Completely dissatisfied\n1\n\n\n2\n2\n\n\n3\n3\n\n\n4\n4\n\n\n5- Completely satisfied\n5\n\n\nValid skip\n6\n\n\nDon’t know\n7\n\n\nRefusal\n8\n\n\nNot stated\n9\n\n\n\n\nMental Health:\n\n\n\nVariable Name: FD_030A\n\nConcept: Perceived mental health\n\n\nQuestion Text:\nIn general, how is your mental health? Would you say:\n\n\n\nAnswer Categories\nCode\n\n\n\n\nExcellent\n1\n\n\nVery good\n2\n\n\nGood\n3\n\n\nFair\n4\n\n\nPoor\n5\n\n\nValid skip\n6\n\n\nDon’t know\n7\n\n\nRefusal\n8\n\n\nNot stated\n9\n\n\n\n\nAgain, since I want to measure mental health in terms of how good or bad a person is, it makes sense that better moods are associated with elevated numbers (so instead of it being coded as 1, Excellent should be coded as 5).\n\nds &lt;- ds %&gt;% mutate(\n    FRISAT = case_when(\n        ts_010a == 1 ~ 1, #completely dissatisfied \n        ts_010a == 2 ~ 2, \n        ts_010a == 3 ~ 3,\n        ts_010a == 4 ~ 4,\n        ts_010a == 5 ~ 5, #completely satisfied \n        .default = 6\n    ),\n    \n    FAMSAT = case_when(\n        ts_010b == 1 ~ 1, #completely dissatisfied \n        ts_010b == 2 ~ 2, \n        ts_010b == 3 ~ 3,\n        ts_010b == 4 ~ 4,\n        ts_010b == 5 ~ 5, #completely satisfied \n        .default = 6\n    ),\n    \n    MH = case_when(\n        fd_030a == 1 ~ 5, #excellent \n        fd_030a == 2 ~ 4, #very good \n        fd_030a == 3 ~ 3, #good \n        fd_030a == 4 ~ 2, #fair\n        fd_030a == 5 ~ 1, #poor\n        .default = 6\n    )\n)\n\nds &lt;- ds %&gt;% filter(\n    FRISAT &lt; 6,\n    FAMSAT &lt; 6,\n    MH &lt; 6\n)\n\nSocial Media use:\n\n\n\nVariable Name: UI_010C\n\nConcept: Activities related to communication\n\n\nQuestion Text:\nDuring the past three months, which of the following activities, related to communication, have you done over the Internet? Have you: Used social networking websites or apps\n\n\n\nAnswer Categories\nCode\n\n\n\n\nYes\n1\n\n\nNo\n2\n\n\nValid skip\n6\n\n\nDon’t know\n7\n\n\nRefusal\n8\n\n\nNot stated\n9\n\n\n\n\n\nds &lt;- ds %&gt;% mutate(\n    SNS = case_when(\n        ui_010c == 1 ~ 1, # yes \n        ui_010c == 2 ~ 0, # no \n        .default = 3\n    )\n)\n\nds &lt;- ds %&gt;% filter(\n    SNS &lt; 3\n)\n\nStep 4. Gather Demographic and Other Information Here, I will select some demographic information. Since these are not direct questions but information retrieved from other sources (such as postal codes for province), some of them do not have Skips/Don’t Know/Refusal/Not Stated answers. If they have, I have added those to the cards. I have also added a frequency column for these variables only to show you the demographic distribution of the sample (not weighted).\n\n\n\nVariable Name: PROVINCE\n\nConcept: PROVINCE\n\n\nNote:\nInformation derived using postal codes.\n\n\n\nAnswer Categories\nCode\nFrequency\n\n\n\n\nNewfoundland and Labrador\n10\n950\n\n\nPrince Edward Island\n11\n1,154\n\n\nNova Scotia\n12\n1,181\n\n\nNew Brunswick\n13\n1,223\n\n\nQuebec\n24\n3,911\n\n\nOntario\n35\n3,719\n\n\nManitoba\n46\n973\n\n\nSaskatchewan\n47\n979\n\n\nAlberta\n48\n1,365\n\n\nBritish Columbia\n59\n1,954\n\n\n\n\n\n\n\nVariable Name: AGE_GRP\n\nConcept: Age Groups – Derived variable\n\n\nNote:\nDerived from age of persons in the household\n\n\n\nAnswer Categories\nCode\nFrequency\n\n\n\n\n15 to 24 years\n01\n828\n\n\n25 to 34 years\n02\n1,805\n\n\n35 to 44 years\n03\n2,520\n\n\n45 to 54 years\n04\n2,469\n\n\n55 to 64 years\n05\n3,889\n\n\n65 years and over\n06\n5,898\n\n\n\n\n\n\n\nVariable Name: GENDER\n\nConcept: Gender - Derived variable\n\n\nNote:\nRefers to current gender which may be different from sex assigned at birth and may be different from what is indicated on legal documents. For data quality and confidentiality reasons, and because of the small population being measured, the dissemination of data according to ’Non binary’ Gender is not possible for this statistical program. So, this release uses a gender variable with only two categories. This variable is derived by looking at a large number of demographic characteristics from the respondent, it allows us to disseminate data on Gender that is reliable and unbiased.\n\n\n\nAnswer Categories\nCode\nFrequency\n\n\n\n\nMale\n1\n8,126\n\n\nFemale\n2\n9,283\n\n\n\n\n\n\n\nVariable Name: EMP\n\nConcept: Employment status - Derived variable\n\n\nNote:\n\n\n\nAnswer Categories\nCode\nFrequency\n\n\n\n\nEmployed\n1\n8,451\n\n\nNot Employed\n2\n8,177\n\n\nValid Skip\n6\n0\n\n\nDon’t Know\n7\n0\n\n\nRefusal\n8\n0\n\n\nNot Stated\n9\n781\n\n\n\n\n\n\n\nVariable Name: G_EDU\n\nConcept: Highest certificate - Derived variable\n\n\nNote:\n\n\n\nAnswer Categories\nCode\nFrequency\n\n\n\n\nHigh school or less\n1\n5,746\n\n\nSome post-secondary (incl. univ certificate)\n2\n5,866\n\n\nUniversity degree\n3\n4,889\n\n\nValid skip\n6\n0\n\n\nDon’t know\n7\n0\n\n\nRefusal\n8\n0\n\n\nNot stated\n9\n908\n\n\n\n\n\n\n\nVariable Name: G_HCOMP\n\nConcept: Type of household - Derived variable\n\n\nNote:\nThis derived variable indicates the household composition. It was derived using RRS_Q12 (number of persons in the household), RR_020CA (age of persons in the household) and RR_040AA (relationship of the respondent with the other members of the household).\n\n\n\nAnswer Categories\nCode\nFrequency\n\n\n\n\nFamily household with children under 18\n1\n3,700\n\n\nFamily household without children under 18\n2\n8,299\n\n\nSingle person household\n3\n4,837\n\n\nOther household type\n4\n414\n\n\nValid skip\n6\n0\n\n\nDon’t know\n7\n0\n\n\nRefusal\n8\n0\n\n\nNot stated\n9\n159\n\n\n\n\n\n\n\nVariable Name: HINCQUIN\n\nConcept: Census family income quintile - Derived variable\n\n\nNote:\nInformation derived using HINC. In order to obtain equal weighted counts in each category, cases with incomes equal to the category cutoffs were randomly assigned to one of the two categories on either side of the cutoff.\nSource: Annual Income Estimates for Census Families and Individuals (T1 Family File)\n\n\n\nAnswer Categories\nCode\nFrequency\n\n\n\n\nQuintile 1 - \\leq $44,119\n1\n4,200\n\n\nQuintile 2 - $44,120 - $75,321\n2\n3,911\n\n\nQuintile 3 - $75,322 - $109,431\n3\n3,394\n\n\nQuintile 4 - $109,432 - $162,799\n4\n3,185\n\n\nQuintile 5 - \\geq $162,800\n5\n2,719\n\n\n\n\n\n\n\nVariable Name: IMM_GSTA\n\nConcept: Immigrant status\n\n\nNote:\nLanded immigrants are permanent residents who have indicated a year of landing in Canada since 1980. Variable derived from LANDING_YEAR, IMDB.\nSource: Longitudinal Immigration Database (IMDB)\n\n\n\nAnswer Categories\nCode\nFrequency\n\n\n\n\nLanded immigrant\n1\n1,696\n\n\nNon-landed immigrant\n2\n15,701\n\n\nValid Skip\n6\n0\n\n\nDon’t Know\n7\n0\n\n\nRefusal\n8\n0\n\n\nNot Stated\n9\n12\n\n\n\n\nVariable pumfid is just a unique identifier.\n\nds &lt;- ds %&gt;% mutate(\n    id = pumfid,\n    province = province, \n    AGE = as.integer(age_grp),\n    SEX = gender,\n    EMP = ifelse(\n        emp == 2,\n        0,\n        emp\n    ),\n    EDU = g_edu,\n    FAM = g_hcomp, \n    IMM = ifelse(\n        imm_gsta == 2,\n        0,\n        imm_gsta\n    ),\n    INC = hincquin\n)\n\nds &lt;- ds %&gt;% filter(\n    SNS &lt; 3,\n    EMP &lt; 3,\n    FAM &lt; 5,\n    IMM &lt; 3\n)\n\nStep 5. Selecting The Variables From Data Set\n\nds &lt;- ds %&gt;% \n    dplyr::select(id, \n                  mBanking, SD, FAMSAT, FRISAT, MH, SNS,\n                  province, AGE, SEX, EMP, EDU,\n                  FAM, IMM, INC, wtpg)\n\nSize of the dataset:\n\ndim(ds)\n\n[1] 11176    16\n\n\n\n\n\n\npsych::describe(ds, type = 2)\n\n         vars     n      mean      sd    median   trimmed     mad      min\nid          1 11176 108666.23 5048.86 108637.50 108660.43 6530.85 100001.0\nmBanking    2 11176      0.86    0.34      1.00      0.95    0.00      0.0\nSD          3 11176      3.39    1.22      3.00      3.32    1.48      1.0\nFAMSAT      4 11176      4.27    0.92      5.00      4.42    0.00      1.0\nFRISAT      5 11176      4.28    0.90      5.00      4.42    0.00      1.0\nMH          6 11176      3.59    1.04      4.00      3.65    1.48      1.0\nSNS         7 11176      0.79    0.41      1.00      0.86    0.00      0.0\nprovince    8 11176     32.33   15.79     35.00     31.72   17.79     10.0\nAGE         9 11176      3.98    1.52      4.00      4.06    1.48      1.0\nSEX        10 11176      1.53    0.50      2.00      1.54    0.00      1.0\nEMP        11 11176      0.63    0.48      1.00      0.67    0.00      0.0\nEDU        12 11176      2.11    0.80      2.00      2.13    1.48      1.0\nFAM        13 11176      1.98    0.78      2.00      1.94    1.48      1.0\nIMM        14 11176      0.12    0.32      0.00      0.02    0.00      0.0\nINC        15 11176      3.10    1.37      3.00      3.12    1.48      1.0\nwtpg       16 11176   2054.12 2104.81   1470.54   1681.77 1375.86     18.2\n               max    range  skew kurtosis    se\nid       117407.00 17406.00  0.01    -1.22 47.76\nmBanking      1.00     1.00 -2.12     2.51  0.00\nSD            6.00     5.00  0.33    -0.70  0.01\nFAMSAT        5.00     4.00 -1.30     1.47  0.01\nFRISAT        5.00     4.00 -1.29     1.49  0.01\nMH            5.00     4.00 -0.39    -0.44  0.01\nSNS           1.00     1.00 -1.39    -0.07  0.00\nprovince     59.00    49.00  0.16    -1.13  0.15\nAGE           6.00     5.00 -0.27    -1.02  0.01\nSEX           2.00     1.00 -0.11    -1.99  0.00\nEMP           1.00     1.00 -0.56    -1.69  0.00\nEDU           9.00     8.00  0.27     2.35  0.01\nFAM           4.00     3.00  0.39    -0.37  0.01\nIMM           1.00     1.00  2.35     3.52  0.00\nINC           5.00     4.00 -0.08    -1.22  0.01\nwtpg      23728.02 23709.82  2.67    11.27 19.91\n\n\nSeems like it’s kind of rare for those that use m-banking to have lower MH scores. Descriptive statistics:\n\nggplot(data    = ds,\n       aes(x   = SD,\n           y   = wtpg,\n           color = as.factor(MH)))+ \n  geom_point() +\n  geom_jitter() +  \n  labs( x = \"Smartphone Dependency\", \n        y = \"Weight\", \n        color = \"MH\") + \n  theme_minimal() \n\n\n\n\n\n\n\n\nChecking Na’s:\n\nsum(is.na(ds))\n\n[1] 0\n\n\n\nglimpse(ds)\n\nRows: 11,176\nColumns: 16\n$ id       &lt;int&gt; 100001, 100002, 100003, 100004, 100005, 100007, 100008, 10001…\n$ mBanking &lt;dbl&gt; 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1…\n$ SD       &lt;dbl&gt; 2, 5, 4, 2, 5, 3, 4, 3, 2, 4, 4, 5, 2, 3, 4, 6, 4, 5, 4, 4, 2…\n$ FAMSAT   &lt;dbl&gt; 4, 5, 4, 5, 5, 4, 5, 4, 4, 4, 5, 5, 5, 4, 3, 4, 4, 4, 5, 5, 4…\n$ FRISAT   &lt;dbl&gt; 4, 5, 5, 5, 5, 4, 5, 4, 4, 4, 5, 4, 4, 4, 2, 4, 4, 4, 5, 3, 4…\n$ MH       &lt;dbl&gt; 4, 3, 4, 5, 3, 3, 5, 3, 4, 2, 3, 3, 3, 4, 2, 2, 4, 3, 4, 3, 4…\n$ SNS      &lt;dbl&gt; 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0…\n$ province &lt;int&gt; 59, 35, 35, 24, 48, 13, 48, 12, 35, 48, 13, 12, 24, 46, 24, 2…\n$ AGE      &lt;int&gt; 6, 2, 3, 6, 4, 4, 2, 2, 2, 5, 5, 4, 2, 6, 2, 1, 6, 3, 3, 4, 5…\n$ SEX      &lt;int&gt; 2, 2, 2, 2, 1, 1, 1, 1, 2, 2, 2, 2, 2, 1, 2, 1, 1, 1, 1, 1, 1…\n$ EMP      &lt;dbl&gt; 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1…\n$ EDU      &lt;int&gt; 2, 3, 3, 3, 1, 3, 2, 1, 2, 2, 2, 2, 3, 3, 3, 1, 3, 3, 2, 2, 1…\n$ FAM      &lt;int&gt; 2, 1, 2, 2, 3, 3, 1, 2, 1, 2, 2, 1, 3, 2, 1, 3, 3, 3, 1, 2, 2…\n$ IMM      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0…\n$ INC      &lt;int&gt; 4, 3, 2, 4, 2, 3, 2, 5, 4, 5, 2, 5, 2, 5, 2, 3, 4, 4, 4, 4, 3…\n$ wtpg     &lt;dbl&gt; 2524.87054, 3720.74855, 5802.87880, 1449.85202, 1155.15218, 1…\n\n\n\n\n\nVisualizng the relationship between Mental Health and M-banking:\n\nggplot(data = ds, aes(x = MH, y = wtpg)) +\n            geom_col(\n            aes(fill = as.factor(mBanking)), stat = \"identity\", color = \"black\", position = position_dodge(0.9)) +\n            fill_palette(\"Pastel1\") + \n            xlab(\"Mental Health\") +\n            ylab(\"Frequencies\") + labs(fill = \"Mbanking\")\n\n\n\n\n\n\n\n\nMH and controls: AGE, SEX, EMP, EDU, FAM, IMM, INC\n\ngg_fam &lt;- ggplot(data = ds , aes(MH, fill = as.factor(FAM))) + geom_bar(position = \"fill\") + labs(x = \"Mental Health\", y = \"Percentage (fill)\", fill = 'FAMILY') + fill_palette(\"Pastel1\")\n\ngg_age &lt;- ggplot(data = ds , aes(MH, fill = as.factor(AGE))) + geom_bar(position = \"fill\") + labs(x = \"Mental Health\", y = \"Percentage (fill)\", fill = 'AGE') + fill_palette(\"Pastel1\")\n\ngg_edu &lt;- ggplot(data = ds , aes(MH, fill = as.factor(EDU))) + geom_bar(position = \"fill\") + labs(x = \"Mental Health\", y = \"Percentage (fill)\", fill = 'EDU') + fill_palette(\"Pastel1\")\n\ngg_inc &lt;- ggplot(data = ds , aes(MH, fill = as.factor(INC))) + geom_bar(position = \"fill\") + labs(x = \"Mental Health\", y = \"Percentage (fill)\", fill = 'INC') + fill_palette(\"Pastel1\")\n\ngg_sex &lt;- ggplot(data = ds , aes(MH, fill = as.factor(SEX))) + geom_bar(position = \"fill\") + labs(x = \"Mental Health\", y = \"Percentage (fill)\", fill = 'SEX') + fill_palette(\"Pastel1\")\n\ngg_emp &lt;- ggplot(data = ds , aes(MH, fill = as.factor(EMP))) + geom_bar(position = \"fill\") + labs(x = \"Mental Health\", y = \"Percentage (fill)\", fill = 'EMP') + fill_palette(\"Pastel1\")\n\ngg_imm &lt;- ggplot(data = ds , aes(MH, fill = as.factor(IMM))) + geom_bar(position = \"fill\") + labs(x = \"Mental Health\", y = \"Percentage (fill)\", fill = 'Immigrant') + fill_palette(\"Pastel1\")\n\n\nggarrange(\n    gg_fam, gg_age, gg_edu, gg_inc, gg_sex, gg_emp, gg_imm,\n    labels = c(\"FAM\", \"AGE\", \"EDU\", \"INC\", \"SEX\", \"EMP\", \"IMM\"),\n    ncol = 3,\n    nrow = 3\n    \n) \n\n\n\n\n\n\n\n\nMH and other variables: SD, FAMSAT, FRISAT, SNS\n\ngg_frisat &lt;- ggplot(data = ds , aes(MH, fill = as.factor(FRISAT))) + geom_bar(position = \"fill\") + labs(x = \"Mental Health\", y = \"Percentage (fill)\", fill = 'FRISAT') + fill_palette(\"Pastel1\")\n\ngg_famsat &lt;- ggplot(data = ds , aes(MH, fill = as.factor(FAMSAT))) + geom_bar(position = \"fill\") + labs(x = \"Mental Health\", y = \"Percentage (fill)\", fill = 'FAMSAT') + fill_palette(\"Pastel1\")\n\ngg_sd &lt;- ggplot(data = ds , aes(MH, fill = as.factor(SD))) + geom_bar(position = \"fill\") + labs(x = \"Mental Health\", y = \"Percentage (fill)\", fill = 'SD') + fill_palette(\"Pastel1\")\n\ngg_sns &lt;- ggplot(data = ds , aes(MH, fill = as.factor(SNS))) + geom_bar(position = \"fill\") + labs(x = \"Mental Health\", y = \"Percentage (fill)\", fill = 'SNS') + fill_palette(\"Pastel1\")\n\nggarrange(\n    gg_frisat, gg_famsat, gg_sd, gg_sns,\n    labels = c(\"FRISAT\", \"FAMSAT\", \"SD\", \"SNS\"),\n    ncol = 2,\n    nrow = 2\n    \n) \n\n\n\n\n\n\n\n\n\n\nAll variables\n\nsle &lt;- ds %&gt;% dplyr::select(mBanking, SD, FAMSAT, FRISAT, SNS, AGE, SEX, EMP, EDU, FAM, INC, MH)\ncorM &lt;- Hmisc::rcorr(as.matrix(sle))\nreg_corM &lt;- as.matrix(corM$r)\n\ncolnames(reg_corM) &lt;- c(\"mBanking\", \"SD\", \"FAMSAT\", \"FRISAT\", \"SNS\", \"AGE\", \"SEX\", \"EMP\", \"EDU\", \"FAM\", \"INC\", \"MH\")\n# \nrownames(reg_corM) &lt;- c(\"mBanking\", \"SD\", \"FAMSAT\", \"FRISAT\", \"SNS\", \"AGE\", \"SEX\", \"EMP\", \"EDU\", \"FAM\", \"INC\", \"MH\")\n# \ncorrplot::corrplot(reg_corM, p.mat = corM$P, method = \"color\", type = \"upper\", insig = 'label_sig', sig.level = c(0.001, 0.01, 0.05), pch.cex = 0.9, order = 'AOE', tl.col = \"black\", tl.cex = 1, diag = F, col = corrplot::COL2('PuOr'))\n\n\n\n\n\n\n\n\nFRISAT and FAMSAT are obviously highly correlated. I’ll combine them into a new variable called RS for Relationship satisfaction.\n\nds &lt;- ds %&gt;% mutate(\n    RS = FAMSAT + FRISAT\n)\n\n\n\nUsing a Likelihood Ratio Test:\n\nmodel_famsatfrisat &lt;- glm(mBanking ~ FAMSAT + FRISAT, \n                          family = \"binomial\",\n                          data = ds)\n\nmodel_rs &lt;- glm(mBanking ~ RS, \n                          family = \"binomial\",\n                          data = ds)   \n\nanova(model_rs, model_famsatfrisat, test = \"Chisq\")               \n\nAnalysis of Deviance Table\n\nModel 1: mBanking ~ RS\nModel 2: mBanking ~ FAMSAT + FRISAT\n  Resid. Df Resid. Dev Df  Deviance Pr(&gt;Chi)\n1     11174     8890.9                      \n2     11173     8890.9  1 0.0045091   0.9465\n\n\nSince p &gt; 0.05, there is no significant difference in model performance. It’s better to keep one variable instead of two (simpler model is better).\nContinuing with our visualizations, let’s see the changes to SD across MH scores.\n\nggplot(data  = ds, aes(x = MH, y = SD))+ geom_point(size = 1.2, alpha = .5, position = \"jitter\") + \n    geom_smooth(method = lm,\n              se     = FALSE, \n              col    = \"red\",\n              size   = 2, \n              alpha  = .8)+ # to add regression line\n  theme_minimal()\n\n\n\n\n\n\n\n\nThere’s an almost negative linear relationship there: better mental health is associated with lower dependency on smartphones. This intuitively makes sense. Adding more nuance to this graph, I can see how this relationship changes for people who adopted m-banking and those that didn’t. It seems the two relationships are almost identical, at least in direction.\n\nggplot(data    = ds,\n       aes(x   = MH,\n           y   = SD,\n           col = as.factor(mBanking)))+ #to add the colours for different classes\n    geom_point(size     = 1.2,\n             alpha    = .8,\n             position = \"jitter\")+ \n    #scale_color_manual(name = \"MBanking\",\n                     #labels = c(\"No\", \"Yes\"),\n                     #values = c(\"red\", \"lightblue\")) + #+ ggnewscale::new_scale_color() +\n    geom_smooth(method   = lm,\n              se       = T, \n              size     = 1.5, \n              linetype = 1, \n              alpha    = .7,\n              ) + \n    scale_color_manual(name = \"MBanking\",\n                     labels = c(\"No\", \"Yes\"),\n                     values = c(\"black\", \"darkred\")) + theme_minimal()\n\n\n\n\n\n\n\n\nThe same type of visualizations can be helpful for other factors in the model. Now, I will separate the data into those that have adopted m-banking and those that haven’t. Let’s look at the differences in familial relationship satisfaction for these two groups across mental health. For m-banking adopters, it seems that those with better mental health (3 or higher) are completely satisfied with their family relationships (blue bar). On the other hand, those that haven’t adopted m-banking are either less satisfied or completely unsatisfied (MH = 5, mBanking = 0 group’s largest bar is red, which is FAMSAT = 1, the lowest score).\n\nds1 &lt;- ds %&gt;% filter(mBanking == 1)\nds2 &lt;- ds %&gt;% filter(mBanking == 0)\n\ngg5 &lt;- ggplot(data = ds1, aes(x = MH, y = wtpg)) +\n            geom_col(\n            aes(fill = as.factor(FAMSAT)), stat = \"identity\", color = \"black\", position = position_dodge(0.9)) +\n            fill_palette(\"RdBu\") + \n            xlab(\"Mental Health\") +\n            ylab(\"Weights\") + labs(fill = \"FAMSAT\")\n\ngg6 &lt;- ggplot(data = ds2 %&gt;% filter(mBanking == 0), aes(x = MH, y = wtpg)) +\n            geom_col(\n            aes(fill = as.factor(FAMSAT)), stat = \"identity\", color = \"black\", position = position_dodge(0.9)) +\n            fill_palette(\"RdBu\") + \n            xlab(\"Mental Health\") +\n            ylab(\"Weights\") + labs(fill = \"FAMSAT\")\n\nggarrange(gg5, gg6, ncol = 2, labels = c(\"mBanking = 1\", \"mBanking = 0\"))\n\n\n\n\n\n\n\n\nIt’s important to mention that visualizations don’t actually tell us the real story. Especially when working with large datasets with more than 10,000 data points. This is simply a somewhat visible pattern that tells me it’s worth looking into when I do my statistical analysis. We do the same type of visualization, now measuring social media use. Surprisingly, social media use goes up with better mental health scores for adopters but decreases for non-adopters.\n\ngg7 &lt;- ggplot(data = ds1, aes(x = MH, y = wtpg)) +\n            geom_col(\n            aes(fill = as.factor(SNS)), stat = \"identity\", color = \"black\", position = position_dodge(0.9)) +\n            fill_palette(\"RdBu\") + \n            xlab(\"Mental Health\") +\n            ylab(\"Weights\") + labs(fill = \"SNS\")\n\ngg8 &lt;- ggplot(data = ds2 %&gt;% filter(mBanking == 0), aes(x = MH, y = wtpg)) +\n            geom_col(\n            aes(fill = as.factor(SNS)), stat = \"identity\", color = \"black\", position = position_dodge(0.9)) +\n            fill_palette(\"RdBu\") + \n            xlab(\"Mental Health\") +\n            ylab(\"Weights\") + labs(fill = \"SNS\")\n\nggarrange(gg7, gg8, ncol = 2, labels = c(\"mBanking = 1\", \"mBanking = 0\"))\n\n\n\n\n\n\n\n\nLastly, we do the same with smartphone dependency. It seems that non-adopters are actually more dependent on their smartphones and also (with better mental health).\n\ngg9 &lt;- ggplot(data = ds1, aes(x = MH, y = wtpg)) +\n            geom_col(\n            aes(fill = as.factor(SD)), stat = \"identity\", color = \"black\", position = position_dodge(0.9)) +\n            fill_palette(\"RdBu\") + \n            xlab(\"Mental Health\") +\n            ylab(\"Weights\") + labs(fill = \"SD\")\n\ngg10 &lt;- ggplot(data = ds2 %&gt;% filter(mBanking == 0), aes(x = MH, y = wtpg)) +\n            geom_col(\n            aes(fill = as.factor(SD)), stat = \"identity\", color = \"black\", position = position_dodge(0.9)) +\n            fill_palette(\"RdBu\") + \n            xlab(\"Mental Health\") +\n            ylab(\"Weights\") + labs(fill = \"SD\")\n\nggarrange(gg9, gg10, ncol = 2, labels = c(\"mBanking = 1\", \"mBanking = 0\"))",
    "crumbs": [
      "Home",
      "Projects",
      "Project 2. Mental Health",
      "Effect Of Mental Health on Mobile Banking"
    ]
  },
  {
    "objectID": "study2_DA.html#screening",
    "href": "study2_DA.html#screening",
    "title": "Data Analysis - CIUS 2020",
    "section": "SCREENING",
    "text": "SCREENING\n\npsych::describe(ds, type = 2)\n\n         vars     n      mean      sd    median   trimmed     mad      min\nid          1 11176 108666.23 5048.86 108637.50 108660.43 6530.85 100001.0\nmBanking    2 11176      0.86    0.34      1.00      0.95    0.00      0.0\nSD          3 11176      3.39    1.22      3.00      3.32    1.48      1.0\nFAMSAT      4 11176      4.27    0.92      5.00      4.42    0.00      1.0\nFRISAT      5 11176      4.28    0.90      5.00      4.42    0.00      1.0\nMH          6 11176      3.59    1.04      4.00      3.65    1.48      1.0\nSNS         7 11176      0.79    0.41      1.00      0.86    0.00      0.0\nprovince    8 11176     32.33   15.79     35.00     31.72   17.79     10.0\nAGE*        9 11176      3.98    1.52      4.00      4.06    1.48      1.0\nSEX        10 11176      1.53    0.50      2.00      1.54    0.00      1.0\nEMP        11 11176      1.37    0.48      1.00      1.33    0.00      1.0\nEDU        12 11176      2.11    0.80      2.00      2.13    1.48      1.0\nFAM        13 11176      1.98    0.78      2.00      1.94    1.48      1.0\nIMM        14 11176      1.88    0.32      2.00      1.98    0.00      1.0\nINC        15 11176      3.10    1.37      3.00      3.12    1.48      1.0\nwtpg       16 11176   2054.12 2104.81   1470.54   1681.77 1375.86     18.2\n               max    range  skew kurtosis    se\nid       117407.00 17406.00  0.01    -1.22 47.76\nmBanking      1.00     1.00 -2.12     2.51  0.00\nSD            6.00     5.00  0.33    -0.70  0.01\nFAMSAT        5.00     4.00 -1.30     1.47  0.01\nFRISAT        5.00     4.00 -1.29     1.49  0.01\nMH            5.00     4.00 -0.39    -0.44  0.01\nSNS           1.00     1.00 -1.39    -0.07  0.00\nprovince     59.00    49.00  0.16    -1.13  0.15\nAGE*          6.00     5.00 -0.27    -1.02  0.01\nSEX           2.00     1.00 -0.11    -1.99  0.00\nEMP           2.00     1.00  0.56    -1.69  0.00\nEDU           9.00     8.00  0.27     2.35  0.01\nFAM           4.00     3.00  0.39    -0.37  0.01\nIMM           2.00     1.00 -2.35     3.52  0.00\nINC           5.00     4.00 -0.08    -1.22  0.01\nwtpg      23728.02 23709.82  2.67    11.27 19.91\n\n\nSeems like it’s kind of rare for those that use m-banking to have lower MH scores. Descriptive statistics:\n\nggplot(data    = ds,\n       aes(x   = SD,\n           y   = wtpg,\n           color = as.factor(MH)))+ \n  geom_point() +\n  geom_jitter() +  \n  labs( x = \"Smartphone Dependency\", \n        y = \"Weight\", \n        color = \"MH\") + \n  theme_minimal() \n\n\n\n\n\n\n\n\nChecking Na’s:\n\nsum(is.na(ds))\n\n[1] 0\n\n\n\nglimpse(ds)\n\nRows: 11,176\nColumns: 16\n$ id       &lt;dbl&gt; 100001, 100002, 100003, 100004, 100005, 100007, 100008, 10001…\n$ mBanking &lt;dbl&gt; 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1…\n$ SD       &lt;dbl&gt; 2, 5, 4, 2, 5, 3, 4, 3, 2, 4, 4, 5, 2, 3, 4, 6, 4, 5, 4, 4, 2…\n$ FAMSAT   &lt;dbl&gt; 4, 5, 4, 5, 5, 4, 5, 4, 4, 4, 5, 5, 5, 4, 3, 4, 4, 4, 5, 5, 4…\n$ FRISAT   &lt;dbl&gt; 4, 5, 5, 5, 5, 4, 5, 4, 4, 4, 5, 4, 4, 4, 2, 4, 4, 4, 5, 3, 4…\n$ MH       &lt;dbl&gt; 4, 3, 4, 5, 3, 3, 5, 3, 4, 2, 3, 3, 3, 4, 2, 2, 4, 3, 4, 3, 4…\n$ SNS      &lt;dbl&gt; 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0…\n$ province &lt;dbl&gt; 59, 35, 35, 24, 48, 13, 48, 12, 35, 48, 13, 12, 24, 46, 24, 2…\n$ AGE      &lt;chr&gt; \"06\", \"02\", \"03\", \"06\", \"04\", \"04\", \"02\", \"02\", \"02\", \"05\", \"…\n$ SEX      &lt;dbl&gt; 2, 2, 2, 2, 1, 1, 1, 1, 2, 2, 2, 2, 2, 1, 2, 1, 1, 1, 1, 1, 1…\n$ EMP      &lt;dbl&gt; 2, 1, 1, 2, 2, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1…\n$ EDU      &lt;dbl&gt; 2, 3, 3, 3, 1, 3, 2, 1, 2, 2, 2, 2, 3, 3, 3, 1, 3, 3, 2, 2, 1…\n$ FAM      &lt;dbl&gt; 2, 1, 2, 2, 3, 3, 1, 2, 1, 2, 2, 1, 3, 2, 1, 3, 3, 3, 1, 2, 2…\n$ IMM      &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2…\n$ INC      &lt;dbl&gt; 4, 3, 2, 4, 2, 3, 2, 5, 4, 5, 2, 5, 2, 5, 2, 3, 4, 4, 4, 4, 3…\n$ wtpg     &lt;dbl&gt; 2524.87054, 3720.74855, 5802.87880, 1449.85202, 1155.15218, 1…",
    "crumbs": [
      "Home",
      "Projects",
      "Project 2. Mental Health",
      "Data Analysis - CIUS 2020"
    ]
  },
  {
    "objectID": "study2_DA.html#relationships-visualizations-contingency-correlations",
    "href": "study2_DA.html#relationships-visualizations-contingency-correlations",
    "title": "Data Analysis - CIUS 2020",
    "section": "RELATIONSHIPS: Visualizations, Contingency, Correlations",
    "text": "RELATIONSHIPS: Visualizations, Contingency, Correlations\nMental Health and Mbanking:\n\nggplot(data = ds, aes(x = MH, y = wtpg)) +\n            geom_col(\n            aes(fill = as.factor(mBanking)), stat = \"identity\", color = \"black\", position = position_dodge(0.9)) +\n            fill_palette(\"Pastel1\") + \n            xlab(\"Mental Health\") +\n            ylab(\"Frequencies\") + labs(fill = \"Mbanking\")\n\n\n\n\n\n\n\n\nMH and controls: AGE, SEX, EMP, EDU, FAM, IMM, INC\n\ngg_fam &lt;- ggplot(data = ds , aes(MH, fill = as.factor(FAM))) + geom_bar(position = \"fill\") + labs(x = \"Mental Health\", y = \"Percentage (fill)\", fill = 'FAMILY') + fill_palette(\"Pastel1\")\n\ngg_age &lt;- ggplot(data = ds , aes(MH, fill = as.factor(AGE))) + geom_bar(position = \"fill\") + labs(x = \"Mental Health\", y = \"Percentage (fill)\", fill = 'AGE') + fill_palette(\"Pastel1\")\n\ngg_edu &lt;- ggplot(data = ds , aes(MH, fill = as.factor(EDU))) + geom_bar(position = \"fill\") + labs(x = \"Mental Health\", y = \"Percentage (fill)\", fill = 'EDU') + fill_palette(\"Pastel1\")\n\ngg_inc &lt;- ggplot(data = ds , aes(MH, fill = as.factor(INC))) + geom_bar(position = \"fill\") + labs(x = \"Mental Health\", y = \"Percentage (fill)\", fill = 'INC') + fill_palette(\"Pastel1\")\n\ngg_sex &lt;- ggplot(data = ds , aes(MH, fill = as.factor(SEX))) + geom_bar(position = \"fill\") + labs(x = \"Mental Health\", y = \"Percentage (fill)\", fill = 'SEX') + fill_palette(\"Pastel1\")\n\ngg_emp &lt;- ggplot(data = ds , aes(MH, fill = as.factor(EMP))) + geom_bar(position = \"fill\") + labs(x = \"Mental Health\", y = \"Percentage (fill)\", fill = 'EMP') + fill_palette(\"Pastel1\")\n\ngg_imm &lt;- ggplot(data = ds , aes(MH, fill = as.factor(IMM))) + geom_bar(position = \"fill\") + labs(x = \"Mental Health\", y = \"Percentage (fill)\", fill = 'Immigrant') + fill_palette(\"Pastel1\")\n\n\nggarrange(\n    gg_fam, gg_age, gg_edu, gg_inc, gg_sex, gg_emp, gg_imm,\n    labels = c(\"FAM\", \"AGE\", \"EDU\", \"INC\", \"SEX\", \"EMP\", \"IMM\"),\n    ncol = 3,\n    nrow = 3\n    \n) \n\n\n\n\n\n\n\n\nMH and other variables: SD, FAMSAT, FRISAT, SNS\n\ngg_frisat &lt;- ggplot(data = ds , aes(MH, fill = as.factor(FRISAT))) + geom_bar(position = \"fill\") + labs(x = \"Mental Health\", y = \"Percentage (fill)\", fill = 'FRISAT') + fill_palette(\"Pastel1\")\n\ngg_famsat &lt;- ggplot(data = ds , aes(MH, fill = as.factor(FAMSAT))) + geom_bar(position = \"fill\") + labs(x = \"Mental Health\", y = \"Percentage (fill)\", fill = 'FAMSAT') + fill_palette(\"Pastel1\")\n\ngg_sd &lt;- ggplot(data = ds , aes(MH, fill = as.factor(SD))) + geom_bar(position = \"fill\") + labs(x = \"Mental Health\", y = \"Percentage (fill)\", fill = 'SD') + fill_palette(\"Pastel1\")\n\ngg_sns &lt;- ggplot(data = ds , aes(MH, fill = as.factor(SNS))) + geom_bar(position = \"fill\") + labs(x = \"Mental Health\", y = \"Percentage (fill)\", fill = 'SNS') + fill_palette(\"Pastel1\")\n\nggarrange(\n    gg_frisat, gg_famsat, gg_sd, gg_sns,\n    labels = c(\"FRISAT\", \"FAMSAT\", \"SD\", \"SNS\"),\n    ncol = 2,\n    nrow = 2\n    \n) \n\n\n\n\n\n\n\n\n\nsle &lt;- ds %&gt;% dplyr::select(mBanking, SD, FAMSAT, FRISAT, SNS, AGE, SEX, EMP, EDU, FAM, INC, MH)\ncorM &lt;- Hmisc::rcorr(as.matrix(sle))\nreg_corM &lt;- as.matrix(corM$r)\n\ncolnames(reg_corM) &lt;- c(\"mBanking\", \"SD\", \"FAMSAT\", \"FRISAT\", \"SNS\", \"AGE\", \"SEX\", \"EMP\", \"EDU\", \"FAM\", \"INC\", \"MH\")\n# \nrownames(reg_corM) &lt;- c(\"mBanking\", \"SD\", \"FAMSAT\", \"FRISAT\", \"SNS\", \"AGE\", \"SEX\", \"EMP\", \"EDU\", \"FAM\", \"INC\", \"MH\")\n# \ncorrplot::corrplot(reg_corM, p.mat = corM$P, method = \"color\", type = \"upper\", insig = 'label_sig', sig.level = c(0.001, 0.01, 0.05), pch.cex = 0.9, order = 'AOE', tl.col = \"black\", tl.cex = 1, diag = F, col = corrplot::COL2('PuOr'))\n\n\n\n\n\n\n\n\nFRISAT and FAMSAT are obviously highly correlated. I’ll combine them into a new variable called RS for Relationship satisfaction.\n\nds &lt;- ds %&gt;% mutate(\n    RS = FAMSAT + FRISAT\n)\n\n\nggplot(data  = ds, aes(x = MH, y = SD))+ geom_point(size = 1.2, alpha = .5, position = \"jitter\") + \n    geom_smooth(method = lm,\n              se     = FALSE, \n              col    = \"red\",\n              size   = 2, \n              alpha  = .8)+ # to add regression line\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nggplot(data    = ds,\n       aes(x   = MH,\n           y   = SD,\n           col = as.factor(mBanking)))+ #to add the colours for different classes\n  geom_point(size     = 1.2,\n             alpha    = .8,\n             position = \"jitter\")+ #to add some random noise for plotting purposes\n  theme_minimal() + scale_color_manual(name = \"MBanking\",\n                     labels = c(\"No\", \"Yes\"),\n                     values = c(\"red\", \"lightblue\"))\n\n\n\n\n\n\n\n\n\nggplot(data    = ds,\n       aes(x   = MH,\n           y   = SD,\n           col = as.factor(mBanking)))+ #to add the colours for different classes\n    geom_point(size     = 1.2,\n             alpha    = .8,\n             position = \"jitter\")+ \n    #scale_color_manual(name = \"MBanking\",\n                     #labels = c(\"No\", \"Yes\"),\n                     #values = c(\"red\", \"lightblue\")) + #+ ggnewscale::new_scale_color() +\n    geom_smooth(method   = lm,\n              se       = T, \n              size     = 1.5, \n              linetype = 1, \n              alpha    = .7,\n              ) + \n    scale_color_manual(name = \"MBanking\",\n                     labels = c(\"No\", \"Yes\"),\n                     values = c(\"black\", \"darkred\")) + theme_minimal()\n\n\n\n\n\n\n\n\n\nggplot(data    = ds,\n       aes(x   = AGE, #CHANGE THIS &lt;&lt;&lt;\n           y   = MH, #CHANGE THIS &lt;&lt;&lt;\n           col = as.factor(mBanking)))+ #to add the colours for different classes\n    geom_point(size     = 1.2,\n             alpha    = .8,\n             position = \"jitter\")+ \n    #scale_color_manual(name = \"MBanking\",\n                     #labels = c(\"No\", \"Yes\"),\n                     #values = c(\"red\", \"lightblue\")) + #+ ggnewscale::new_scale_color() +\n    geom_smooth(method   = lm,\n              se       = T, \n              size     = 1.5, \n              linetype = 1, \n              alpha    = .7,\n              ) + \n    scale_color_manual(name = \"MBanking\",\n                     labels = c(\"No\", \"Yes\"),\n                     values = c(\"black\", \"grey70\")) + theme_minimal()\n\n\n\n\n\n\n\n\n\nds1 &lt;- ds %&gt;% filter(mBanking == 1)\nds2 &lt;- ds %&gt;% filter(mBanking == 0)\n\ngg5 &lt;- ggplot(data = ds1, aes(x = MH, y = wtpg)) +\n            geom_col(\n            aes(fill = as.factor(FAMSAT)), stat = \"identity\", color = \"black\", position = position_dodge(0.9)) +\n            fill_palette(\"RdBu\") + \n            xlab(\"Mental Health\") +\n            ylab(\"Weights\") + labs(fill = \"FAMSAT\")\n\ngg6 &lt;- ggplot(data = ds2 %&gt;% filter(mBanking == 0), aes(x = MH, y = wtpg)) +\n            geom_col(\n            aes(fill = as.factor(FAMSAT)), stat = \"identity\", color = \"black\", position = position_dodge(0.9)) +\n            fill_palette(\"RdBu\") + \n            xlab(\"Mental Health\") +\n            ylab(\"Weights\") + labs(fill = \"FAMSAT\")\n\nggarrange(gg5, gg6, ncol = 2, labels = c(\"mBanking = 1\", \"mBanking = 0\"))\n\n\n\n\n\n\n\n\n\ngg7 &lt;- ggplot(data = ds1, aes(x = MH, y = wtpg)) +\n            geom_col(\n            aes(fill = as.factor(SNS)), stat = \"identity\", color = \"black\", position = position_dodge(0.9)) +\n            fill_palette(\"RdBu\") + \n            xlab(\"Mental Health\") +\n            ylab(\"Weights\") + labs(fill = \"SNS\")\n\ngg8 &lt;- ggplot(data = ds2 %&gt;% filter(mBanking == 0), aes(x = MH, y = wtpg)) +\n            geom_col(\n            aes(fill = as.factor(SNS)), stat = \"identity\", color = \"black\", position = position_dodge(0.9)) +\n            fill_palette(\"RdBu\") + \n            xlab(\"Mental Health\") +\n            ylab(\"Weights\") + labs(fill = \"SNS\")\n\nggarrange(gg7, gg8, ncol = 2, labels = c(\"mBanking = 1\", \"mBanking = 0\"))\n\n\n\n\n\n\n\n\n\ngg9 &lt;- ggplot(data = ds1, aes(x = MH, y = wtpg)) +\n            geom_col(\n            aes(fill = as.factor(SD)), stat = \"identity\", color = \"black\", position = position_dodge(0.9)) +\n            fill_palette(\"RdBu\") + \n            xlab(\"Mental Health\") +\n            ylab(\"Weights\") + labs(fill = \"SD\")\n\ngg10 &lt;- ggplot(data = ds2 %&gt;% filter(mBanking == 0), aes(x = MH, y = wtpg)) +\n            geom_col(\n            aes(fill = as.factor(SD)), stat = \"identity\", color = \"black\", position = position_dodge(0.9)) +\n            fill_palette(\"RdBu\") + \n            xlab(\"Mental Health\") +\n            ylab(\"Weights\") + labs(fill = \"SD\")\n\nggarrange(gg9, gg10, ncol = 2, labels = c(\"mBanking = 1\", \"mBanking = 0\"))",
    "crumbs": [
      "Home",
      "Projects",
      "Project 2. Mental Health",
      "Data Analysis - CIUS 2020"
    ]
  },
  {
    "objectID": "study2.html",
    "href": "study2.html",
    "title": "Mental Health and Mobile Banking",
    "section": "",
    "text": "In this chapter, I look into how mental health might affect whether people choose to adopt mobile banking or not. While doing my review of past studies (refer to study 1 ), I noticed that a lot of research talks about how things like trust, risk, and perceived ease of use affect adoption. But very few studies look at how a person’s mental and emotional state influences their behavior. Especially because some studies actively showed how not accounting for people’s emotions were sometiems giving them strange results.\nI also found that even if the literature did talk about emotions, it was mostly about emotions as a result of the adoption/use of a technology. Not about how your general mood or feelings or struggles impact how you behave and interact with technology. Like I know when I’m stressed out or dreading something, the doom scrolling gets worse! But social media apps are built to be addictive and drag you in and make you want it more. Until you actually stop enjoying it and being online is no longer fun. In fact, I read just recently how engagement and use of social media apps has dropped in recent years! I’ll find the citation for it later! Anyway, I digress… Point is, I think your mood definitely affects how you interact with the world and technology is part of that world. So, what about people who have special moods? That is, certain behavioral or mental challenges? Not exactly disabilities, but hidden challenges often ignored!\nI wanted to explore whether people who are feeling mentally well are more or less likely to use mobile banking. My guess going in was that people with better mental health might feel more confident and more open to using this tech. But I found out it was actually the opposite! Then I got to thinking… why is that? Well, Here’s my guess:\n\nMaybe people with poorer mental health actually rely on mobile banking to avoid going out and talking to people, or dealing with stress in traditional banking.\nMaybe people who are happy don’t bank and don’t stress about money!\nIt’s not a causation, and I don’t know which comes first - are people who bank less happier or is it that happy people don’t bank as often? This is not the study to answer that. But that would be a cool study!\n\nI used Canadian survey data and looked at three things related to mental health:\n\nHow satisfied are you in your relationships (RS)\nHow dependent are you on your smartphones (SD)\nWhether or not you use social media (SNS)\n\nI also tested whether these three things change (or “moderate”) how mental health affects mobile banking adoption. This chapter is where I introduce mental health as a new factor in mobile banking adoption models. I show how mental and emotional states might be just as important as the usual stuff like trust or app features. The results surprised me, and hopefully, they’ll give banks and researchers something new to think about too.\nBy the way, this chapter was already published in .",
    "crumbs": [
      "Home",
      "Projects",
      "Project 2. Mental Health"
    ]
  },
  {
    "objectID": "study2_DA.html#exploring-data",
    "href": "study2_DA.html#exploring-data",
    "title": "Effect Of Mental Health on Mobile Banking",
    "section": "EXPLORING DATA",
    "text": "EXPLORING DATA\n\nSCREENING\n\npsych::describe(ds, type = 2) %&gt;% dplyr::select(n, mean, sd, median, min, max)\n\nSeems like it’s kind of rare for those that use m-banking to have lower MH scores. Descriptive statistics:\n\nggplot(data    = ds,\n       aes(x   = SD,\n           y   = wtpg,\n           color = as.factor(MH)))+ \n  geom_point() +\n  geom_jitter() +  \n  labs( x = \"Smartphone Dependency\", \n        y = \"Weight\", \n        color = \"MH\") + \n  theme_minimal() \n\nChecking Na’s:\n\nsum(is.na(ds))\n\n\nglimpse(ds)\n\n\n\nRELATIONSHIPS: Visualizations, Contingency, Correlations\nMental Health and Mbanking:\n\nggplot(data = ds, aes(x = MH, y = wtpg)) +\n            geom_col(\n            aes(fill = as.factor(mBanking)), stat = \"identity\", color = \"black\", position = position_dodge(0.9)) +\n            fill_palette(\"Pastel1\") + \n            xlab(\"Mental Health\") +\n            ylab(\"Frequencies\") + labs(fill = \"Mbanking\")\n\nMH and controls: AGE, SEX, EMP, EDU, FAM, IMM, INC\n\ngg_fam &lt;- ggplot(data = ds , aes(MH, fill = as.factor(FAM))) + geom_bar(position = \"fill\") + labs(x = \"Mental Health\", y = \"Percentage (fill)\", fill = 'FAMILY') + fill_palette(\"Pastel1\")\n\ngg_age &lt;- ggplot(data = ds , aes(MH, fill = as.factor(AGE))) + geom_bar(position = \"fill\") + labs(x = \"Mental Health\", y = \"Percentage (fill)\", fill = 'AGE') + fill_palette(\"Pastel1\")\n\ngg_edu &lt;- ggplot(data = ds , aes(MH, fill = as.factor(EDU))) + geom_bar(position = \"fill\") + labs(x = \"Mental Health\", y = \"Percentage (fill)\", fill = 'EDU') + fill_palette(\"Pastel1\")\n\ngg_inc &lt;- ggplot(data = ds , aes(MH, fill = as.factor(INC))) + geom_bar(position = \"fill\") + labs(x = \"Mental Health\", y = \"Percentage (fill)\", fill = 'INC') + fill_palette(\"Pastel1\")\n\ngg_sex &lt;- ggplot(data = ds , aes(MH, fill = as.factor(SEX))) + geom_bar(position = \"fill\") + labs(x = \"Mental Health\", y = \"Percentage (fill)\", fill = 'SEX') + fill_palette(\"Pastel1\")\n\ngg_emp &lt;- ggplot(data = ds , aes(MH, fill = as.factor(EMP))) + geom_bar(position = \"fill\") + labs(x = \"Mental Health\", y = \"Percentage (fill)\", fill = 'EMP') + fill_palette(\"Pastel1\")\n\ngg_imm &lt;- ggplot(data = ds , aes(MH, fill = as.factor(IMM))) + geom_bar(position = \"fill\") + labs(x = \"Mental Health\", y = \"Percentage (fill)\", fill = 'Immigrant') + fill_palette(\"Pastel1\")\n\n\nggarrange(\n    gg_fam, gg_age, gg_edu, gg_inc, gg_sex, gg_emp, gg_imm,\n    labels = c(\"FAM\", \"AGE\", \"EDU\", \"INC\", \"SEX\", \"EMP\", \"IMM\"),\n    ncol = 3,\n    nrow = 3\n    \n) \n\nMH and other variables: SD, FAMSAT, FRISAT, SNS\n\ngg_frisat &lt;- ggplot(data = ds , aes(MH, fill = as.factor(FRISAT))) + geom_bar(position = \"fill\") + labs(x = \"Mental Health\", y = \"Percentage (fill)\", fill = 'FRISAT') + fill_palette(\"Pastel1\")\n\ngg_famsat &lt;- ggplot(data = ds , aes(MH, fill = as.factor(FAMSAT))) + geom_bar(position = \"fill\") + labs(x = \"Mental Health\", y = \"Percentage (fill)\", fill = 'FAMSAT') + fill_palette(\"Pastel1\")\n\ngg_sd &lt;- ggplot(data = ds , aes(MH, fill = as.factor(SD))) + geom_bar(position = \"fill\") + labs(x = \"Mental Health\", y = \"Percentage (fill)\", fill = 'SD') + fill_palette(\"Pastel1\")\n\ngg_sns &lt;- ggplot(data = ds , aes(MH, fill = as.factor(SNS))) + geom_bar(position = \"fill\") + labs(x = \"Mental Health\", y = \"Percentage (fill)\", fill = 'SNS') + fill_palette(\"Pastel1\")\n\nggarrange(\n    gg_frisat, gg_famsat, gg_sd, gg_sns,\n    labels = c(\"FRISAT\", \"FAMSAT\", \"SD\", \"SNS\"),\n    ncol = 2,\n    nrow = 2\n    \n) \n\n\nsle &lt;- ds %&gt;% dplyr::select(mBanking, SD, FAMSAT, FRISAT, SNS, AGE, SEX, EMP, EDU, FAM, INC, MH)\ncorM &lt;- Hmisc::rcorr(as.matrix(sle))\nreg_corM &lt;- as.matrix(corM$r)\n\ncolnames(reg_corM) &lt;- c(\"mBanking\", \"SD\", \"FAMSAT\", \"FRISAT\", \"SNS\", \"AGE\", \"SEX\", \"EMP\", \"EDU\", \"FAM\", \"INC\", \"MH\")\n# \nrownames(reg_corM) &lt;- c(\"mBanking\", \"SD\", \"FAMSAT\", \"FRISAT\", \"SNS\", \"AGE\", \"SEX\", \"EMP\", \"EDU\", \"FAM\", \"INC\", \"MH\")\n# \ncorrplot::corrplot(reg_corM, p.mat = corM$P, method = \"color\", type = \"upper\", insig = 'label_sig', sig.level = c(0.001, 0.01, 0.05), pch.cex = 0.9, order = 'AOE', tl.col = \"black\", tl.cex = 1, diag = F, col = corrplot::COL2('PuOr'))\n\nFRISAT and FAMSAT are obviously highly correlated. I’ll combine them into a new variable called RS for Relationship satisfaction.\n\nds &lt;- ds %&gt;% mutate(\n    RS = FAMSAT + FRISAT\n)\n\n\nShowing that it’s ok to replace FAMSAT + FRISAT with RS\nUsing a Likelihood Ratio Test:\n\nmodel_famsatfrisat &lt;- glm(mBanking ~ FAMSAT + FRISAT, \n                          family = \"binomial\",\n                          data = ds)\n\nmodel_rs &lt;- glm(mBanking ~ RS, \n                          family = \"binomial\",\n                          data = ds)   \n\nanova(model_rs, model_famsatfrisat, test = \"Chisq\")               \n\nSince p &gt; 0.05, there is no significant loss in model performance. It’s better to keep one variable instead of two.\n\n\n\nSome Visualizations\n\nggplot(data  = ds, aes(x = MH, y = SD))+ geom_point(size = 1.2, alpha = .5, position = \"jitter\") + \n    geom_smooth(method = lm,\n              se     = FALSE, \n              col    = \"red\",\n              size   = 2, \n              alpha  = .8)+ # to add regression line\n  theme_minimal()\n\n\nggplot(data    = ds,\n       aes(x   = MH,\n           y   = SD,\n           col = as.factor(mBanking)))+ #to add the colours for different classes\n  geom_point(size     = 1.2,\n             alpha    = .8,\n             position = \"jitter\")+ #to add some random noise for plotting purposes\n  theme_minimal() + scale_color_manual(name = \"MBanking\",\n                     labels = c(\"No\", \"Yes\"),\n                     values = c(\"red\", \"lightblue\"))\n\n\nggplot(data    = ds,\n       aes(x   = MH,\n           y   = SD,\n           col = as.factor(mBanking)))+ #to add the colours for different classes\n    geom_point(size     = 1.2,\n             alpha    = .8,\n             position = \"jitter\")+ \n    #scale_color_manual(name = \"MBanking\",\n                     #labels = c(\"No\", \"Yes\"),\n                     #values = c(\"red\", \"lightblue\")) + #+ ggnewscale::new_scale_color() +\n    geom_smooth(method   = lm,\n              se       = T, \n              size     = 1.5, \n              linetype = 1, \n              alpha    = .7,\n              ) + \n    scale_color_manual(name = \"MBanking\",\n                     labels = c(\"No\", \"Yes\"),\n                     values = c(\"black\", \"darkred\")) + theme_minimal()\n\n\nggplot(data    = ds,\n       aes(x   = AGE, #CHANGE THIS &lt;&lt;&lt;\n           y   = MH, #CHANGE THIS &lt;&lt;&lt;\n           col = as.factor(mBanking)))+ #to add the colours for different classes\n    geom_point(size     = 1.2,\n             alpha    = .8,\n             position = \"jitter\")+ \n    #scale_color_manual(name = \"MBanking\",\n                     #labels = c(\"No\", \"Yes\"),\n                     #values = c(\"red\", \"lightblue\")) + #+ ggnewscale::new_scale_color() +\n    geom_smooth(method   = lm,\n              se       = T, \n              size     = 1.5, \n              linetype = 1, \n              alpha    = .7,\n              ) + \n    scale_color_manual(name = \"MBanking\",\n                     labels = c(\"No\", \"Yes\"),\n                     values = c(\"black\", \"grey70\")) + theme_minimal()\n\n\nds1 &lt;- ds %&gt;% filter(mBanking == 1)\nds2 &lt;- ds %&gt;% filter(mBanking == 0)\n\ngg5 &lt;- ggplot(data = ds1, aes(x = MH, y = wtpg)) +\n            geom_col(\n            aes(fill = as.factor(FAMSAT)), stat = \"identity\", color = \"black\", position = position_dodge(0.9)) +\n            fill_palette(\"RdBu\") + \n            xlab(\"Mental Health\") +\n            ylab(\"Weights\") + labs(fill = \"FAMSAT\")\n\ngg6 &lt;- ggplot(data = ds2 %&gt;% filter(mBanking == 0), aes(x = MH, y = wtpg)) +\n            geom_col(\n            aes(fill = as.factor(FAMSAT)), stat = \"identity\", color = \"black\", position = position_dodge(0.9)) +\n            fill_palette(\"RdBu\") + \n            xlab(\"Mental Health\") +\n            ylab(\"Weights\") + labs(fill = \"FAMSAT\")\n\nggarrange(gg5, gg6, ncol = 2, labels = c(\"mBanking = 1\", \"mBanking = 0\"))\n\n\ngg7 &lt;- ggplot(data = ds1, aes(x = MH, y = wtpg)) +\n            geom_col(\n            aes(fill = as.factor(SNS)), stat = \"identity\", color = \"black\", position = position_dodge(0.9)) +\n            fill_palette(\"RdBu\") + \n            xlab(\"Mental Health\") +\n            ylab(\"Weights\") + labs(fill = \"SNS\")\n\ngg8 &lt;- ggplot(data = ds2 %&gt;% filter(mBanking == 0), aes(x = MH, y = wtpg)) +\n            geom_col(\n            aes(fill = as.factor(SNS)), stat = \"identity\", color = \"black\", position = position_dodge(0.9)) +\n            fill_palette(\"RdBu\") + \n            xlab(\"Mental Health\") +\n            ylab(\"Weights\") + labs(fill = \"SNS\")\n\nggarrange(gg7, gg8, ncol = 2, labels = c(\"mBanking = 1\", \"mBanking = 0\"))\n\n\ngg9 &lt;- ggplot(data = ds1, aes(x = MH, y = wtpg)) +\n            geom_col(\n            aes(fill = as.factor(SD)), stat = \"identity\", color = \"black\", position = position_dodge(0.9)) +\n            fill_palette(\"RdBu\") + \n            xlab(\"Mental Health\") +\n            ylab(\"Weights\") + labs(fill = \"SD\")\n\ngg10 &lt;- ggplot(data = ds2 %&gt;% filter(mBanking == 0), aes(x = MH, y = wtpg)) +\n            geom_col(\n            aes(fill = as.factor(SD)), stat = \"identity\", color = \"black\", position = position_dodge(0.9)) +\n            fill_palette(\"RdBu\") + \n            xlab(\"Mental Health\") +\n            ylab(\"Weights\") + labs(fill = \"SD\")\n\nggarrange(gg9, gg10, ncol = 2, labels = c(\"mBanking = 1\", \"mBanking = 0\"))\n\n\n\nModeling\nPreparing the data for modeling:\n\nds &lt;- ds %&gt;% mutate(\n    MH_c = MH - mean(MH), \n    SD_c = SD - mean(SD),\n    SNS_f = as.factor(SNS),\n    RS_c = RS - mean(RS),\n    AGE_c = AGE - mean(AGE),\n    SEX_f = as.factor(SEX),\n    EMP_f = as.factor(EMP),\n    EDU_c = EDU - mean(EDU),\n    FAM_f = as.factor(FAM),\n    INC_c = INC - mean(INC),\n    IMM_f = as.factor(IMM),\n    PRVNC = as.factor(province)\n)\n\nds &lt;- ds %&gt;% \n    mutate(\n        # SEX \n        SEX_factor_Fem = relevel(SEX_f, ref = '2'),\n        SEX_factor_Mal = relevel(SEX_f, ref = '1'),\n        # EMP\n        EMP_factor_not = relevel(EMP_f, ref = '0'),\n        EMP_factor_Emp = relevel(EMP_f, ref = '1'),\n        # FAM \n        FAM_factor_1 = relevel(FAM_f, ref = '1'),\n        FAM_factor_2 = relevel(FAM_f, ref = '2'),\n        FAM_factor_3 = relevel(FAM_f, ref = '3'),\n        FAM_factor_4 = relevel(FAM_f, ref = '4'),\n        # IMM\n        IMM_factor_Imm = relevel(IMM_f, ref = '1'),\n        IMM_factor_non = relevel(IMM_f, ref = '0'),\n        # SNS \n        SNS_factor_notuse = relevel(SNS_f, ref = '0'),\n        SNS_factor_use = relevel(SNS_f, ref = '1')\n    )\n\nSo, I believe there may be some variation due to the sampling method (clusters on provinces).\n\nds &lt;- ds %&gt;% \n    mutate(\n        province_f_coded = fct_recode(\n            PRVNC,\n            'NL' = '10',\n            'NS' = '12', \n            'NB' = '13',\n            'QC' = '24',\n            'ON' = '35', \n            'MB' = '46', \n            'SK' = '47', \n            'AB' = '48', \n            'BC' = '59'\n        )\n    )\n\n\nggplot(ds, aes(province_f_coded, mBanking, color = as.factor(MH))) +\n                  stat_summary(fun = mean, geom = \"point\") +\n                  stat_summary(fun.data = mean_cl_boot, geom = \"errorbar\", width = 0.4) +\n                  theme_set(theme_bw(base_size = 10)) +\n                  theme(legend.position = \"top\") +\n                  labs(x = \"Province\", y = \"Observed Probabilty of mobile banking\", color = \"MH\") + theme_minimal()\n\n\nprov_ &lt;- c('NL', 'NS', 'NB', 'QC', 'ON', 'MB', 'SK', 'AB', 'BC')\nprov_n &lt;- c(10, 12, 13, 24, 35, 46, 47, 48, 59)\n\n# random effects are from model 2 \nranefs_ &lt;- c(-0.029445800, 0.001523515,-0.034782017, 0.308732844,-0.132427568,-0.129533645,-0.047807245,-0.001728474,0.053000416)\nranefs_ &lt;- round(ranefs_, 4)\n\nd_graph &lt;- cbind(prov_, prov_n, ranefs_)\nd_graph &lt;- as.data.frame(d_graph)\n\nprovs_fullnames &lt;- c('Newfoundland and Labrador', 'Nova Scotia', 'New Brunswick','Quebec', 'Ontario', 'Manitoba', 'Saskatchewan', 'Alberta', 'British Columbia')\n\n\nggplot(data = d_graph, aes(x = prov_n, y = ranefs_, label = c('NL', 'NS', 'NB', 'QC', 'ON', 'MB', 'SK', 'AB', 'BC'))) +   \n    geom_point(size = 2, alpha = .5) + \n    geom_text(check_overlap = TRUE) + \n    labs(\n        x = \"Province Code\",\n        y = \"Random Effect\",\n        fill = \"Province\"\n    ) + geom_label(aes(fill = provs_fullnames), colour = \"white\", fontface = \"bold\") + geom_line(linetype = \"dashed\") + \n    scale_color_manual(values = provs_fullnames, name = \"province\")\n\nFollowing the paper, I have these models:\n\nModel 1. Standard Logistic Regression with RS \n\\begin{equation*}\n\\begin{split}\n& \\ln\\frac{P(Y = 1)}{1 - P(Y = 1)} = \\ \\beta_0 + \\beta_{1} \\ MH + \\ \\beta_2 \\ SD + \\ \\beta_3 \\ SNS + \\ \\beta_4 \\ AGE \\ + \\ \\beta_5 \\ SEX \\\\\n&  + \\ \\beta_6 \\ EMP \\ + \\ \\beta_7 \\ EDU + \\ \\beta_8 \\ INC \\ + \\ \\beta_9 \\ D_{FAM_1} + \\ \\beta_{10} \\ D_{FAM_3}\\\\\n& + \\ \\beta_{11} \\ D_{FAM_4} \\ + \\ \\beta_{12} \\ IMM + \\ \\beta_{13} \\ RS + \\ \\epsilon \\\\\n\\end{split}\n\\end{equation*}\n\n\n\nmodel1 &lt;- glm(\n    mBanking ~ MH_c + SD_c + SNS_factor_use + RS_c + AGE_c + SEX_factor_Fem + EMP_factor_Emp + EDU_c + FAM_factor_2 + IMM_factor_non + INC_c,\n    data = ds,\n    family = \"binomial\"\n)\n\n\nModel 2. Fixed Effect Logistic Regression \n\\begin{equation*}\n\\begin{split}\n& \\ln\\frac{P(Y = 1)}{1 - P(Y = 1)} = \\\\\n& \\ \\gamma_{0,j} + u_{0,j} + \\beta_{1} \\ MH + \\ \\beta_2 \\ SD + \\ \\beta_3 \\ SNS + \\ \\beta_4 \\ AGE \\ + \\ \\beta_5 \\ SEX\\\\\n&  + \\ \\beta_6 \\ EMP \\ + \\ \\beta_7 \\ EDU + \\ \\beta_8 \\ INC \\ + \\ \\beta_9 \\ D_{FAM_1} + \\ \\beta_{10} \\ D_{FAM_3}\\\\\n& + \\ \\beta_{11} \\ D_{FAM_4} \\ + \\ \\beta_{12} \\ IMM + \\ \\beta_{13} \\ RS + \\ \\epsilon \\\\\n\\end{split}\n\\end{equation*}\n\n\n\nmodel2 &lt;- glmer(\n    mBanking ~ MH_c + SD_c + SNS_factor_use + RS_c + AGE_c + SEX_factor_Fem + EMP_factor_Emp + EDU_c + FAM_factor_2 + IMM_factor_non + INC_c + (1 | province),\n    data = ds,\n    family = binomial(),\n    control = glmerControl(optimizer = \"bobyqa\")\n)\n\n\nModel 3. Random Effect Logistic Regression\n\n\n\\begin{equation*}\n\\begin{split}\n   & \\ln\\frac{P(Y = 1)}{1 - P(Y = 1)} = \\\\\n   & \\ \\gamma_{0,0} + u_{0,j} + (\\gamma_{1,0} + u_{1,j}) \\ MH + \\ (\\gamma_{2,0} + u_{2,j}) \\ SD \\ + \\ (\\gamma_{3,0} + u_{3,j}) \\ SNS \\\\\n   & + \\ (\\gamma_{4,0} + u_{4,j}) \\ AGE \\ + \\ (\\gamma_{5,0} + u_{5,j}) \\ SEX \\ + \\ (\\gamma_{6,0} + u_{6,j}) EMP \\\\\n   & + \\ (\\gamma_{7,0} + u_{7,j}) \\ EDU \\ + \\ (\\gamma_{8,0} + u_{8,j}) \\ INC \\\n    + \\ (\\gamma_{9,0} + u_{9,j}) \\ D_{FAM_1} \\\\\n   & + \\ (\\gamma_{10,0} + u_{10,j}) \\ D_{FAM_3} \\ + \\ (\\gamma_{11,0} + u_{11,j}) \\ D_{FAM_4} \\\\\n   & + \\ (\\gamma_{12,0} + u_{12,j}) \\ IMM_1 \\ + \\ (\\gamma_{13,0} + u_{13,j}) \\ RS + \\ \\epsilon \\\\\n\\end{split}\n\\end{equation*}\n\n\nmodel3 &lt;- glmer(\n    mBanking ~ MH_c + SD_c + SNS_factor_use + RS_c + AGE_c + SEX_factor_Fem + EMP_factor_Emp + EDU_c + FAM_factor_2 + IMM_factor_non + INC_c + \n    (1 + MH_c + SD_c + SNS + RS_c + AGE_c + SEX_f + EMP + EDU_c + FAM_2 + IMM_n + \n    INC_c | province),\n    data = ds,\n    family = binomial(link = \"logit\"),\n    control = glmerControl(optimizer = \"bobyqa\"))\n\nFor faster speeds, I’ll test everything with this model (random):\n\nmodel4 &lt;- glmer(\n    mBanking ~ MH_c + SD_c + SNS_factor_use + RS_c + AGE_c + SEX_factor_Fem + EMP_factor_Emp + EDU_c + FAM_factor_2 + IMM_factor_non + INC_c + \n    (1 + MH_c + SD_c + SNS_factor_use + RS_c | province),\n    data = ds,\n    family = binomial(link = \"logit\"),\n    control = glmerControl(optimizer = \"bobyqa\"))\n\nComparing models:\n\nLikelihood Ratio Test model1 vs model2\nIf model2 wins, we need the Hausman test to see if model3 is better\nIf model1 wins, we should just use that\n\n\ntest_performance(model1, model2)\n\nHausman test for fixed effects in R is a bit tricky, so, I did it manually following the formula:\n\nExtract per-group (province) coefficients for both models and convert them to data frames\n\n\ncoefs_fixed &lt;- coef(model2)\ncoefs_rando &lt;- coef(model4)\n\ncoefs_fixed_df &lt;- as.data.frame(coefs_fixed$province)\ncoefs_rando_df &lt;- as.data.frame(coefs_rando$province)\n\n\nSubtract Random Effects model coefficients from Fixed Effect model, per province. This gives you the core term of the Hausman test:\n\n\ncoefs_diff &lt;- coefs_fixed_df - coefs_rando_df\n\ncoef_diffs_matrix &lt;- as.matrix(coefs_diff)\n\n\nEstimate the difference in the variance-covariance matrices of the coefficient estimates.\n\n\nV_diff &lt;- as.matrix(vcov(model2) - vcov(model4))\n\nWhat I need is:\n\nH = (\\hat{\\beta_{F}} - \\hat{\\beta_{R}}) \\cdot V^{-1} \\cdot (\\hat{\\beta_{F}} - \\hat{\\beta_{R}})^T\n\nJust checking that the matrix multiplications make sense:\n\ndim(t(coef_diffs_matrix))\ndim(solve(V_diff))\ndim(coef_diffs_matrix)\n\nThey do! So, calculate H:\n\nH &lt;- coef_diffs_matrix %*% solve(V_diff) %*% t(coef_diffs_matrix)\n\nThis is the actual critical \\chi^2 value at degrees of freedom 13 (for 14 covariates), in fact, I can check:\n\nqr(V_diff)$rank\n\n\nchisq_critical &lt;- qchisq(p = .05, df = 13, lower.tail = FALSE)\nchisq_critical\n\nIf H1 &gt; \\chi^2 then reject the null hypothesis that says the fixed model is better.\n\nH &gt; chisq_critical #reject H0: the fixed model is better.  \n\nThe p-value:\n\npchisq(H, df = 13, lower.tail = FALSE)\n\nOk, we can’t reject this hypothesis - therefore, the fixed model is better. Another way to check:\n\nanova(model2, model4)\n\nOk, best model is model2. Now adding interaction terms:\n\nmodel2_int &lt;- glmer(\n    mBanking ~ MH_c + SD_c + SNS_factor_use + RS_c + AGE_c + SEX_factor_Fem \n    + EMP_factor_Emp + EDU_c + FAM_factor_2 + IMM_factor_non + INC_c \n    + MH_c:RS_c + MH_c:SD_c + MH_c:SNS_factor_use \n    + (1 | province),\n    data = ds,\n    family = binomial(),\n    control = glmerControl(optimizer = \"bobyqa\"))\n\nPrinting both odds ratios and log-odds versions:\n\nsumm(\n    model2,\n    scale = F,\n    pvals = T,\n    exp = T, \n    digits = 3,\n    #part.corr = T, #Print partial (labeled \"partial.r\") and semipartial (labeled \"part.r\")\n    #confint = getOption(\"summ-confint\", FALSE),\n    #ci.width = getOption(\"summ-ci.width\", 0.95),\n    #vifs = T\n)\n\n\nsummary(model2)\n\n\nsumm(\n    model2_int,\n    scale = F,\n    pvals = T,\n    exp = T, \n    digits = 3,\n    #part.corr = T, #Print partial (labeled \"partial.r\") and semipartial (labeled \"part.r\")\n    #confint = getOption(\"summ-confint\", FALSE),\n    #ci.width = getOption(\"summ-ci.width\", 0.95),\n    #vifs = T\n)\n\n\nsummary(model2_int)\n\nAlso calculating the confidence interval for the variances of each model:\n\nround(confint(model2),3)\n\n\nAnd the random effects for provinces (for visualization):\n\nranef(model2)\n\n\n\nMarginal Effects\nFirst, let’s see which model is better:\n\nanova(model2, model2_int)\n\n\ntest_performance(model2, model2_int)\n\nMargins at different provinces:\n\nmargins_summary(model2)",
    "crumbs": [
      "Home",
      "Projects",
      "Project 2. Mental Health",
      "Effect Of Mental Health on Mobile Banking"
    ]
  },
  {
    "objectID": "study2.html#whats-the-connection",
    "href": "study2.html#whats-the-connection",
    "title": "Mental Health and Mobile Banking",
    "section": "",
    "text": "In this chapter, I look into how mental health might affect whether people choose to adopt mobile banking or not. While doing my review of past studies (refer to study 1 ), I noticed that a lot of research talks about how things like trust, risk, and perceived ease of use affect adoption. But very few studies look at how a person’s mental and emotional state influences their behavior. Especially because some studies actively showed how not accounting for people’s emotions were sometiems giving them strange results.\nI also found that even if the literature did talk about emotions, it was mostly about emotions as a result of the adoption/use of a technology. Not about how your general mood or feelings or struggles impact how you behave and interact with technology. Like I know when I’m stressed out or dreading something, the doom scrolling gets worse! But social media apps are built to be addictive and drag you in and make you want it more. Until you actually stop enjoying it and being online is no longer fun. In fact, I read just recently how engagement and use of social media apps has dropped in recent years! I’ll find the citation for it later! Anyway, I digress… Point is, I think your mood definitely affects how you interact with the world and technology is part of that world. So, what about people who have special moods? That is, certain behavioral or mental challenges? Not exactly disabilities, but hidden challenges often ignored!\nI wanted to explore whether people who are feeling mentally well are more or less likely to use mobile banking. My guess going in was that people with better mental health might feel more confident and more open to using this tech. But I found out it was actually the opposite! Then I got to thinking… why is that? Well, Here’s my guess:\n\nMaybe people with poorer mental health actually rely on mobile banking to avoid going out and talking to people, or dealing with stress in traditional banking.\nMaybe people who are happy don’t bank and don’t stress about money!\nIt’s not a causation, and I don’t know which comes first - are people who bank less happier or is it that happy people don’t bank as often? This is not the study to answer that. But that would be a cool study!\n\nI used Canadian survey data and looked at three things related to mental health:\n\nHow satisfied are you in your relationships (RS)\nHow dependent are you on your smartphones (SD)\nWhether or not you use social media (SNS)\n\nI also tested whether these three things change (or “moderate”) how mental health affects mobile banking adoption. This chapter is where I introduce mental health as a new factor in mobile banking adoption models. I show how mental and emotional states might be just as important as the usual stuff like trust or app features. The results surprised me, and hopefully, they’ll give banks and researchers something new to think about too.\nBy the way, this chapter was already published in .",
    "crumbs": [
      "Home",
      "Projects",
      "Project 2. Mental Health"
    ]
  },
  {
    "objectID": "study2_DA.html#mh_c-predicted-95-ci",
    "href": "study2_DA.html#mh_c-predicted-95-ci",
    "title": "Data Analysis - CIUS 2020",
    "section": "MH_c | Predicted | 95% CI",
    "text": "MH_c | Predicted | 95% CI\n-2.59 | 0.88 | 0.86, 0.90 -1.59 | 0.87 | 0.86, 0.89 -0.59 | 0.86 | 0.85, 0.88 0.41 | 0.85 | 0.84, 0.87 1.41 | 0.84 | 0.83, 0.86\nprovince: 12",
    "crumbs": [
      "Home",
      "Projects",
      "Project 2. Mental Health",
      "Data Analysis - CIUS 2020"
    ]
  },
  {
    "objectID": "study2_DA.html#mh_c-predicted-95-ci-1",
    "href": "study2_DA.html#mh_c-predicted-95-ci-1",
    "title": "Data Analysis - CIUS 2020",
    "section": "MH_c | Predicted | 95% CI",
    "text": "MH_c | Predicted | 95% CI\n-2.59 | 0.89 | 0.87, 0.90 -1.59 | 0.88 | 0.86, 0.89 -0.59 | 0.87 | 0.86, 0.88 0.41 | 0.86 | 0.85, 0.87 1.41 | 0.85 | 0.83, 0.87\nprovince: 13",
    "crumbs": [
      "Home",
      "Projects",
      "Project 2. Mental Health",
      "Data Analysis - CIUS 2020"
    ]
  },
  {
    "objectID": "study2_DA.html#mh_c-predicted-95-ci-2",
    "href": "study2_DA.html#mh_c-predicted-95-ci-2",
    "title": "Data Analysis - CIUS 2020",
    "section": "MH_c | Predicted | 95% CI",
    "text": "MH_c | Predicted | 95% CI\n-2.59 | 0.88 | 0.86, 0.90 -1.59 | 0.87 | 0.85, 0.89 -0.59 | 0.86 | 0.85, 0.88 0.41 | 0.85 | 0.84, 0.87 1.41 | 0.84 | 0.82, 0.86\nprovince: 24",
    "crumbs": [
      "Home",
      "Projects",
      "Project 2. Mental Health",
      "Data Analysis - CIUS 2020"
    ]
  },
  {
    "objectID": "study2_DA.html#mh_c-predicted-95-ci-3",
    "href": "study2_DA.html#mh_c-predicted-95-ci-3",
    "title": "Data Analysis - CIUS 2020",
    "section": "MH_c | Predicted | 95% CI",
    "text": "MH_c | Predicted | 95% CI\n-2.59 | 0.91 | 0.90, 0.93 -1.59 | 0.90 | 0.89, 0.92 -0.59 | 0.90 | 0.89, 0.91 0.41 | 0.89 | 0.88, 0.90 1.41 | 0.88 | 0.87, 0.90\nprovince: 35",
    "crumbs": [
      "Home",
      "Projects",
      "Project 2. Mental Health",
      "Data Analysis - CIUS 2020"
    ]
  },
  {
    "objectID": "study2_DA.html#mh_c-predicted-95-ci-4",
    "href": "study2_DA.html#mh_c-predicted-95-ci-4",
    "title": "Data Analysis - CIUS 2020",
    "section": "MH_c | Predicted | 95% CI",
    "text": "MH_c | Predicted | 95% CI\n-2.59 | 0.87 | 0.85, 0.89 -1.59 | 0.86 | 0.84, 0.88 -0.59 | 0.85 | 0.84, 0.87 0.41 | 0.84 | 0.83, 0.86 1.41 | 0.83 | 0.81, 0.85\nprovince: 46",
    "crumbs": [
      "Home",
      "Projects",
      "Project 2. Mental Health",
      "Data Analysis - CIUS 2020"
    ]
  },
  {
    "objectID": "study2_DA.html#mh_c-predicted-95-ci-5",
    "href": "study2_DA.html#mh_c-predicted-95-ci-5",
    "title": "Data Analysis - CIUS 2020",
    "section": "MH_c | Predicted | 95% CI",
    "text": "MH_c | Predicted | 95% CI\n-2.59 | 0.87 | 0.85, 0.89 -1.59 | 0.86 | 0.85, 0.88 -0.59 | 0.85 | 0.84, 0.87 0.41 | 0.84 | 0.83, 0.86 1.41 | 0.83 | 0.82, 0.85\nprovince: 47",
    "crumbs": [
      "Home",
      "Projects",
      "Project 2. Mental Health",
      "Data Analysis - CIUS 2020"
    ]
  },
  {
    "objectID": "study2_DA.html#mh_c-predicted-95-ci-6",
    "href": "study2_DA.html#mh_c-predicted-95-ci-6",
    "title": "Data Analysis - CIUS 2020",
    "section": "MH_c | Predicted | 95% CI",
    "text": "MH_c | Predicted | 95% CI\n-2.59 | 0.88 | 0.86, 0.90 -1.59 | 0.87 | 0.85, 0.89 -0.59 | 0.86 | 0.85, 0.88 0.41 | 0.85 | 0.84, 0.87 1.41 | 0.84 | 0.82, 0.86\nprovince: 48",
    "crumbs": [
      "Home",
      "Projects",
      "Project 2. Mental Health",
      "Data Analysis - CIUS 2020"
    ]
  },
  {
    "objectID": "study2_DA.html#mh_c-predicted-95-ci-7",
    "href": "study2_DA.html#mh_c-predicted-95-ci-7",
    "title": "Data Analysis - CIUS 2020",
    "section": "MH_c | Predicted | 95% CI",
    "text": "MH_c | Predicted | 95% CI\n-2.59 | 0.88 | 0.86, 0.90 -1.59 | 0.88 | 0.86, 0.89 -0.59 | 0.87 | 0.85, 0.88 0.41 | 0.86 | 0.84, 0.87 1.41 | 0.85 | 0.83, 0.87\nprovince: 59",
    "crumbs": [
      "Home",
      "Projects",
      "Project 2. Mental Health",
      "Data Analysis - CIUS 2020"
    ]
  },
  {
    "objectID": "study2_DA.html#mh_c-predicted-95-ci-8",
    "href": "study2_DA.html#mh_c-predicted-95-ci-8",
    "title": "Data Analysis - CIUS 2020",
    "section": "MH_c | Predicted | 95% CI",
    "text": "MH_c | Predicted | 95% CI\n-2.59 | 0.89 | 0.87, 0.91 -1.59 | 0.88 | 0.87, 0.90 -0.59 | 0.87 | 0.86, 0.89 0.41 | 0.86 | 0.85, 0.88 1.41 | 0.85 | 0.84, 0.87\nMargins at different provinces:\n\nhead(margins_summary(model2, \n     at = list(province = c(10, 12, 13, 24, 35, 46, 47, 48, 59))))\n\n factor province    AME     SE      z      p   lower  upper\n  AGE_c  10.0000 0.0029 0.0027 1.0837 0.2785 -0.0023 0.0081\n  AGE_c  12.0000 0.0028 0.0026 1.0837 0.2785 -0.0023 0.0079\n  AGE_c  13.0000 0.0029 0.0027 1.0837 0.2785 -0.0023 0.0081\n  AGE_c  24.0000 0.0023 0.0021 1.0835 0.2786 -0.0019 0.0065\n  AGE_c  35.0000 0.0031 0.0028 1.0838 0.2784 -0.0025 0.0086\n  AGE_c  46.0000 0.0031 0.0028 1.0838 0.2784 -0.0025 0.0086\n\n\nSummary Table of Margins:\n\nmargins_summary(model2)\n\n          factor     AME     SE        z      p   lower   upper\n           AGE_c  0.0028 0.0026   1.0838 0.2785 -0.0022  0.0078\n           EDU_c  0.0490 0.0044  11.1029 0.0000  0.0403  0.0576\n EMP_factor_Emp0 -0.0663 0.0076  -8.6816 0.0000 -0.0813 -0.0513\n   FAM_factor_21 -0.0269 0.0091  -2.9500 0.0032 -0.0447 -0.0090\n   FAM_factor_23 -0.0058 0.0083  -0.6964 0.4862 -0.0220  0.0105\n   FAM_factor_24  0.0007 0.0192   0.0341 0.9728 -0.0370  0.0383\n IMM_factor_non1 -0.0241 0.0113  -2.1415 0.0322 -0.0462 -0.0020\n           INC_c  0.0053 0.0026   2.0105 0.0444  0.0001  0.0104\n            MH_c -0.0090 0.0033  -2.7429 0.0061 -0.0155 -0.0026\n            RS_c  0.0021 0.0020   1.0827 0.2790 -0.0017  0.0059\n            SD_c  0.0193 0.0030   6.5080 0.0000  0.0135  0.0252\n SEX_factor_Fem1 -0.0123 0.0064  -1.9175 0.0552 -0.0250  0.0003\n SNS_factor_use0 -0.1254 0.0100 -12.5093 0.0000 -0.1450 -0.1057",
    "crumbs": [
      "Home",
      "Projects",
      "Project 2. Mental Health",
      "Data Analysis - CIUS 2020"
    ]
  },
  {
    "objectID": "study2_DA.html#data-analysis---cius-2020",
    "href": "study2_DA.html#data-analysis---cius-2020",
    "title": "Effect Of Mental Health on Mobile Banking",
    "section": "",
    "text": "In this project, I focused on analyzing how mental health relates to mobile banking adoption. I used data from the Canadian Internet Use Survey, which includes questions about digital habits, mental health, and demographics. You can find the dataset . I built a fixed-effects logistic regression model, grouped by province, to control for regional differences as this was the sampling cluster. My main variable was self-reported mental health scores. I included factors like relationship satisfaction, smartphone dependency, and social media use. I also tested interaction effects to see if these variables change the way mental health influences mobile banking use. The following is a step-by-step on the coding and analysis of the project.\n\n\nNote that not all libraries may be utilized. The most important ones are dplyr, lme4, tidyr, ggplot2, psych, corrr, haven, marginaleffects and margins and any related libraries to these.\n\n\n\nI first started by reading the entire PUMF file available.\nThis gives you information on how the survey was set up, why, and how things were measured. Then, I looked at the individual survey questions to see the available data, and how they were measured. In general, questions are measured numerically were answeres follow as such: &gt; Yes : 1, No : 2, Valid Skip: 6, Don’t Know: 7, Refusal: 8, Not Stated: 9 Of course this differs question-by-question as some questions have other answer categories and some questions (which were note used in my study) asked for numerical input from the participants (like how much did you spend online last year). To help readers understand the data, I will include the question exactly as it appears in the CIUS 2020 PUMF Data Dictionary with corresponding answer choices and codes. These will be in a blue-bordered box, and will include the Variable name (on the PUMF file), Concept, Question Body and Answers. Then I will show you in R code how I’ve re-coded and used the question as a model variable. The Variables I need are as follows:\n\nMobile banking adoption (mBanking)\nAge Group (AGE)\nSmartphone Dependent (SD)\nIncome Quintile (INC)\nFriendship Satisfaction (FRISAT)\nMental Health (MH)\nFamily Relation Satisfaction (FAMSAT)\nEducation Level (EDU)\nImmigration Status (IMM)\nEmployment Status (EMP)\nFamily Type (FAM)\nGender (SEX)\nSocial Media Use (SNS)\nProvince (province)\n\nThe data is available in various formats. To avoid data loss, I decided to use the .dta format (SAS file). You need the haven package to read SAS files. However, this file is 150MB in size and since I am uploading the code in my GitHub repository, I am not able to use the SAS file. So I’ve saved the data in a .csv file with only the columns I will need. This is how you’d read a SAS file:\n\nds20 &lt;- read_dta(\"data/cius2020_2022nov18_en.dta\")\nds &lt;- ds20\n\nInstead, I run this:\n\nds10 &lt;- read.csv(\"ds.csv\")\nds &lt;- ds10 \n\nStep 1. Smartphone Users Select only those who use smartphones because the question is about “online banking” and not “mobile banking”.\n\n\n\nVariable Name: DV_010A\n\nConcept: Devices used\n\n\nQuestion Text:\nDuring the past three months, what devices did you use to access the Internet?\nDid you use:\nA smartphone\n\n\n\nAnswer Categories\nCode\n\n\n\n\nYes\n1\n\n\nNo\n2\n\n\nValid Skip\n6\n\n\nDon’t Know\n7\n\n\nRefusal\n8\n\n\nNot Stated\n9\n\n\n\n\n\nds &lt;- ds %&gt;% \n    mutate(\n        devSM = case_when(\n            dv_010a == 1 ~ 1, #yes\n            dv_010a == 2 ~ 0, #no\n        .default = -1, #any valid skip and not stated \n        )\n    )\n\nds &lt;- ds %&gt;% \n   filter(devSM == 1)\n\nStep 2. Mobile Banking Question Select the outcome variable, mobile banking:\n\n\n\nVariable Name: UI_050D\n\nConcept: Activities related to other online activities\n\n\nQuestion Text:\nDuring the past three months, which of the following other online activities, have you done over the Internet? Have you: Conducted online banking\n\n\n\nAnswer Categories\nCode\n\n\n\n\nYes\n1\n\n\nNo\n2\n\n\nValid Skip\n6\n\n\nDon’t Know\n7\n\n\nRefusal\n8\n\n\nNot Stated\n9\n\n\n\n\nStep 3. Select Other Model Variables Now I will move on to selecting the predictors. For Smartphone Dependency:\n\n\n\nVariable Name: SM_030A\n\nConcept: Frequency of use of smartphone\n\n\nQuestion Text:\nIn a typical day, how often do you check your smartphone?\n\n\n\nAnswer Categories\nCode\n\n\n\n\nAt least every 5 minutes\n01\n\n\nAt least every 15 minutes\n02\n\n\nAt least every 30 minutes\n03\n\n\nOne time per hour\n04\n\n\nOnce a day or a few times per day\n05\n\n\nLess than one time per day\n06\n\n\nValid skip\n96\n\n\nDon’t know\n97\n\n\nRefusal\n98\n\n\nNot stated\n99\n\n\n\n\nAs you can see, this one is no longer just a yes/no question, but has a few categories for answers. Since I’m thinking of tracking “dependence”, it makes sense that the more frequent checking gets a higher value. So, this is how I code this variable:\n\nds &lt;- ds %&gt;%\n    mutate(\n        #timeline : past 3 months \n        mBanking = case_when(\n            ui_050d == 1 ~ 1, # Yes \n            ui_050d == 2 ~ 0, # No \n            .default = -1 # valid skip, don't know, refused, not stated \n        ),\n        \n        SD = case_when(\n            sm_030a == 1 ~ 6, # At least every 5 minutes \n            sm_030a == 2 ~ 5, # At least every 15 minutes \n            sm_030a == 3 ~ 4, # At least every 30 minutes  \n            sm_030a == 4 ~ 3, # One time per hour  \n            sm_030a == 5 ~ 2, # Once a day or a few times per day  \n            sm_030a == 6 ~ 1, # Less than one time per day\n            .default = 96 # Valid skip 96 , Don’t know 97 , Refusal  98, Not stated  99\n        )\n    )\n\nds &lt;- ds %&gt;% filter(SD &lt; 10)\nds &lt;- ds %&gt;% filter(mBanking != -1) \n\nThe filters are just making sure that the skip’s, don’t know’s, refusal’s and not stated answers are dropped. This is not too big of a loss. There are 17,409 rows of data.\nFriendship Satisfaction:\n\n\n\nVariable Name: TS_010A\n\nConcept: Satisfaction with relationships\n\n\nQuestion Text:\nIn general, on a scale from 1 to 5 where 1 means «completely dissatisfied» and 5 means «completely satisfied», how satisfied are you with the relationships you have with the following people? Friends\n\n\n\nAnswer Categories\nCode\n\n\n\n\n1-Completely dissatisfied\n1\n\n\n2\n2\n\n\n3\n3\n\n\n4\n4\n\n\n5- Completely satisfied\n5\n\n\nValid skip\n6\n\n\nDon’t know\n7\n\n\nRefusal\n8\n\n\nNot stated\n9\n\n\n\n\nFamily Satisfaction:\n\n\n\nVariable Name: TS_010B\n\nConcept: Satisfaction with relationships\n\n\nQuestion Text:\nIn general, on a scale from 1 to 5 where 1 means «completely dissatisfied» and 5 means «completely satisfied», how satisfied are you with the relationships you have with the following people? Relatives or family members, excluding those you live with\n\n\n\nAnswer Categories\nCode\n\n\n\n\n1-Completely dissatisfied\n1\n\n\n2\n2\n\n\n3\n3\n\n\n4\n4\n\n\n5- Completely satisfied\n5\n\n\nValid skip\n6\n\n\nDon’t know\n7\n\n\nRefusal\n8\n\n\nNot stated\n9\n\n\n\n\nMental Health:\n\n\n\nVariable Name: FD_030A\n\nConcept: Perceived mental health\n\n\nQuestion Text:\nIn general, how is your mental health? Would you say:\n\n\n\nAnswer Categories\nCode\n\n\n\n\nExcellent\n1\n\n\nVery good\n2\n\n\nGood\n3\n\n\nFair\n4\n\n\nPoor\n5\n\n\nValid skip\n6\n\n\nDon’t know\n7\n\n\nRefusal\n8\n\n\nNot stated\n9\n\n\n\n\nAgain, since I want to measure mental health in terms of how good or bad a person is, it makes sense that better moods are associated with elevated numbers (so instead of it being coded as 1, Excellent should be coded as 5).\n\nds &lt;- ds %&gt;% mutate(\n    FRISAT = case_when(\n        ts_010a == 1 ~ 1, #completely dissatisfied \n        ts_010a == 2 ~ 2, \n        ts_010a == 3 ~ 3,\n        ts_010a == 4 ~ 4,\n        ts_010a == 5 ~ 5, #completely satisfied \n        .default = 6\n    ),\n    \n    FAMSAT = case_when(\n        ts_010b == 1 ~ 1, #completely dissatisfied \n        ts_010b == 2 ~ 2, \n        ts_010b == 3 ~ 3,\n        ts_010b == 4 ~ 4,\n        ts_010b == 5 ~ 5, #completely satisfied \n        .default = 6\n    ),\n    \n    MH = case_when(\n        fd_030a == 1 ~ 5, #excellent \n        fd_030a == 2 ~ 4, #very good \n        fd_030a == 3 ~ 3, #good \n        fd_030a == 4 ~ 2, #fair\n        fd_030a == 5 ~ 1, #poor\n        .default = 6\n    )\n)\n\nds &lt;- ds %&gt;% filter(\n    FRISAT &lt; 6,\n    FAMSAT &lt; 6,\n    MH &lt; 6\n)\n\nSocial Media use:\n\n\n\nVariable Name: UI_010C\n\nConcept: Activities related to communication\n\n\nQuestion Text:\nDuring the past three months, which of the following activities, related to communication, have you done over the Internet? Have you: Used social networking websites or apps\n\n\n\nAnswer Categories\nCode\n\n\n\n\nYes\n1\n\n\nNo\n2\n\n\nValid skip\n6\n\n\nDon’t know\n7\n\n\nRefusal\n8\n\n\nNot stated\n9\n\n\n\n\n\nds &lt;- ds %&gt;% mutate(\n    SNS = case_when(\n        ui_010c == 1 ~ 1, # yes \n        ui_010c == 2 ~ 0, # no \n        .default = 3\n    )\n)\n\nds &lt;- ds %&gt;% filter(\n    SNS &lt; 3\n)\n\nStep 4. Gather Demographic and Other Information Here, I will select some demographic information. Since these are not direct questions but information retrieved from other sources (such as postal codes for province), some of them do not have Skips/Don’t Know/Refusal/Not Stated answers. If they have, I have added those to the cards. I have also added a frequency column for these variables only to show you the demographic distribution of the sample (not weighted).\n\n\n\n\n\nVariable Name: PROVINCE\n\nConcept: PROVINCE\n\n\nNote:\nInformation derived using postal codes.\n\n\n\nAnswer Categories\nCode\nFrequency\n\n\n\n\nNewfoundland and Labrador\n10\n950\n\n\nPrince Edward Island\n11\n1,154\n\n\nNova Scotia\n12\n1,181\n\n\nNew Brunswick\n13\n1,223\n\n\nQuebec\n24\n3,911\n\n\nOntario\n35\n3,719\n\n\nManitoba\n46\n973\n\n\nSaskatchewan\n47\n979\n\n\nAlberta\n48\n1,365\n\n\nBritish Columbia\n59\n1,954\n\n\n\n\n\n\n\n\n\n\n\nVariable Name: AGE_GRP\n\nConcept: Age Groups – Derived variable\n\n\nNote:\nDerived from age of persons in the household\n\n\n\nAnswer Categories\nCode\nFrequency\n\n\n\n\n15 to 24 years\n01\n828\n\n\n25 to 34 years\n02\n1,805\n\n\n35 to 44 years\n03\n2,520\n\n\n45 to 54 years\n04\n2,469\n\n\n55 to 64 years\n05\n3,889\n\n\n65 years and over\n06\n5,898\n\n\n\n\n\n\n\n\n\n\n\nVariable Name: GENDER\n\nConcept: Gender - Derived variable\n\n\nNote:\nRefers to current gender which may be different from sex assigned at birth and may be different from what is indicated on legal documents. For data quality and confidentiality reasons, and because of the small population being measured, the dissemination of data according to ’Non binary’ Gender is not possible for this statistical program. So, this release uses a gender variable with only two categories. This variable is derived by looking at a large number of demographic characteristics from the respondent, it allows us to disseminate data on Gender that is reliable and unbiased.\n\n\n\nAnswer Categories\nCode\nFrequency\n\n\n\n\nMale\n1\n8,126\n\n\nFemale\n2\n9,283\n\n\n\n\n\n\n\n\n\n\n\nVariable Name: EMP\n\nConcept: Employment status - Derived variable\n\n\nNote:\n\n\n\nAnswer Categories\nCode\nFrequency\n\n\n\n\nEmployed\n1\n8,451\n\n\nNot Employed\n2\n8,177\n\n\nValid Skip\n6\n0\n\n\nDon’t Know\n7\n0\n\n\nRefusal\n8\n0\n\n\nNot Stated\n9\n781\n\n\n\n\n\n\n\n\n\n\n\nVariable Name: G_EDU\n\nConcept: Highest certificate - Derived variable\n\n\nNote:\n\n\n\nAnswer Categories\nCode\nFrequency\n\n\n\n\nHigh school or less\n1\n5,746\n\n\nSome post-secondary (incl. univ certificate)\n2\n5,866\n\n\nUniversity degree\n3\n4,889\n\n\nValid skip\n6\n0\n\n\nDon’t know\n7\n0\n\n\nRefusal\n8\n0\n\n\nNot stated\n9\n908\n\n\n\n\n\n\n\n\n\n\n\nVariable Name: G_HCOMP\n\nConcept: Type of household - Derived variable\n\n\nNote:\nThis derived variable indicates the household composition. It was derived using RRS_Q12 (number of persons in the household), RR_020CA (age of persons in the household) and RR_040AA (relationship of the respondent with the other members of the household).\n\n\n\nAnswer Categories\nCode\nFrequency\n\n\n\n\nFamily household with children under 18\n1\n3,700\n\n\nFamily household without children under 18\n2\n8,299\n\n\nSingle person household\n3\n4,837\n\n\nOther household type\n4\n414\n\n\nValid skip\n6\n0\n\n\nDon’t know\n7\n0\n\n\nRefusal\n8\n0\n\n\nNot stated\n9\n159\n\n\n\n\n\n\n\n\n\n\n\nVariable Name: HINCQUIN\n\nConcept: Census family income quintile - Derived variable\n\n\nNote:\nInformation derived using HINC. In order to obtain equal weighted counts in each category, cases with incomes equal to the category cutoffs were randomly assigned to one of the two categories on either side of the cutoff.\nSource: Annual Income Estimates for Census Families and Individuals (T1 Family File)\n\n\n\nAnswer Categories\nCode\nFrequency\n\n\n\n\nQuintile 1 - \\leq $44,119\n1\n4,200\n\n\nQuintile 2 - $44,120 - $75,321\n2\n3,911\n\n\nQuintile 3 - $75,322 - $109,431\n3\n3,394\n\n\nQuintile 4 - $109,432 - $162,799\n4\n3,185\n\n\nQuintile 5 - \\geq $162,800\n5\n2,719\n\n\n\n\n\n\n\n\n\n\n\nVariable Name: IMM_GSTA\n\nConcept: Immigrant status\n\n\nNote:\nLanded immigrants are permanent residents who have indicated a year of landing in Canada since 1980. Variable derived from LANDING_YEAR, IMDB.\nSource: Longitudinal Immigration Database (IMDB)\n\n\n\nAnswer Categories\nCode\nFrequency\n\n\n\n\nLanded immigrant\n1\n1,696\n\n\nNon-landed immigrant\n2\n15,701\n\n\nValid Skip\n6\n0\n\n\nDon’t Know\n7\n0\n\n\nRefusal\n8\n0\n\n\nNot Stated\n9\n12\n\n\n\n\n\n\nVariable pumfid is just a unique identifier.\n\nds &lt;- ds %&gt;% mutate(\n    id = pumfid,\n    province = province, \n    AGE = as.integer(age_grp),\n    SEX = gender,\n    EMP = ifelse(\n        emp == 2,\n        0,\n        emp\n    ),\n    EDU = g_edu,\n    FAM = g_hcomp, \n    IMM = ifelse(\n        imm_gsta == 2,\n        0,\n        imm_gsta\n    ),\n    INC = hincquin\n)\n\nds &lt;- ds %&gt;% filter(\n    SNS &lt; 3,\n    EMP &lt; 3,\n    FAM &lt; 5,\n    IMM &lt; 3\n)\n\nStep 5. Selecting The Variables From Data Set\n\nds &lt;- ds %&gt;% \n    dplyr::select(id, \n                  mBanking, SD, FAMSAT, FRISAT, MH, SNS,\n                  province, AGE, SEX, EMP, EDU,\n                  FAM, IMM, INC, wtpg)\n\nSize of the dataset:\n\ndim(ds)",
    "crumbs": [
      "Home",
      "Projects",
      "Project 2. Mental Health",
      "Effect Of Mental Health on Mobile Banking"
    ]
  },
  {
    "objectID": "study2_DA.html#data-analysis",
    "href": "study2_DA.html#data-analysis",
    "title": "Effect Of Mental Health on Mobile Banking",
    "section": "",
    "text": "In this project, I focused on analyzing how mental health relates to mobile banking adoption. I used data from the Canadian Internet Use Survey, which includes questions about digital habits, mental health, and demographics. You can find the dataset here. I built a fixed-effects logistic regression model, grouped by province, to control for regional differences as this was the sampling cluster. My main variable was self-reported mental health scores. I included factors like relationship satisfaction, smartphone dependency, and social media use. I also tested interaction effects to see if these variables change the way mental health influences mobile banking use. The following is a step-by-step on the coding and analysis of the project.\n\n\nNote that not all libraries may be utilized. The most important ones are dplyr, lme4, tidyr, ggplot2, psych, corrr, haven, marginaleffects and margins and any related libraries to these.\n\n\n\nI first started by reading the entire PUMF file available.\nThis gives you information on how the survey was set up, why, and how things were measured. Then, I looked at the individual survey questions to see the available data, and how they were measured. In general, questions are measured numerically were answeres follow as such: &gt; Yes : 1, No : 2, Valid Skip: 6, Don’t Know: 7, Refusal: 8, Not Stated: 9 Of course this differs question-by-question as some questions have other answer categories and some questions (which were note used in my study) asked for numerical input from the participants (like how much did you spend online last year). To help readers understand the data, I will include the question exactly as it appears in the CIUS 2020 PUMF Data Dictionary with corresponding answer choices and codes. These will be in a blue-bordered box, and will include the Variable name (on the PUMF file), Concept, Question Body and Answers. Then I will show you in R code how I’ve re-coded and used the question as a model variable. The Variables I need are as follows:\n\nMobile banking adoption (mBanking)\nAge Group (AGE)\nSmartphone Dependent (SD)\nIncome Quintile (INC)\nFriendship Satisfaction (FRISAT)\nMental Health (MH)\nFamily Relation Satisfaction (FAMSAT)\nEducation Level (EDU)\nImmigration Status (IMM)\nEmployment Status (EMP)\nFamily Type (FAM)\nGender (SEX)\nSocial Media Use (SNS)\nProvince (province)\n\nThe data is available in various formats. To avoid data loss, I decided to use the .dta format (SAS file). You need the haven package to read SAS files. However, this file is 150MB in size and since I am uploading the code in my GitHub repository, I am not able to use the SAS file. So I’ve saved the data in a .csv file with only the columns I will need. This is how you’d read a SAS file:\n\nds20 &lt;- read_dta(\"data/cius2020_2022nov18_en.dta\")\nds &lt;- ds20\n\nInstead, I run this:\n\nds10 &lt;- read.csv(\"ds.csv\")\nds &lt;- ds10 \n\nStep 1. Smartphone Users Select only those who use smartphones because the question is about “online banking” and not “mobile banking”.\n\n\n\nVariable Name: DV_010A\n\nConcept: Devices used\n\n\nQuestion Text:\nDuring the past three months, what devices did you use to access the Internet?\nDid you use:\nA smartphone\n\n\n\nAnswer Categories\nCode\n\n\n\n\nYes\n1\n\n\nNo\n2\n\n\nValid Skip\n6\n\n\nDon’t Know\n7\n\n\nRefusal\n8\n\n\nNot Stated\n9\n\n\n\n\n\nds &lt;- ds %&gt;% \n    mutate(\n        devSM = case_when(\n            dv_010a == 1 ~ 1, #yes\n            dv_010a == 2 ~ 0, #no\n        .default = -1, #any valid skip and not stated \n        )\n    )\n\nds &lt;- ds %&gt;% \n   filter(devSM == 1)\n\nStep 2. Mobile Banking Question Select the outcome variable, mobile banking:\n\n\n\nVariable Name: UI_050D\n\nConcept: Activities related to other online activities\n\n\nQuestion Text:\nDuring the past three months, which of the following other online activities, have you done over the Internet? Have you: Conducted online banking\n\n\n\nAnswer Categories\nCode\n\n\n\n\nYes\n1\n\n\nNo\n2\n\n\nValid Skip\n6\n\n\nDon’t Know\n7\n\n\nRefusal\n8\n\n\nNot Stated\n9\n\n\n\n\nStep 3. Select Other Model Variables Now I will move on to selecting the predictors. For Smartphone Dependency:\n\n\n\nVariable Name: SM_030A\n\nConcept: Frequency of use of smartphone\n\n\nQuestion Text:\nIn a typical day, how often do you check your smartphone?\n\n\n\nAnswer Categories\nCode\n\n\n\n\nAt least every 5 minutes\n01\n\n\nAt least every 15 minutes\n02\n\n\nAt least every 30 minutes\n03\n\n\nOne time per hour\n04\n\n\nOnce a day or a few times per day\n05\n\n\nLess than one time per day\n06\n\n\nValid skip\n96\n\n\nDon’t know\n97\n\n\nRefusal\n98\n\n\nNot stated\n99\n\n\n\n\nAs you can see, this one is no longer just a yes/no question, but has a few categories for answers. Since I’m thinking of tracking “dependence”, it makes sense that the more frequent checking gets a higher value. So, this is how I code this variable:\n\nds &lt;- ds %&gt;%\n    mutate(\n        #timeline : past 3 months \n        mBanking = case_when(\n            ui_050d == 1 ~ 1, # Yes \n            ui_050d == 2 ~ 0, # No \n            .default = -1 # valid skip, don't know, refused, not stated \n        ),\n        \n        SD = case_when(\n            sm_030a == 1 ~ 6, # At least every 5 minutes \n            sm_030a == 2 ~ 5, # At least every 15 minutes \n            sm_030a == 3 ~ 4, # At least every 30 minutes  \n            sm_030a == 4 ~ 3, # One time per hour  \n            sm_030a == 5 ~ 2, # Once a day or a few times per day  \n            sm_030a == 6 ~ 1, # Less than one time per day\n            .default = 96 # Valid skip 96 , Don’t know 97 , Refusal  98, Not stated  99\n        )\n    )\n\nds &lt;- ds %&gt;% filter(SD &lt; 10)\nds &lt;- ds %&gt;% filter(mBanking != -1) \n\nThe filters are just making sure that the skip’s, don’t know’s, refusal’s and not stated answers are dropped. This is not too big of a loss. There are 17,409 rows of data.\nFriendship Satisfaction:\n\n\n\nVariable Name: TS_010A\n\nConcept: Satisfaction with relationships\n\n\nQuestion Text:\nIn general, on a scale from 1 to 5 where 1 means «completely dissatisfied» and 5 means «completely satisfied», how satisfied are you with the relationships you have with the following people? Friends\n\n\n\nAnswer Categories\nCode\n\n\n\n\n1-Completely dissatisfied\n1\n\n\n2\n2\n\n\n3\n3\n\n\n4\n4\n\n\n5- Completely satisfied\n5\n\n\nValid skip\n6\n\n\nDon’t know\n7\n\n\nRefusal\n8\n\n\nNot stated\n9\n\n\n\n\nFamily Satisfaction:\n\n\n\nVariable Name: TS_010B\n\nConcept: Satisfaction with relationships\n\n\nQuestion Text:\nIn general, on a scale from 1 to 5 where 1 means «completely dissatisfied» and 5 means «completely satisfied», how satisfied are you with the relationships you have with the following people? Relatives or family members, excluding those you live with\n\n\n\nAnswer Categories\nCode\n\n\n\n\n1-Completely dissatisfied\n1\n\n\n2\n2\n\n\n3\n3\n\n\n4\n4\n\n\n5- Completely satisfied\n5\n\n\nValid skip\n6\n\n\nDon’t know\n7\n\n\nRefusal\n8\n\n\nNot stated\n9\n\n\n\n\nMental Health:\n\n\n\nVariable Name: FD_030A\n\nConcept: Perceived mental health\n\n\nQuestion Text:\nIn general, how is your mental health? Would you say:\n\n\n\nAnswer Categories\nCode\n\n\n\n\nExcellent\n1\n\n\nVery good\n2\n\n\nGood\n3\n\n\nFair\n4\n\n\nPoor\n5\n\n\nValid skip\n6\n\n\nDon’t know\n7\n\n\nRefusal\n8\n\n\nNot stated\n9\n\n\n\n\nAgain, since I want to measure mental health in terms of how good or bad a person is, it makes sense that better moods are associated with elevated numbers (so instead of it being coded as 1, Excellent should be coded as 5).\n\nds &lt;- ds %&gt;% mutate(\n    FRISAT = case_when(\n        ts_010a == 1 ~ 1, #completely dissatisfied \n        ts_010a == 2 ~ 2, \n        ts_010a == 3 ~ 3,\n        ts_010a == 4 ~ 4,\n        ts_010a == 5 ~ 5, #completely satisfied \n        .default = 6\n    ),\n    \n    FAMSAT = case_when(\n        ts_010b == 1 ~ 1, #completely dissatisfied \n        ts_010b == 2 ~ 2, \n        ts_010b == 3 ~ 3,\n        ts_010b == 4 ~ 4,\n        ts_010b == 5 ~ 5, #completely satisfied \n        .default = 6\n    ),\n    \n    MH = case_when(\n        fd_030a == 1 ~ 5, #excellent \n        fd_030a == 2 ~ 4, #very good \n        fd_030a == 3 ~ 3, #good \n        fd_030a == 4 ~ 2, #fair\n        fd_030a == 5 ~ 1, #poor\n        .default = 6\n    )\n)\n\nds &lt;- ds %&gt;% filter(\n    FRISAT &lt; 6,\n    FAMSAT &lt; 6,\n    MH &lt; 6\n)\n\nSocial Media use:\n\n\n\nVariable Name: UI_010C\n\nConcept: Activities related to communication\n\n\nQuestion Text:\nDuring the past three months, which of the following activities, related to communication, have you done over the Internet? Have you: Used social networking websites or apps\n\n\n\nAnswer Categories\nCode\n\n\n\n\nYes\n1\n\n\nNo\n2\n\n\nValid skip\n6\n\n\nDon’t know\n7\n\n\nRefusal\n8\n\n\nNot stated\n9\n\n\n\n\n\nds &lt;- ds %&gt;% mutate(\n    SNS = case_when(\n        ui_010c == 1 ~ 1, # yes \n        ui_010c == 2 ~ 0, # no \n        .default = 3\n    )\n)\n\nds &lt;- ds %&gt;% filter(\n    SNS &lt; 3\n)\n\nStep 4. Gather Demographic and Other Information Here, I will select some demographic information. Since these are not direct questions but information retrieved from other sources (such as postal codes for province), some of them do not have Skips/Don’t Know/Refusal/Not Stated answers. If they have, I have added those to the cards. I have also added a frequency column for these variables only to show you the demographic distribution of the sample (not weighted).\n\n\n\nVariable Name: PROVINCE\n\nConcept: PROVINCE\n\n\nNote:\nInformation derived using postal codes.\n\n\n\nAnswer Categories\nCode\nFrequency\n\n\n\n\nNewfoundland and Labrador\n10\n950\n\n\nPrince Edward Island\n11\n1,154\n\n\nNova Scotia\n12\n1,181\n\n\nNew Brunswick\n13\n1,223\n\n\nQuebec\n24\n3,911\n\n\nOntario\n35\n3,719\n\n\nManitoba\n46\n973\n\n\nSaskatchewan\n47\n979\n\n\nAlberta\n48\n1,365\n\n\nBritish Columbia\n59\n1,954\n\n\n\n\n\n\n\nVariable Name: AGE_GRP\n\nConcept: Age Groups – Derived variable\n\n\nNote:\nDerived from age of persons in the household\n\n\n\nAnswer Categories\nCode\nFrequency\n\n\n\n\n15 to 24 years\n01\n828\n\n\n25 to 34 years\n02\n1,805\n\n\n35 to 44 years\n03\n2,520\n\n\n45 to 54 years\n04\n2,469\n\n\n55 to 64 years\n05\n3,889\n\n\n65 years and over\n06\n5,898\n\n\n\n\n\n\n\nVariable Name: GENDER\n\nConcept: Gender - Derived variable\n\n\nNote:\nRefers to current gender which may be different from sex assigned at birth and may be different from what is indicated on legal documents. For data quality and confidentiality reasons, and because of the small population being measured, the dissemination of data according to ’Non binary’ Gender is not possible for this statistical program. So, this release uses a gender variable with only two categories. This variable is derived by looking at a large number of demographic characteristics from the respondent, it allows us to disseminate data on Gender that is reliable and unbiased.\n\n\n\nAnswer Categories\nCode\nFrequency\n\n\n\n\nMale\n1\n8,126\n\n\nFemale\n2\n9,283\n\n\n\n\n\n\n\nVariable Name: EMP\n\nConcept: Employment status - Derived variable\n\n\nNote:\n\n\n\nAnswer Categories\nCode\nFrequency\n\n\n\n\nEmployed\n1\n8,451\n\n\nNot Employed\n2\n8,177\n\n\nValid Skip\n6\n0\n\n\nDon’t Know\n7\n0\n\n\nRefusal\n8\n0\n\n\nNot Stated\n9\n781\n\n\n\n\n\n\n\nVariable Name: G_EDU\n\nConcept: Highest certificate - Derived variable\n\n\nNote:\n\n\n\nAnswer Categories\nCode\nFrequency\n\n\n\n\nHigh school or less\n1\n5,746\n\n\nSome post-secondary (incl. univ certificate)\n2\n5,866\n\n\nUniversity degree\n3\n4,889\n\n\nValid skip\n6\n0\n\n\nDon’t know\n7\n0\n\n\nRefusal\n8\n0\n\n\nNot stated\n9\n908\n\n\n\n\n\n\n\nVariable Name: G_HCOMP\n\nConcept: Type of household - Derived variable\n\n\nNote:\nThis derived variable indicates the household composition. It was derived using RRS_Q12 (number of persons in the household), RR_020CA (age of persons in the household) and RR_040AA (relationship of the respondent with the other members of the household).\n\n\n\nAnswer Categories\nCode\nFrequency\n\n\n\n\nFamily household with children under 18\n1\n3,700\n\n\nFamily household without children under 18\n2\n8,299\n\n\nSingle person household\n3\n4,837\n\n\nOther household type\n4\n414\n\n\nValid skip\n6\n0\n\n\nDon’t know\n7\n0\n\n\nRefusal\n8\n0\n\n\nNot stated\n9\n159\n\n\n\n\n\n\n\nVariable Name: HINCQUIN\n\nConcept: Census family income quintile - Derived variable\n\n\nNote:\nInformation derived using HINC. In order to obtain equal weighted counts in each category, cases with incomes equal to the category cutoffs were randomly assigned to one of the two categories on either side of the cutoff.\nSource: Annual Income Estimates for Census Families and Individuals (T1 Family File)\n\n\n\nAnswer Categories\nCode\nFrequency\n\n\n\n\nQuintile 1 - \\leq $44,119\n1\n4,200\n\n\nQuintile 2 - $44,120 - $75,321\n2\n3,911\n\n\nQuintile 3 - $75,322 - $109,431\n3\n3,394\n\n\nQuintile 4 - $109,432 - $162,799\n4\n3,185\n\n\nQuintile 5 - \\geq $162,800\n5\n2,719\n\n\n\n\n\n\n\nVariable Name: IMM_GSTA\n\nConcept: Immigrant status\n\n\nNote:\nLanded immigrants are permanent residents who have indicated a year of landing in Canada since 1980. Variable derived from LANDING_YEAR, IMDB.\nSource: Longitudinal Immigration Database (IMDB)\n\n\n\nAnswer Categories\nCode\nFrequency\n\n\n\n\nLanded immigrant\n1\n1,696\n\n\nNon-landed immigrant\n2\n15,701\n\n\nValid Skip\n6\n0\n\n\nDon’t Know\n7\n0\n\n\nRefusal\n8\n0\n\n\nNot Stated\n9\n12\n\n\n\n\nVariable pumfid is just a unique identifier.\n\nds &lt;- ds %&gt;% mutate(\n    id = pumfid,\n    province = province, \n    AGE = as.integer(age_grp),\n    SEX = gender,\n    EMP = ifelse(\n        emp == 2,\n        0,\n        emp\n    ),\n    EDU = g_edu,\n    FAM = g_hcomp, \n    IMM = ifelse(\n        imm_gsta == 2,\n        0,\n        imm_gsta\n    ),\n    INC = hincquin\n)\n\nds &lt;- ds %&gt;% filter(\n    SNS &lt; 3,\n    EMP &lt; 3,\n    FAM &lt; 5,\n    IMM &lt; 3\n)\n\nStep 5. Selecting The Variables From Data Set\n\nds &lt;- ds %&gt;% \n    dplyr::select(id, \n                  mBanking, SD, FAMSAT, FRISAT, MH, SNS,\n                  province, AGE, SEX, EMP, EDU,\n                  FAM, IMM, INC, wtpg)\n\nSize of the dataset:\n\ndim(ds)\n\n[1] 11176    16\n\n\n\n\n\n\npsych::describe(ds, type = 2)\n\n         vars     n      mean      sd    median   trimmed     mad      min\nid          1 11176 108666.23 5048.86 108637.50 108660.43 6530.85 100001.0\nmBanking    2 11176      0.86    0.34      1.00      0.95    0.00      0.0\nSD          3 11176      3.39    1.22      3.00      3.32    1.48      1.0\nFAMSAT      4 11176      4.27    0.92      5.00      4.42    0.00      1.0\nFRISAT      5 11176      4.28    0.90      5.00      4.42    0.00      1.0\nMH          6 11176      3.59    1.04      4.00      3.65    1.48      1.0\nSNS         7 11176      0.79    0.41      1.00      0.86    0.00      0.0\nprovince    8 11176     32.33   15.79     35.00     31.72   17.79     10.0\nAGE         9 11176      3.98    1.52      4.00      4.06    1.48      1.0\nSEX        10 11176      1.53    0.50      2.00      1.54    0.00      1.0\nEMP        11 11176      0.63    0.48      1.00      0.67    0.00      0.0\nEDU        12 11176      2.11    0.80      2.00      2.13    1.48      1.0\nFAM        13 11176      1.98    0.78      2.00      1.94    1.48      1.0\nIMM        14 11176      0.12    0.32      0.00      0.02    0.00      0.0\nINC        15 11176      3.10    1.37      3.00      3.12    1.48      1.0\nwtpg       16 11176   2054.12 2104.81   1470.54   1681.77 1375.86     18.2\n               max    range  skew kurtosis    se\nid       117407.00 17406.00  0.01    -1.22 47.76\nmBanking      1.00     1.00 -2.12     2.51  0.00\nSD            6.00     5.00  0.33    -0.70  0.01\nFAMSAT        5.00     4.00 -1.30     1.47  0.01\nFRISAT        5.00     4.00 -1.29     1.49  0.01\nMH            5.00     4.00 -0.39    -0.44  0.01\nSNS           1.00     1.00 -1.39    -0.07  0.00\nprovince     59.00    49.00  0.16    -1.13  0.15\nAGE           6.00     5.00 -0.27    -1.02  0.01\nSEX           2.00     1.00 -0.11    -1.99  0.00\nEMP           1.00     1.00 -0.56    -1.69  0.00\nEDU           9.00     8.00  0.27     2.35  0.01\nFAM           4.00     3.00  0.39    -0.37  0.01\nIMM           1.00     1.00  2.35     3.52  0.00\nINC           5.00     4.00 -0.08    -1.22  0.01\nwtpg      23728.02 23709.82  2.67    11.27 19.91\n\n\nSeems like it’s kind of rare for those that use m-banking to have lower MH scores. Descriptive statistics:\n\nggplot(data    = ds,\n       aes(x   = SD,\n           y   = wtpg,\n           color = as.factor(MH)))+ \n  geom_point() +\n  geom_jitter() +  \n  labs( x = \"Smartphone Dependency\", \n        y = \"Weight\", \n        color = \"MH\") + \n  theme_minimal() \n\n\n\n\n\n\n\n\nChecking Na’s:\n\nsum(is.na(ds))\n\n[1] 0\n\n\n\nglimpse(ds)\n\nRows: 11,176\nColumns: 16\n$ id       &lt;int&gt; 100001, 100002, 100003, 100004, 100005, 100007, 100008, 10001…\n$ mBanking &lt;dbl&gt; 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1…\n$ SD       &lt;dbl&gt; 2, 5, 4, 2, 5, 3, 4, 3, 2, 4, 4, 5, 2, 3, 4, 6, 4, 5, 4, 4, 2…\n$ FAMSAT   &lt;dbl&gt; 4, 5, 4, 5, 5, 4, 5, 4, 4, 4, 5, 5, 5, 4, 3, 4, 4, 4, 5, 5, 4…\n$ FRISAT   &lt;dbl&gt; 4, 5, 5, 5, 5, 4, 5, 4, 4, 4, 5, 4, 4, 4, 2, 4, 4, 4, 5, 3, 4…\n$ MH       &lt;dbl&gt; 4, 3, 4, 5, 3, 3, 5, 3, 4, 2, 3, 3, 3, 4, 2, 2, 4, 3, 4, 3, 4…\n$ SNS      &lt;dbl&gt; 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0…\n$ province &lt;int&gt; 59, 35, 35, 24, 48, 13, 48, 12, 35, 48, 13, 12, 24, 46, 24, 2…\n$ AGE      &lt;int&gt; 6, 2, 3, 6, 4, 4, 2, 2, 2, 5, 5, 4, 2, 6, 2, 1, 6, 3, 3, 4, 5…\n$ SEX      &lt;int&gt; 2, 2, 2, 2, 1, 1, 1, 1, 2, 2, 2, 2, 2, 1, 2, 1, 1, 1, 1, 1, 1…\n$ EMP      &lt;dbl&gt; 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1…\n$ EDU      &lt;int&gt; 2, 3, 3, 3, 1, 3, 2, 1, 2, 2, 2, 2, 3, 3, 3, 1, 3, 3, 2, 2, 1…\n$ FAM      &lt;int&gt; 2, 1, 2, 2, 3, 3, 1, 2, 1, 2, 2, 1, 3, 2, 1, 3, 3, 3, 1, 2, 2…\n$ IMM      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0…\n$ INC      &lt;int&gt; 4, 3, 2, 4, 2, 3, 2, 5, 4, 5, 2, 5, 2, 5, 2, 3, 4, 4, 4, 4, 3…\n$ wtpg     &lt;dbl&gt; 2524.87054, 3720.74855, 5802.87880, 1449.85202, 1155.15218, 1…\n\n\n\n\n\nVisualizng the relationship between Mental Health and M-banking:\n\nggplot(data = ds, aes(x = MH, y = wtpg)) +\n            geom_col(\n            aes(fill = as.factor(mBanking)), stat = \"identity\", color = \"black\", position = position_dodge(0.9)) +\n            fill_palette(\"Pastel1\") + \n            xlab(\"Mental Health\") +\n            ylab(\"Frequencies\") + labs(fill = \"Mbanking\")\n\n\n\n\n\n\n\n\nMH and controls: AGE, SEX, EMP, EDU, FAM, IMM, INC\n\ngg_fam &lt;- ggplot(data = ds , aes(MH, fill = as.factor(FAM))) + geom_bar(position = \"fill\") + labs(x = \"Mental Health\", y = \"Percentage (fill)\", fill = 'FAMILY') + fill_palette(\"Pastel1\")\n\ngg_age &lt;- ggplot(data = ds , aes(MH, fill = as.factor(AGE))) + geom_bar(position = \"fill\") + labs(x = \"Mental Health\", y = \"Percentage (fill)\", fill = 'AGE') + fill_palette(\"Pastel1\")\n\ngg_edu &lt;- ggplot(data = ds , aes(MH, fill = as.factor(EDU))) + geom_bar(position = \"fill\") + labs(x = \"Mental Health\", y = \"Percentage (fill)\", fill = 'EDU') + fill_palette(\"Pastel1\")\n\ngg_inc &lt;- ggplot(data = ds , aes(MH, fill = as.factor(INC))) + geom_bar(position = \"fill\") + labs(x = \"Mental Health\", y = \"Percentage (fill)\", fill = 'INC') + fill_palette(\"Pastel1\")\n\ngg_sex &lt;- ggplot(data = ds , aes(MH, fill = as.factor(SEX))) + geom_bar(position = \"fill\") + labs(x = \"Mental Health\", y = \"Percentage (fill)\", fill = 'SEX') + fill_palette(\"Pastel1\")\n\ngg_emp &lt;- ggplot(data = ds , aes(MH, fill = as.factor(EMP))) + geom_bar(position = \"fill\") + labs(x = \"Mental Health\", y = \"Percentage (fill)\", fill = 'EMP') + fill_palette(\"Pastel1\")\n\ngg_imm &lt;- ggplot(data = ds , aes(MH, fill = as.factor(IMM))) + geom_bar(position = \"fill\") + labs(x = \"Mental Health\", y = \"Percentage (fill)\", fill = 'Immigrant') + fill_palette(\"Pastel1\")\n\n\nggarrange(\n    gg_fam, gg_age, gg_edu, gg_inc, gg_sex, gg_emp, gg_imm,\n    labels = c(\"FAM\", \"AGE\", \"EDU\", \"INC\", \"SEX\", \"EMP\", \"IMM\"),\n    ncol = 3,\n    nrow = 3\n    \n) \n\n\n\n\n\n\n\n\nMH and other variables: SD, FAMSAT, FRISAT, SNS\n\ngg_frisat &lt;- ggplot(data = ds , aes(MH, fill = as.factor(FRISAT))) + geom_bar(position = \"fill\") + labs(x = \"Mental Health\", y = \"Percentage (fill)\", fill = 'FRISAT') + fill_palette(\"Pastel1\")\n\ngg_famsat &lt;- ggplot(data = ds , aes(MH, fill = as.factor(FAMSAT))) + geom_bar(position = \"fill\") + labs(x = \"Mental Health\", y = \"Percentage (fill)\", fill = 'FAMSAT') + fill_palette(\"Pastel1\")\n\ngg_sd &lt;- ggplot(data = ds , aes(MH, fill = as.factor(SD))) + geom_bar(position = \"fill\") + labs(x = \"Mental Health\", y = \"Percentage (fill)\", fill = 'SD') + fill_palette(\"Pastel1\")\n\ngg_sns &lt;- ggplot(data = ds , aes(MH, fill = as.factor(SNS))) + geom_bar(position = \"fill\") + labs(x = \"Mental Health\", y = \"Percentage (fill)\", fill = 'SNS') + fill_palette(\"Pastel1\")\n\nggarrange(\n    gg_frisat, gg_famsat, gg_sd, gg_sns,\n    labels = c(\"FRISAT\", \"FAMSAT\", \"SD\", \"SNS\"),\n    ncol = 2,\n    nrow = 2\n    \n) \n\n\n\n\n\n\n\n\n\n\nAll variables\n\nsle &lt;- ds %&gt;% dplyr::select(mBanking, SD, FAMSAT, FRISAT, SNS, AGE, SEX, EMP, EDU, FAM, INC, MH)\ncorM &lt;- Hmisc::rcorr(as.matrix(sle))\nreg_corM &lt;- as.matrix(corM$r)\n\ncolnames(reg_corM) &lt;- c(\"mBanking\", \"SD\", \"FAMSAT\", \"FRISAT\", \"SNS\", \"AGE\", \"SEX\", \"EMP\", \"EDU\", \"FAM\", \"INC\", \"MH\")\n# \nrownames(reg_corM) &lt;- c(\"mBanking\", \"SD\", \"FAMSAT\", \"FRISAT\", \"SNS\", \"AGE\", \"SEX\", \"EMP\", \"EDU\", \"FAM\", \"INC\", \"MH\")\n# \ncorrplot::corrplot(reg_corM, p.mat = corM$P, method = \"color\", type = \"upper\", insig = 'label_sig', sig.level = c(0.001, 0.01, 0.05), pch.cex = 0.9, order = 'AOE', tl.col = \"black\", tl.cex = 1, diag = F, col = corrplot::COL2('PuOr'))\n\n\n\n\n\n\n\n\nFRISAT and FAMSAT are obviously highly correlated. I’ll combine them into a new variable called RS for Relationship satisfaction.\n\nds &lt;- ds %&gt;% mutate(\n    RS = FAMSAT + FRISAT\n)\n\n\n\nUsing a Likelihood Ratio Test:\n\nmodel_famsatfrisat &lt;- glm(mBanking ~ FAMSAT + FRISAT, \n                          family = \"binomial\",\n                          data = ds)\n\nmodel_rs &lt;- glm(mBanking ~ RS, \n                          family = \"binomial\",\n                          data = ds)   \n\nanova(model_rs, model_famsatfrisat, test = \"Chisq\")               \n\nAnalysis of Deviance Table\n\nModel 1: mBanking ~ RS\nModel 2: mBanking ~ FAMSAT + FRISAT\n  Resid. Df Resid. Dev Df  Deviance Pr(&gt;Chi)\n1     11174     8890.9                      \n2     11173     8890.9  1 0.0045091   0.9465\n\n\nSince p &gt; 0.05, there is no significant difference in model performance. It’s better to keep one variable instead of two (simpler model is better).\nContinuing with our visualizations, let’s see the changes to SD across MH scores.\n\nggplot(data  = ds, aes(x = MH, y = SD))+ geom_point(size = 1.2, alpha = .5, position = \"jitter\") + \n    geom_smooth(method = lm,\n              se     = FALSE, \n              col    = \"red\",\n              size   = 2, \n              alpha  = .8)+ # to add regression line\n  theme_minimal()\n\n\n\n\n\n\n\n\nThere’s an almost negative linear relationship there: better mental health is associated with lower dependency on smartphones. This intuitively makes sense. Adding more nuance to this graph, I can see how this relationship changes for people who adopted m-banking and those that didn’t. It seems the two relationships are almost identical, at least in direction.\n\nggplot(data    = ds,\n       aes(x   = MH,\n           y   = SD,\n           col = as.factor(mBanking)))+ #to add the colours for different classes\n    geom_point(size     = 1.2,\n             alpha    = .8,\n             position = \"jitter\")+ \n    #scale_color_manual(name = \"MBanking\",\n                     #labels = c(\"No\", \"Yes\"),\n                     #values = c(\"red\", \"lightblue\")) + #+ ggnewscale::new_scale_color() +\n    geom_smooth(method   = lm,\n              se       = T, \n              size     = 1.5, \n              linetype = 1, \n              alpha    = .7,\n              ) + \n    scale_color_manual(name = \"MBanking\",\n                     labels = c(\"No\", \"Yes\"),\n                     values = c(\"black\", \"darkred\")) + theme_minimal()\n\n\n\n\n\n\n\n\nThe same type of visualizations can be helpful for other factors in the model. Now, I will separate the data into those that have adopted m-banking and those that haven’t. Let’s look at the differences in familial relationship satisfaction for these two groups across mental health. For m-banking adopters, it seems that those with better mental health (3 or higher) are completely satisfied with their family relationships (blue bar). On the other hand, those that haven’t adopted m-banking are either less satisfied or completely unsatisfied (MH = 5, mBanking = 0 group’s largest bar is red, which is FAMSAT = 1, the lowest score).\n\nds1 &lt;- ds %&gt;% filter(mBanking == 1)\nds2 &lt;- ds %&gt;% filter(mBanking == 0)\n\ngg5 &lt;- ggplot(data = ds1, aes(x = MH, y = wtpg)) +\n            geom_col(\n            aes(fill = as.factor(FAMSAT)), stat = \"identity\", color = \"black\", position = position_dodge(0.9)) +\n            fill_palette(\"RdBu\") + \n            xlab(\"Mental Health\") +\n            ylab(\"Weights\") + labs(fill = \"FAMSAT\")\n\ngg6 &lt;- ggplot(data = ds2 %&gt;% filter(mBanking == 0), aes(x = MH, y = wtpg)) +\n            geom_col(\n            aes(fill = as.factor(FAMSAT)), stat = \"identity\", color = \"black\", position = position_dodge(0.9)) +\n            fill_palette(\"RdBu\") + \n            xlab(\"Mental Health\") +\n            ylab(\"Weights\") + labs(fill = \"FAMSAT\")\n\nggarrange(gg5, gg6, ncol = 2, labels = c(\"mBanking = 1\", \"mBanking = 0\"))\n\n\n\n\n\n\n\n\nIt’s important to mention that visualizations don’t actually tell us the real story. Especially when working with large datasets with more than 10,000 data points. This is simply a somewhat visible pattern that tells me it’s worth looking into when I do my statistical analysis. We do the same type of visualization, now measuring social media use. Surprisingly, social media use goes up with better mental health scores for adopters but decreases for non-adopters.\n\ngg7 &lt;- ggplot(data = ds1, aes(x = MH, y = wtpg)) +\n            geom_col(\n            aes(fill = as.factor(SNS)), stat = \"identity\", color = \"black\", position = position_dodge(0.9)) +\n            fill_palette(\"RdBu\") + \n            xlab(\"Mental Health\") +\n            ylab(\"Weights\") + labs(fill = \"SNS\")\n\ngg8 &lt;- ggplot(data = ds2 %&gt;% filter(mBanking == 0), aes(x = MH, y = wtpg)) +\n            geom_col(\n            aes(fill = as.factor(SNS)), stat = \"identity\", color = \"black\", position = position_dodge(0.9)) +\n            fill_palette(\"RdBu\") + \n            xlab(\"Mental Health\") +\n            ylab(\"Weights\") + labs(fill = \"SNS\")\n\nggarrange(gg7, gg8, ncol = 2, labels = c(\"mBanking = 1\", \"mBanking = 0\"))\n\n\n\n\n\n\n\n\nLastly, we do the same with smartphone dependency. It seems that non-adopters are actually more dependent on their smartphones and also (with better mental health).\n\ngg9 &lt;- ggplot(data = ds1, aes(x = MH, y = wtpg)) +\n            geom_col(\n            aes(fill = as.factor(SD)), stat = \"identity\", color = \"black\", position = position_dodge(0.9)) +\n            fill_palette(\"RdBu\") + \n            xlab(\"Mental Health\") +\n            ylab(\"Weights\") + labs(fill = \"SD\")\n\ngg10 &lt;- ggplot(data = ds2 %&gt;% filter(mBanking == 0), aes(x = MH, y = wtpg)) +\n            geom_col(\n            aes(fill = as.factor(SD)), stat = \"identity\", color = \"black\", position = position_dodge(0.9)) +\n            fill_palette(\"RdBu\") + \n            xlab(\"Mental Health\") +\n            ylab(\"Weights\") + labs(fill = \"SD\")\n\nggarrange(gg9, gg10, ncol = 2, labels = c(\"mBanking = 1\", \"mBanking = 0\"))",
    "crumbs": [
      "Home",
      "Projects",
      "Project 2. Mental Health",
      "Effect Of Mental Health on Mobile Banking"
    ]
  },
  {
    "objectID": "study2_DA.html#modeling",
    "href": "study2_DA.html#modeling",
    "title": "Effect Of Mental Health on Mobile Banking",
    "section": "Modeling",
    "text": "Modeling\nI prepare the data for modeling by:\n\nCalculating mean-centered values for the numeric columns for easier interpretation since no value has a true zero.\nI convert some variables to factors, which are categorical variables with different levels and define various reference points for each for easier modeling practices\nNote that if a variable’s reference level is l, then all the logistic regression coefficients for other levels are in comparison to level l\n\n\nds &lt;- ds %&gt;% mutate(\n    MH_c = MH - mean(MH), \n    SD_c = SD - mean(SD),\n    SNS_f = as.factor(SNS),\n    RS_c = RS - mean(RS),\n    AGE_c = AGE - mean(AGE),\n    SEX_f = as.factor(SEX),\n    EMP_f = as.factor(EMP),\n    EDU_c = EDU - mean(EDU),\n    FAM_f = as.factor(FAM),\n    INC_c = INC - mean(INC),\n    IMM_f = as.factor(IMM),\n    PRVNC = as.factor(province)\n)\n\nds &lt;- ds %&gt;% \n    mutate(\n        # SEX \n        SEX_factor_Fem = relevel(SEX_f, ref = '2'),\n        SEX_factor_Mal = relevel(SEX_f, ref = '1'),\n        # EMP\n        EMP_factor_not = relevel(EMP_f, ref = '0'),\n        EMP_factor_Emp = relevel(EMP_f, ref = '1'),\n        # FAM \n        FAM_factor_1 = relevel(FAM_f, ref = '1'),\n        FAM_factor_2 = relevel(FAM_f, ref = '2'),\n        FAM_factor_3 = relevel(FAM_f, ref = '3'),\n        FAM_factor_4 = relevel(FAM_f, ref = '4'),\n        # IMM\n        IMM_factor_Imm = relevel(IMM_f, ref = '1'),\n        IMM_factor_non = relevel(IMM_f, ref = '0'),\n        # SNS \n        SNS_factor_notuse = relevel(SNS_f, ref = '0'),\n        SNS_factor_use = relevel(SNS_f, ref = '1')\n    )\n\n\nConsidering Sampling Clusters\nThis dataset was sampled by clustering on provinces. CIUS does not disclose their sample measure in full detail, but it is best practice to consider this sampling design in the model. There are multiple ways to do it:\n\nApply the weights to the model in glm(); since the dataset is quite large, adding the weights will render everything statistically significant and therefore is not useful for our purposes\nUse a survey-specific module like the survey package developed by Thomas Lumley to account for survey methods, stratification, framing, sampling strategy and weights. The way you’d do this is:\n\n\nlibrary(survey)\n\n# define survey design \nds_svy &lt;- svydesign(ids = ~province, weights = ~wtpg, data = ds)\n\n# run survey glm : svyglm()\nmodel1_svy &lt;- svyglm(\n    mBanking ~ MH_c + SD_c + SNS_factor_use + RS_c + AGE_c + SEX_factor_Fem + ..., \n    family=stats::gaussian(),\n    start=NULL, rescale=TRUE, deff=FALSE,influence=FALSE)\n\nsummary(model1_svy, correlation = FALSE, df.resid=NULL)\n\n\nAnother method is to consider robust standard errors\nLastly, the method I’ve decided on is to consider this clustering in the model as a group using mixed-effect models\n\nLet’s visualize the distribution of m-banking adoption across provinces for all MH scores:\n\nds &lt;- ds %&gt;% \n    mutate(\n        province_f_coded = fct_recode(\n            PRVNC,\n            'NL' = '10',\n            'NS' = '12', \n            'NB' = '13',\n            'QC' = '24',\n            'ON' = '35', \n            'MB' = '46', \n            'SK' = '47', \n            'AB' = '48', \n            'BC' = '59'\n        )\n    )\n\n\nggplot(ds, aes(province_f_coded, mBanking, color = as.factor(MH))) +\n                  stat_summary(fun = mean, geom = \"point\") +\n                  stat_summary(fun.data = mean_cl_boot, geom = \"errorbar\", width = 0.4) +\n                  theme_set(theme_bw(base_size = 10)) +\n                  theme(legend.position = \"top\") +\n                  labs(x = \"Province\", y = \"Observed Probabilty of mobile banking\", color = \"MH\") + theme_minimal()\n\n\n\n\n\n\n\n\nThere is clearly some variation in the probabilities of m-banking and the effects of MH on this across the provinces.\n\n\nMathematical Modeling\nFollowing the paper, I have these models (I’m skipping over the choise of RS instead of FAMSAT + FRISAT as it was discussed above):\nModel 1. Standard Logistic Regression\n\n\\begin{equation*}\n\\begin{split}\n  & \\ln\\frac{P(Y = 1)}{1 - P(Y = 1)} = \\\\\n  & \\ \\beta_0 + \\beta_{1} \\ MH + \\ \\beta_2 \\ SD + \\ \\beta_3 \\ D_{SNS_0} + \\ \\beta_4 \\ AGE \\ + \\ \\beta_5 \\ D_{SEX_m} \\\\\n  &  + \\ \\beta_6 \\ D_{EMP_0} \\ + \\ \\beta_7 \\ EDU + \\ \\beta_8 \\ INC \\ + \\ \\beta_9 \\ D_{FAM_1} + \\ \\beta_{10} \\ D_{FAM_3}\\\\\n  & + \\ \\beta_{11} \\ D_{FAM_4} \\ + \\ \\beta_{12} \\ D_{IMM_1} + \\ \\beta_{13} \\ RS + \\ \\epsilon \\\\\n\\end{split}\n\\end{equation*}\n\nWhere D is a dummy variable representing the case that the value of the variable in the index is some specific level. For example, D_{SNS_0} gets a 1 in every cell where SNS = 0. This is modeled in R:\n\nmodel1 &lt;- glm(\n    mBanking ~ MH_c + SD_c + SNS_factor_use + RS_c + AGE_c + SEX_factor_Fem + EMP_factor_Emp + EDU_c + FAM_factor_2 + IMM_factor_non + INC_c,\n    data = ds,\n    family = \"binomial\"\n)\n\nModel 2. Fixed Effect Logistic Regression\n\n\\begin{equation*}\n\\begin{split}\n  & \\ln\\frac{P(Y = 1)}{1 - P(Y = 1)} = \\\\\n  & \\ \\gamma_{0,j} + u_{0,j} + \\beta_{1} \\ MH + \\ \\beta_2 \\ SD + \\ \\beta_3 \\ D_{SNS_0} + \\ \\beta_4 \\ AGE \\ + \\ \\beta_5 \\ D_{SEX_m}\\\\\n  &  + \\ \\beta_6 \\ D_{EMP_0} \\ + \\ \\beta_7 \\ EDU + \\ \\beta_8 \\ INC \\ + \\ \\beta_9 \\ D_{FAM_1} + \\ \\beta_{10} \\ D_{FAM_3}\\\\\n  & + \\ \\beta_{11} \\ D_{FAM_4} \\ + \\ \\beta_{12} \\ D_{IMM_1} + \\ \\beta_{13} \\ RS + \\ \\epsilon \\\\\n\\end{split}\n\\end{equation*}\n\nThis is where province only affects the intercept (m-banking). I used the glmer function from the lme4 package.\n\nmodel2 &lt;- glmer(\n    mBanking ~ MH_c + SD_c + SNS_factor_use + RS_c + AGE_c + SEX_factor_Fem + EMP_factor_Emp + EDU_c + FAM_factor_2 + IMM_factor_non + INC_c + (1 | province),\n    data = ds,\n    family = binomial(),\n    control = glmerControl(optimizer = \"bobyqa\")\n)\n\nModel 3. Random Effect Logistic Regression\n\n\\begin{equation*}\n\\begin{split}\n   & \\ln\\frac{P(Y = 1)}{1 - P(Y = 1)} = \\\\\n   & \\ \\gamma_{0,0} + u_{0,j} + (\\gamma_{1,0} + u_{1,j}) \\ MH + \\ (\\gamma_{2,0} + u_{2,j}) \\ SD \\ + \\ (\\gamma_{3,0} + u_{3,j}) \\ SNS \\\\\n   & + \\ (\\gamma_{4,0} + u_{4,j}) \\ AGE \\ + \\ (\\gamma_{5,0} + u_{5,j}) \\ SEX \\ + \\ (\\gamma_{6,0} + u_{6,j}) EMP \\\\\n   & + \\ (\\gamma_{7,0} + u_{7,j}) \\ EDU \\ + \\ (\\gamma_{8,0} + u_{8,j}) \\ INC \\\n    + \\ (\\gamma_{9,0} + u_{9,j}) \\ D_{FAM_1} \\\\\n   & + \\ (\\gamma_{10,0} + u_{10,j}) \\ D_{FAM_3} \\ + \\ (\\gamma_{11,0} + u_{11,j}) \\ D_{FAM_4} \\\\\n   & + \\ (\\gamma_{12,0} + u_{12,j}) \\ IMM_1 \\ + \\ (\\gamma_{13,0} + u_{13,j}) \\ RS + \\ \\epsilon \\\\\n\\end{split}\n\\end{equation*}\n\nThis is where province only affects everything. This model may not converge (it does in this case, but it takes about 30 minutes with no other processes running on a Macbook Air M2). An easier model that considers random effects may be to just include the random effect on the primary variable MH and its related constructs (model4).\n\nmodel3 &lt;- glmer(\n    mBanking ~ MH_c + SD_c + SNS_factor_use + RS_c + AGE_c + SEX_factor_Fem + EMP_factor_Emp + EDU_c + FAM_factor_2 + IMM_factor_non + INC_c + \n    (1 + MH_c + SD_c + SNS + RS_c + AGE_c + SEX_f + EMP + EDU_c + FAM_2 + IMM_n + \n    INC_c | province),\n    data = ds,\n    family = binomial(link = \"logit\"),\n    control = glmerControl(optimizer = \"bobyqa\"))\n\nFor faster speeds, I’ll test everything with this model (random):\n\nmodel4 &lt;- glmer(\n    mBanking ~ MH_c + SD_c + SNS_factor_use + RS_c + AGE_c + SEX_factor_Fem + EMP_factor_Emp + EDU_c + FAM_factor_2 + IMM_factor_non + INC_c + \n    (1 + MH_c + SD_c + SNS_factor_use + RS_c | province),\n    data = ds,\n    family = binomial(link = \"logit\"),\n    control = glmerControl(optimizer = \"bobyqa\"))\n\n\n\nComparing models\nSince model3 doesn’t run fast and I am using Quarto for rendering the code blocks, I decided to use model4 for comparisons. The values here will be different from the paper, but the ideas are the same. To pick the best model I do:\n\nLikelihood Ratio Test to pick between model1 vs model2 - the null hypothesis is that there is no significant difference between the models (if you reject this, then you should go with the model that captures more information)\n\nIf you reject H_0 of LR test, i.e., model2 wins \\implies we compare it to model4 and we need the Hausman test\n\nUnfortunately, the Hausman test requires the plm package and panel data. To avoid messing the data up again, I coded the calculations manually based on the Hausman formula. This is referenced in the paper.\n\nIf you can’t reject H_0 of LR test, i.e., model1 wins, we should just use that as it’s simpler\n\n\n\ntest_performance(model1, model2)\n\nName   |    Model |     BF | df | df_diff |  Chi2 |      p\n----------------------------------------------------------\nmodel1 |      glm |        | 14 |         |       |       \nmodel2 | glmerMod | 333.29 | 15 |    1.00 | 20.94 | &lt; .001\nModels were detected as nested (in terms of fixed parameters) and are compared in sequential order.\n\n\nSince model2 won, we now compare it with model4. Hausman test for fixed effects in R is a bit tricky, so, I did it manually following the formula:\n\nExtract per-group (province) coefficients for both models and convert them to data frames\n\n\ncoefs_fixed &lt;- coef(model2)\ncoefs_rando &lt;- coef(model4)\n\ncoefs_fixed_df &lt;- as.data.frame(coefs_fixed$province)\ncoefs_rando_df &lt;- as.data.frame(coefs_rando$province)\n\n\nSubtract Random Effects model coefficients from Fixed Effect model, per province. This gives you the core term of the Hausman test:\n\n\ncoefs_diff &lt;- coefs_fixed_df - coefs_rando_df\ncoef_diffs_matrix &lt;- as.matrix(coefs_diff)\n\n\nEstimate the difference in the variance-covariance matrices of the coefficient estimates.\n\n\nV_diff &lt;- as.matrix(vcov(model2) - vcov(model4))\n\nWhat I need is:\n\nH = (\\hat{\\beta_{F}} - \\hat{\\beta_{R}}) \\cdot V^{-1} \\cdot (\\hat{\\beta_{F}} - \\hat{\\beta_{R}})^T\n\nJust checking that the matrix multiplications make sense:\n\ndim(t(coef_diffs_matrix))\n\n[1] 14 10\n\ndim(solve(V_diff))\n\n[1] 14 14\n\ndim(coef_diffs_matrix)\n\n[1] 10 14\n\n\nThey do! So, calculate H:\n\nH &lt;- coef_diffs_matrix %*% solve(V_diff) %*% t(coef_diffs_matrix)\n\nThis is the actual critical \\chi^2 value at degrees of freedom 13 (for 14 covariates), in fact, I can check:\n\nqr(V_diff)$rank\n\n[1] 14\n\n\n\nchisq_critical &lt;- qchisq(p = .05, df = 13, lower.tail = FALSE)\nchisq_critical\n\n[1] 22.36203\n\n\nIf H1 &gt; \\chi^2 then reject the null hypothesis that says the fixed model is better.\n\nH &gt; chisq_critical #reject H0: the fixed model is better.  \n\n      10    11    12    13    24    35    46    47    48    59\n10 FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n11 FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n12 FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n13 FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n24 FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n35 FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n46 FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n47 FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n48 FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n59 FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n\n\nThe p-value:\n\npchisq(H, df = 13, lower.tail = FALSE)\n\n          10        11        12        13        24        35        46\n10 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000\n11 1.0000000 1.0000000 1.0000000 1.0000000 0.9999569 1.0000000 1.0000000\n12 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 0.9999936\n13 1.0000000 1.0000000 1.0000000 1.0000000 0.9998382 1.0000000 1.0000000\n24 1.0000000 0.9999569 1.0000000 0.9998382 1.0000000 0.9999958 1.0000000\n35 1.0000000 1.0000000 1.0000000 1.0000000 0.9999958 1.0000000 1.0000000\n46 1.0000000 1.0000000 0.9999936 1.0000000 1.0000000 1.0000000 1.0000000\n47 0.9999977 1.0000000 0.9999852 1.0000000 1.0000000 0.9999937 0.9999616\n48 1.0000000 1.0000000 0.9999999 1.0000000 1.0000000 1.0000000 0.9999067\n59 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000\n          47        48 59\n10 0.9999977 1.0000000  1\n11 1.0000000 1.0000000  1\n12 0.9999852 0.9999999  1\n13 1.0000000 1.0000000  1\n24 1.0000000 1.0000000  1\n35 0.9999937 1.0000000  1\n46 0.9999616 0.9999067  1\n47 0.9994578 0.9999190  1\n48 0.9999190 0.9999998  1\n59 1.0000000 1.0000000  1\n\n\nOk, we can’t reject this hypothesis - therefore, the fixed model is better. Another way to check:\n\nanova(model2, model4)\n\nData: ds\nModels:\nmodel2: mBanking ~ MH_c + SD_c + SNS_factor_use + RS_c + AGE_c + SEX_factor_Fem + EMP_factor_Emp + EDU_c + FAM_factor_2 + IMM_factor_non + INC_c + (1 | province)\nmodel4: mBanking ~ MH_c + SD_c + SNS_factor_use + RS_c + AGE_c + SEX_factor_Fem + EMP_factor_Emp + EDU_c + FAM_factor_2 + IMM_factor_non + INC_c + (1 + MH_c + SD_c + SNS_factor_use + RS_c | province)\n       npar    AIC    BIC  logLik deviance  Chisq Df Pr(&gt;Chisq)\nmodel2   15 8176.0 8285.8 -4073.0   8146.0                     \nmodel4   29 8202.8 8415.1 -4072.4   8144.8 1.2569 14          1\n\n\nThe models are basically the same - which means, go with the simpler one! So, best model is model2. Now adding interaction terms to test the hypotheses of study 2, which are:\n\nMental Health significantly affects mobile banking adoption.\nRelationship Satisfaction moderates the effect of Mental Health on adoption.\nSmartphone Dependency moderates the effect of Mental Health on adoption.\nSocial Media use moderates the effect of Mental Health on adoption.\n\n\nmodel2_int &lt;- glmer(\n    mBanking ~ MH_c + SD_c + SNS_factor_use + RS_c + AGE_c + SEX_factor_Fem \n    + EMP_factor_Emp + EDU_c + FAM_factor_2 + IMM_factor_non + INC_c \n    + MH_c:RS_c + MH_c:SD_c + MH_c:SNS_factor_use \n    + (1 | province),\n    data = ds,\n    family = binomial(),\n    control = glmerControl(optimizer = \"bobyqa\"))\n\nPrinting both odds ratios and log-odds versions using two different summary functions:\n\nsumm(\n    model2,\n    scale = F,\n    pvals = T,\n    exp = T, \n    digits = 3,\n    #part.corr = T, #Print partial (labeled \"partial.r\") and semipartial (labeled \"part.r\")\n    #confint = getOption(\"summ-confint\", FALSE),\n    #ci.width = getOption(\"summ-ci.width\", 0.95),\n    #vifs = T\n)\n\n\n\n\n\nObservations\n11176\n\n\nDependent variable\nmBanking\n\n\nType\nMixed effects generalized linear model\n\n\nFamily\nbinomial\n\n\nLink\nlogit\n\n\n\n\n \n\n\n\n\nAIC\n8176.019\n\n\nBIC\n8285.842\n\n\nPseudo-R² (fixed effects)\n0.143\n\n\nPseudo-R² (total)\n0.148\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\nFixed Effects\n\n\n\n\nexp(Est.)\nS.E.\nz val.\np\n\n\n\n\n(Intercept)\n13.580\n0.081\n32.200\n0.000\n\n\nMH_c\n0.920\n0.030\n-2.753\n0.006\n\n\nSD_c\n1.195\n0.027\n6.622\n0.000\n\n\nSNS_factor_use0\n0.377\n0.064\n-15.308\n0.000\n\n\nRS_c\n1.020\n0.018\n1.084\n0.279\n\n\nAGE_c\n1.026\n0.024\n1.084\n0.278\n\n\nSEX_factor_Fem1\n0.893\n0.059\n-1.924\n0.054\n\n\nEMP_factor_Emp0\n0.557\n0.063\n-9.313\n0.000\n\n\nEDU_c\n1.570\n0.039\n11.661\n0.000\n\n\nFAM_factor_21\n0.786\n0.079\n-3.031\n0.002\n\n\nFAM_factor_23\n0.947\n0.078\n-0.702\n0.483\n\n\nFAM_factor_24\n1.006\n0.187\n0.034\n0.973\n\n\nIMM_factor_non1\n0.810\n0.094\n-2.256\n0.024\n\n\nINC_c\n1.050\n0.024\n2.013\n0.044\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\nRandom Effects\n\n\n\nGroup\nParameter\nStd. Dev.\n\n\n\n\nprovince\n(Intercept)\n0.142\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\nGrouping Variables\n\n\n\nGroup\n# groups\nICC\n\n\n\n\nprovince\n10\n0.006\n\n\n\n\n\n\n\nsummary(model2)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: \nmBanking ~ MH_c + SD_c + SNS_factor_use + RS_c + AGE_c + SEX_factor_Fem +  \n    EMP_factor_Emp + EDU_c + FAM_factor_2 + IMM_factor_non +  \n    INC_c + (1 | province)\n   Data: ds\nControl: glmerControl(optimizer = \"bobyqa\")\n\n     AIC      BIC   logLik deviance df.resid \n  8176.0   8285.8  -4073.0   8146.0    11161 \n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-16.2213   0.2326   0.3067   0.4159   1.1380 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n province (Intercept) 0.02003  0.1415  \nNumber of obs: 11176, groups:  province, 10\n\nFixed effects:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)      2.608586   0.081011  32.200  &lt; 2e-16 ***\nMH_c            -0.082966   0.030135  -2.753  0.00590 ** \nSD_c             0.178056   0.026889   6.622 3.54e-11 ***\nSNS_factor_use0 -0.974642   0.063671 -15.308  &lt; 2e-16 ***\nRS_c             0.019430   0.017930   1.084  0.27852    \nAGE_c            0.025562   0.023579   1.084  0.27833    \nSEX_factor_Fem1 -0.113369   0.058935  -1.924  0.05440 .  \nEMP_factor_Emp0 -0.586067   0.062929  -9.313  &lt; 2e-16 ***\nEDU_c            0.450903   0.038666  11.661  &lt; 2e-16 ***\nFAM_factor_21   -0.240510   0.079362  -3.031  0.00244 ** \nFAM_factor_23   -0.054846   0.078170  -0.702  0.48292    \nFAM_factor_24    0.006352   0.186508   0.034  0.97283    \nIMM_factor_non1 -0.210967   0.093525  -2.256  0.02409 *  \nINC_c            0.048330   0.024007   2.013  0.04410 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nsumm(\n    model2_int,\n    scale = F,\n    pvals = T,\n    exp = T, \n    digits = 3,\n    #part.corr = T, #Print partial (labeled \"partial.r\") and semipartial (labeled \"part.r\")\n    #confint = getOption(\"summ-confint\", FALSE),\n    #ci.width = getOption(\"summ-ci.width\", 0.95),\n    #vifs = T\n)\n\n\n\n\n\nObservations\n11176\n\n\nDependent variable\nmBanking\n\n\nType\nMixed effects generalized linear model\n\n\nFamily\nbinomial\n\n\nLink\nlogit\n\n\n\n\n \n\n\n\n\nAIC\n8178.438\n\n\nBIC\n8310.225\n\n\nPseudo-R² (fixed effects)\n0.144\n\n\nPseudo-R² (total)\n0.149\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\nFixed Effects\n\n\n\n\nexp(Est.)\nS.E.\nz val.\np\n\n\n\n\n(Intercept)\n13.584\n0.081\n32.059\n0.000\n\n\nMH_c\n0.888\n0.036\n-3.246\n0.001\n\n\nSD_c\n1.193\n0.027\n6.560\n0.000\n\n\nSNS_factor_use0\n0.372\n0.064\n-15.449\n0.000\n\n\nRS_c\n1.020\n0.018\n1.081\n0.279\n\n\nAGE_c\n1.026\n0.024\n1.104\n0.269\n\n\nSEX_factor_Fem1\n0.894\n0.059\n-1.900\n0.057\n\n\nEMP_factor_Emp0\n0.556\n0.063\n-9.305\n0.000\n\n\nEDU_c\n1.570\n0.039\n11.667\n0.000\n\n\nFAM_factor_21\n0.787\n0.079\n-3.023\n0.003\n\n\nFAM_factor_23\n0.948\n0.078\n-0.688\n0.491\n\n\nFAM_factor_24\n1.003\n0.187\n0.017\n0.987\n\n\nIMM_factor_non1\n0.812\n0.094\n-2.229\n0.026\n\n\nINC_c\n1.049\n0.024\n2.011\n0.044\n\n\nMH_c:RS_c\n1.002\n0.016\n0.116\n0.908\n\n\nMH_c:SD_c\n1.016\n0.023\n0.685\n0.493\n\n\nMH_c:SNS_factor_use0\n1.121\n0.061\n1.874\n0.061\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\nRandom Effects\n\n\n\nGroup\nParameter\nStd. Dev.\n\n\n\n\nprovince\n(Intercept)\n0.142\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\nGrouping Variables\n\n\n\nGroup\n# groups\nICC\n\n\n\n\nprovince\n10\n0.006\n\n\n\n\n\n\n\nsummary(model2_int)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: \nmBanking ~ MH_c + SD_c + SNS_factor_use + RS_c + AGE_c + SEX_factor_Fem +  \n    EMP_factor_Emp + EDU_c + FAM_factor_2 + IMM_factor_non +  \n    INC_c + MH_c:RS_c + MH_c:SD_c + MH_c:SNS_factor_use + (1 |      province)\n   Data: ds\nControl: glmerControl(optimizer = \"bobyqa\")\n\n     AIC      BIC   logLik deviance df.resid \n  8178.4   8310.2  -4071.2   8142.4    11158 \n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-16.1594   0.2314   0.3066   0.4157   1.1163 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n province (Intercept) 0.0201   0.1418  \nNumber of obs: 11176, groups:  province, 10\n\nFixed effects:\n                      Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)           2.608913   0.081379  32.059  &lt; 2e-16 ***\nMH_c                 -0.118271   0.036438  -3.246  0.00117 ** \nSD_c                  0.176551   0.026915   6.560 5.39e-11 ***\nSNS_factor_use0      -0.989511   0.064049 -15.449  &lt; 2e-16 ***\nRS_c                  0.019530   0.018059   1.081  0.27950    \nAGE_c                 0.026100   0.023634   1.104  0.26944    \nSEX_factor_Fem1      -0.112071   0.058972  -1.900  0.05738 .  \nEMP_factor_Emp0      -0.586152   0.062993  -9.305  &lt; 2e-16 ***\nEDU_c                 0.451151   0.038668  11.667  &lt; 2e-16 ***\nFAM_factor_21        -0.240031   0.079399  -3.023  0.00250 ** \nFAM_factor_23        -0.053828   0.078216  -0.688  0.49133    \nFAM_factor_24         0.003119   0.186692   0.017  0.98667    \nIMM_factor_non1      -0.208542   0.093579  -2.229  0.02585 *  \nINC_c                 0.048288   0.024016   2.011  0.04436 *  \nMH_c:RS_c             0.001805   0.015611   0.116  0.90795    \nMH_c:SD_c             0.016050   0.023429   0.685  0.49332    \nMH_c:SNS_factor_use0  0.114106   0.060903   1.874  0.06099 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAlso calculating the confidence interval for the variances of each model (we should really only care about the intercept variance, maybe MH):\n\nround(confint(model2),3)\n\n\nAnd the random effects for provinces (for visualization later):\n\nranefs_ &lt;- ranef(model2)\n\n\n\nMarginal Effects\nFirst, let’s see which model is better:\n\nanova(model2, model2_int)\n\nData: ds\nModels:\nmodel2: mBanking ~ MH_c + SD_c + SNS_factor_use + RS_c + AGE_c + SEX_factor_Fem + EMP_factor_Emp + EDU_c + FAM_factor_2 + IMM_factor_non + INC_c + (1 | province)\nmodel2_int: mBanking ~ MH_c + SD_c + SNS_factor_use + RS_c + AGE_c + SEX_factor_Fem + EMP_factor_Emp + EDU_c + FAM_factor_2 + IMM_factor_non + INC_c + MH_c:RS_c + MH_c:SD_c + MH_c:SNS_factor_use + (1 | province)\n           npar    AIC    BIC  logLik deviance  Chisq Df Pr(&gt;Chisq)\nmodel2       15 8176.0 8285.8 -4073.0   8146.0                     \nmodel2_int   18 8178.4 8310.2 -4071.2   8142.4 3.5812  3     0.3104\n\n\n\ntest_performance(model2, model2_int)\n\nName       |    Model |      BF | df | df_diff | Chi2 |     p\n-------------------------------------------------------------\nmodel2     | glmerMod |         | 15 |         |      |      \nmodel2_int | glmerMod | &lt; 0.001 | 18 |    3.00 | 3.58 | 0.310\nModels were detected as nested (in terms of fixed parameters) and are compared in sequential order.\n\n\nSince model2 is simpler, and they’re not different, I’ll go with that. Average marginal effects are:\n\nmargins_summary(model2)\n\n          factor     AME     SE        z      p   lower   upper\n           AGE_c  0.0028 0.0026   1.0838 0.2785 -0.0022  0.0078\n           EDU_c  0.0490 0.0044  11.1029 0.0000  0.0403  0.0576\n EMP_factor_Emp0 -0.0663 0.0076  -8.6816 0.0000 -0.0813 -0.0513\n   FAM_factor_21 -0.0269 0.0091  -2.9500 0.0032 -0.0447 -0.0090\n   FAM_factor_23 -0.0058 0.0083  -0.6964 0.4862 -0.0220  0.0105\n   FAM_factor_24  0.0007 0.0192   0.0341 0.9728 -0.0370  0.0383\n IMM_factor_non1 -0.0241 0.0113  -2.1415 0.0322 -0.0462 -0.0020\n           INC_c  0.0053 0.0026   2.0105 0.0444  0.0001  0.0104\n            MH_c -0.0090 0.0033  -2.7429 0.0061 -0.0155 -0.0026\n            RS_c  0.0021 0.0020   1.0827 0.2790 -0.0017  0.0059\n            SD_c  0.0193 0.0030   6.5079 0.0000  0.0135  0.0252\n SEX_factor_Fem1 -0.0123 0.0064  -1.9175 0.0552 -0.0250  0.0003\n SNS_factor_use0 -0.1254 0.0100 -12.5093 0.0000 -0.1450 -0.1057\n\n\n\n\nFixed Effects of Provinces Visualized\nTo finish this discussion, I’ll visualize the fixed effects of provinces. The y-axis shows the intercept shift (effect) for each province. A higher value on the y-axis (positive effect) means that after controlling for all variables, people in that province have higher baseline odds of using m-banking. A lower value (negative effect) means lower baseline odds. Since Quebec (QC) and British Columbia (BC) have the highest positive effects, we can say mobile banking is more common there, even after accounting for mental health and other variables. On the opposite end, Alberta (AB) and Newfoundland & Labrador (NL) have the lowest random effects, meaning adoption is lower in these provinces.\n\nprov_ &lt;- c('NL', 'NS', 'NB', 'QC', 'ON', 'MB', 'SK', 'AB', 'BC')\nprov_n &lt;- c(10, 12, 13, 24, 35, 46, 47, 48, 59)\n\n# random effects are from model 2 \nranefs_ &lt;- c(-0.029445800, 0.001523515,-0.034782017, 0.308732844,-0.132427568,-0.129533645,-0.047807245,-0.001728474,0.053000416)\nranefs_ &lt;- round(ranefs_, 4)\n\nd_graph &lt;- cbind(prov_, prov_n, ranefs_)\nd_graph &lt;- as.data.frame(d_graph)\n\nprovs_fullnames &lt;- c('Newfoundland and Labrador', 'Nova Scotia', 'New Brunswick','Quebec', 'Ontario', 'Manitoba', 'Saskatchewan', 'Alberta', 'British Columbia')\n\n\nggplot(data = d_graph, aes(x = prov_n, y = ranefs_, label = c('NL', 'NS', 'NB', 'QC', 'ON', 'MB', 'SK', 'AB', 'BC'))) +   \n    geom_point(size = 2, alpha = .5) + \n    geom_text(check_overlap = TRUE) + \n    labs(\n        x = \"Province Code\",\n        y = \"Random Effect\",\n        fill = \"Province\"\n    ) + geom_label(aes(fill = provs_fullnames), colour = \"white\", fontface = \"bold\") + geom_line(linetype = \"dashed\") + \n    scale_color_manual(values = provs_fullnames, name = \"province\")",
    "crumbs": [
      "Home",
      "Projects",
      "Project 2. Mental Health",
      "Effect Of Mental Health on Mobile Banking"
    ]
  },
  {
    "objectID": "study3.html",
    "href": "study3.html",
    "title": "Smartphone VS Smartwear",
    "section": "",
    "text": "In this chapter, I explore something that hasn’t really been looked at before: Does the type of device someone uses affect how they adopt mobile banking?\nIn real life, using a smartphone is very different from using a smart wearable device like a smartwatch. These devices have different features, screen sizes, and even different ways users interact with them. So, I wanted to find out:\n\nDo trust, security, and time-saving value of m-banking apps affect people differently depending on whether they use a smartphone or a wearable? Do demographics like age, gender, or income play a different role across devices?\n\nI used survey data from over 18,000 people in Canada and ran a set of logistic regression models. I grouped people based on whether they use just a smartphone or both a smartphone and a wearable. That’s really because you don’t use a smart wearable device as a standalone device. What I found is that device type really matters:\n\nPeople who use smartphones care more about trust and security.\nIn fact, people seem to perceive smartphones as more secure (they probably are?).\nPeople who use wearables care more about saving time and convenience.\nSome demographic factors (like education or age) influence adoption in different ways based on the device.\n\nThis work shows that mobile banking adoption isn’t one-size-fits-all. If we want to understand how people really use these tools, and how banks should design better apps; we need to look at the devices they’re using too. By “we” I mean banks and researchers alike!",
    "crumbs": [
      "Home",
      "Projects",
      "Project 3. Digital Divide"
    ]
  },
  {
    "objectID": "study3.html#do-devices-change-how-we-use-mobile-banking",
    "href": "study3.html#do-devices-change-how-we-use-mobile-banking",
    "title": "Smartphone VS Smartwear",
    "section": "",
    "text": "In this chapter, I explore something that hasn’t really been looked at before: Does the type of device someone uses affect how they adopt mobile banking?\nIn real life, using a smartphone is very different from using a smart wearable device like a smartwatch. These devices have different features, screen sizes, and even different ways users interact with them. So, I wanted to find out:\n\nDo trust, security, and time-saving value of m-banking apps affect people differently depending on whether they use a smartphone or a wearable? Do demographics like age, gender, or income play a different role across devices?\n\nI used survey data from over 18,000 people in Canada and ran a set of logistic regression models. I grouped people based on whether they use just a smartphone or both a smartphone and a wearable. That’s really because you don’t use a smart wearable device as a standalone device. What I found is that device type really matters:\n\nPeople who use smartphones care more about trust and security.\nIn fact, people seem to perceive smartphones as more secure (they probably are?).\nPeople who use wearables care more about saving time and convenience.\nSome demographic factors (like education or age) influence adoption in different ways based on the device.\n\nThis work shows that mobile banking adoption isn’t one-size-fits-all. If we want to understand how people really use these tools, and how banks should design better apps; we need to look at the devices they’re using too. By “we” I mean banks and researchers alike!",
    "crumbs": [
      "Home",
      "Projects",
      "Project 3. Digital Divide"
    ]
  },
  {
    "objectID": "study3_DA.html",
    "href": "study3_DA.html",
    "title": "Device Divide",
    "section": "",
    "text": "In this project, I focused on analyzing how mental health relates to mobile banking adoption. I used data from the Canadian Internet Use Survey 2022, which includes questions about various digital habits, and demographics. You can find the dataset here.\nFor this project, I conducted a comparative analysis of two logistic regression models one for smartphone users, refered to from here on out and PHONE users and one for smart wearable users, refered to as WEAR users. Since the sampling methodology involved clustering by provinces, I considered robust standard errors for reporting the results. To build the models, I needed to conceptualize technology, in this case, m-banking adoption. Since I’m considering two different devices, I needed factors that impact m-banking decisions for both devices to be able to compare them.\nThis was very challenging because there are not many m-banking using smartwearable device studies! So, I broadened the scope to consider any technology adoption. This is ok to do as long as the factors are not specific to a niche context. The variables are Trust, Perceived Security, Perceived Value, and few demographic varaibles such as Age, Gender, Education and Income.\nHere are my hypotheses:\n\nH1: The association between Trust and m-banking adoption is the same for smartphone and smart wearable users\nH2: The association between Perceived security and m-banking adoption is the same for smartphone and smart wearable users.\nH3: The association between Perceived value (measured by time savings) and m-banking adoption is the same for smartphone and smart wearable users.\n\nH4.1 : The association between Age and m-banking adoption is the same for smartphone and smart wearable users.\nH4.2 : The association between Gender and m-banking adoption is the same for smartphone and smart wearable users.\nH4.3 : The association between Education and m-banking adoption is the same for smartphone and smart wearable users.\nH4.4 : The association between Income and m-banking adoption is the same for smartphone and smart wearable users.\n\n\n\n\nNote that not all libraries may be utilized. The most important ones are dplyr, lme4, tidyr, lavaan, ggplot2, psych, corrr, haven, poLCA and any related libraries to these.\n\n\n\nThis dataset is very similar to CIUS 2020 from study 2. I first started by reading the entire PUMF file available.\nThis gives you information on how the survey was set up, why, and how things were measured. Then, I looked at the individual survey questions to see the available data, and how they were measured. In general, questions are measured numerically were answeres follow as such: &gt; Yes : 1, No : 2, Valid Skip: 6, Don’t Know: 7, Refusal: 8, Not Stated: 9 Of course this differs question-by-question as some questions have other answer categories and some questions (which were note used in my study) asked for numerical input from the participants (like how much did you spend online last year). To help readers understand the data, I will include the question exactly as it appears in the CIUS 2022 PUMF Data Dictionary with corresponding answer choices and codes. These will be in a blue-bordered box, and will include the Variable name (on the PUMF file), Concept, Question Body and Answers. Then I will show you in R code how I’ve re-coded and used the question as a model variable. The Variables I need are as follows:\n\nMobile banking adoption (MBANK)\nProvince (PRVNC)\nAge Group (AGE)\nGender (SEX)\nEducation Level (EDU)\nIncome Quintile (INCOME)\nUser Type (USR_TYP) - based on the following\n\nSmartphone User (isSmartPhone)\nSmartwearable User (isSmartWear)\n\nSaved Time Because of m-banking (EFF_TIME)\nPerceived Security (PSEC) - based on the following\n\nSecurity measure: restricting access to location (SEC_RES_LOC)\nSecurity measure: restricting access to data (SEC_RES_DAT)\nSecurity check: checked security of a website (SEC_ACC_WEBSEC)\nSecurity check: changed privacy settings (SEC_ACC_CHNGPRV)\nSecurity feature: security questions (SECOPT_QS)\nSecurity feature: partner login (SECOPT_PL)\nSecurity feature: two factor authentication (SECOPT_2FA)\nSecurity feature: biometric (SECOPT_BIO)\nSecurity feature: password manager (SECOPT_PAS)\n\nTrust in Banks (TRST_BANK)\nFamily Relation Satisfaction (FAMSAT)\n\nThe data is available in various formats. To avoid data loss, I decided to use the .dta format (SAS file). You need the haven package to read SAS files. This is how you’d read a SAS file:\n\ndata_2022 &lt;- read_dta(\"data/00_CIUS2022.dta\")\nds00 &lt;- data_2022\ndim(ds00)\n\n[1] 25118   342\n\n\n\nds0 &lt;- ds00\n\n\n\n\n\nRenaming variables\nCleaning data: delete the skips and such for both categorical and numerical variables\nVerifying that our measure of latent constructs are strong enough\n\n\n\n\nSince these are not direct questions but information retrieved from other sources (such as postal codes for province), some of them do not have Skips/Don’t Know/Refusal/Not Stated answers. If they have, I have added those to the cards.\nProvince, Age, Sex, Education, Income:\n\n\n\nVariable Name: PROVINCE\n\nConcept: PROVINCE\n\n\nQuestion Text/Note:\nInformation derived using postal codes.\n\n\n\nAnswer Categories\nCode\n\n\n\n\nNewfoundland and Labrador\n10\n\n\nPrince Edward Island\n11\n\n\nNova Scotia\n12\n\n\nNew Brunswick\n13\n\n\nQuebec\n24\n\n\nOntario\n35\n\n\nManitoba\n46\n\n\nSaskatchewan\n47\n\n\nAlberta\n48\n\n\nBritish Columbia\n59\n\n\nValid skip\n96\n\n\nDon’t know\n97\n\n\nRefusal\n98\n\n\nNot stated\n99\n\n\n\n\n\n\n\nVariable Name: AGE_GRP\n\nConcept: Age Groups - Derived variable\n\n\nQuestion Text/Note:\nInformation derived from age of persons in household.\n\n\n\nAnswer Categories\nCode\n\n\n\n\n15 to 24 years\n01\n\n\n25 to 34 years\n02\n\n\n35 to 44 years\n03\n\n\n45 to 54 years\n04\n\n\n55 to 64 years\n05\n\n\n65 years and over\n06\n\n\nValid skip\n96\n\n\nDon’t know\n97\n\n\nRefusal\n98\n\n\nNot stated\n99\n\n\n\n\n\n\n\nVariable Name: GENDER\n\nConcept: Gender - Derived variable\n\n\nQuestion Text/Note:\nRefers to current gender which may be different from sex assigned at birth and may be different from what is indicated on legal documents. For data quality and confidentiality reasons, and because of the small population being measured, the dissemination of data according to ’Non binary’ Gender is not possible for this statistical program. So, this release uses a gender variable with only two categories. This variable is derived by looking at a large number of demographic characteristics from the respondent, it allows us to disseminate data on Gender that is reliable and unbiased.\n\n\n\nAnswer Categories\nCode\n\n\n\n\nMale\n1\n\n\nFemale\n2\n\n\nValid skip\n6\n\n\nDon’t know\n7\n\n\nRefusal\n8\n\n\nNot stated\n9\n\n\n\n\n\n\n\nVariable Name: EMP\n\nConcept: Employment status - Derived variable\n\n\nQuestion Text/Note:\n\n\n\nAnswer Categories\nCode\n\n\n\n\nEmployed\n1\n\n\nNot employed\n2\n\n\nValid skip\n6\n\n\nDon’t know\n7\n\n\nRefusal\n8\n\n\nNot stated\n9\n\n\n\n\n\n\n\nVariable Name: EDU\n\nConcept: Highest certificate - Derived variable\n\n\nQuestion Text/Note:\n\n\n\nAnswer Categories\nCode\n\n\n\n\nHigh school or less\n1\n\n\nSome post-secondary (incl. univ certificate)\n2\n\n\nUniversity degree\n3\n\n\nValid skip\n6\n\n\nDon’t know\n7\n\n\nRefusal\n8\n\n\nNot stated\n9\n\n\n\n\n\n\n\nVariable Name: HINCQUIN\n\nConcept: Census family income quintile - Derived variable\n\n\nQuestion Text/Note:\nInformation derived using HINC. In order to obtain equal weighted counts in each category, cases with incomes equal to the category cutoffs were randomly assigned to one of the two categories on either side of the cutoff.\nSource Annual Income Estimates for Census Families and Individuals (T1 Family File)\n\n\n\nAnswer Categories\nCode\n\n\n\n\nQuintile 1 - \\leq $42,256\n1\n\n\nQuintile 2 - $42,257 - $72,366\n2\n\n\nQuintile 3 - $72,367 - $107,480\n3\n\n\nQuintile 4 - $107,481 - $163,750\n4\n\n\nQuintile 5 - &gt; $163,750\n5\n\n\nValid skip\n6\n\n\nDon’t know\n7\n\n\nRefusal\n8\n\n\nNot stated\n9\n\n\n\n\n\nds0 &lt;- ds0 %&gt;% mutate(\n    \n    ID = as.factor(pumfid),\n    PRVNC = case_when(\n        province == 10 ~ \"NL\",\n        province == 11 ~ \"PEI\",\n        province == 12 ~ \"NS\",\n        province == 13 ~ \"NB\",\n        province == 24 ~ \"QC\",\n        province == 35 ~ \"ON\",\n        province == 46 ~ \"MB\",\n        province == 47 ~ \"SK\",\n        province == 48 ~ \"AB\",\n        province == 59 ~ \"BC\",\n        .default = \"default\"\n    ),\n\n    AGE = ifelse(\n        AGE_GRP &gt; 10,\n        0,\n        AGE_GRP\n    ),\n    \n    SEX = case_when(\n        gender == 1 ~ 0, #\"M\",\n        gender == 2 ~ 1, #\"F\",\n        .default = -1 #\"default\" #other\n    ),\n    \n    EMP = case_when(\n        emp == 1 ~ 1,\n        emp == 2 ~ 0, #no\n        .default = -1\n    ),\n    \n    EDU = case_when(\n        edu == 1 ~ 1, #\"Highschool\",\n        edu == 2 ~ 2, #\"College\",\n        edu == 3 ~ 3, #\"University\",\n        .default = 0 #\"default\"\n    ),\n    \n    INCOME = case_when(\n        hincquin == 1 ~ 1, #\"Q1\",\n        hincquin == 2 ~ 2, #\"Q2\",\n        hincquin == 3 ~ 3, #\"Q3\",\n        hincquin == 4 ~ 4, #\"Q4\",\n        hincquin == 5 ~ 5, #\"Q5\",\n        .default = 0 #\"default\"\n    )\n)\n\n\n\n\nDevices and Mbanking:\n\n\n\nVariable Name: DV_010A\n\nConcept: Devices used\n\n\nQuestion Text/Note:\nDuring the past three months, what devices did you use to access the Internet? Did you use: A smartphone\n\n\n\nAnswer Categories\nCode\n\n\n\n\nYes\n1\n\n\nNo\n2\n\n\nValid skip\n6\n\n\nDon’t know\n7\n\n\nRefusal\n8\n\n\nNot stated\n9\n\n\n\n\n\n\n\nVariable Name: DV_010G\n\nConcept: Devices used\n\n\nQuestion Text/Note:\nDuring the past three months, what devices did you use to access the Internet? Did you use: Internet-connected wearable smart devices\n\n\n\nAnswer Categories\nCode\n\n\n\n\nYes\n1\n\n\nNo\n2\n\n\nValid skip\n6\n\n\nDon’t know\n7\n\n\nRefusal\n8\n\n\nNot stated\n9\n\n\n\n\nCIUS’s Microdata User Guide has a section (section 4. Concepts and Defintions) but it does not include a definition for Internet connected smart wearable devices. Using the internet, some examples of these smart wearable devices are: - Smart glasses - Smart watch - Fitness Trackers - Smart Shirt - GPS devices (SGPS/GPRS Body Control) - Bluetooth Key Trackers - Smart Belts - Smart Rings - Smart Bracelets - Virtual Reality devices - Smart clothing\nMore specific to Canada, according to Ingenium.ca, the top devices are: - Smartwatches (Apple Watch, Samsung Galaxy Watch and Fitbits) - Fitness Trackers (Fitbit, Garmin) - Health Monitoring Devices (continuous glucose monitors)\nAlso true from CIUS 2020’s report: &gt; In addition, 14% of Canadians used Internet-connected wearable smart devices, such as a smart watch, Fit Bit or glucose monitoring device\nIt’s safe to assume that smart wearables most definitely include smartwatches.\n\n\n\nVariable Name: UI_050D\n\nConcept: Activities related to other online activities\n\n\nQuestion Text/Note:\nDuring the past three months, which of the following other online activities, have you done over the Internet? Have you: Conducted online banking\n\n\n\nAnswer Categories\nCode\n\n\n\n\nYes\n1\n\n\nNo\n2\n\n\nValid skip\n6\n\n\nDon’t know\n7\n\n\nRefusal\n8\n\n\nNot stated\n9\n\n\n\n\n\nds0 &lt;- ds0 %&gt;% mutate(\n    SMRTPHN = case_when(\n        DV_010A == 1 ~ 1,\n        DV_010A == 2 ~ 0,\n        .default = -1\n    ),\n    \n    SMRTWTCH = case_when(\n        DV_010G == 1 ~ 1,\n        DV_010G == 2 ~ 0,\n        .default = -1\n    ),\n    \n    MBANK = case_when(\n        UI_050D == 1 ~ 1,\n        UI_050D == 2 ~ 0,\n        .default = -1\n    )\n)\n\nTime Saving Effects\n\n\n\nVariable Name: UI_110E\n\nConcept: Effects of the use of online activities\n\n\nQuestion Text/Note:\nDuring the past 12 months, did your use of online activities have any of the following effects? Did it: Save you time\n\n\n\nAnswer Categories\nCode\n\n\n\n\nYes\n1\n\n\nNo\n2\n\n\nValid skip\n6\n\n\nDon’t know\n7\n\n\nRefusal\n8\n\n\nNot stated\n9\n\n\n\n\n\nds0 &lt;- ds0 %&gt;% mutate(\n    EFF_TIME = case_when(\n        UI_110E == 1 ~ 1,\n        UI_110E == 2 ~ 0,\n        .default = -1\n    )\n)\n\nAnd since the online activity I’m considering is mobile banking, this would be about mobile banking (more on this in my paper, as it’s not entirely true - this is a limitation).\nSecurity\n\n\n\nVariable Name: SP_010A\n\nConcept: Activities carried out to manage access to personal data\n\n\nQuestion Text/Note:\nHave you carried out any of the following to manage access to your personal data over the Internet during the past 12 months? Have you: Restricted or refused access to your geographical location\n\n\n\nAnswer Categories\nCode\n\n\n\n\nYes\n1\n\n\nNo\n2\n\n\nValid skip\n6\n\n\nDon’t know\n7\n\n\nRefusal\n8\n\n\nNot stated\n9\n\n\n\n\n\n\n\nVariable Name: SP_010B\n\nConcept: Activities carried out to manage access to personal data\n\n\nQuestion Text/Note:\nHave you carried out any of the following to manage access to your personal data over the Internet during the past 12 months? Have you: Refused allowing the use of personal data for advertising purposes\n\n\n\nAnswer Categories\nCode\n\n\n\n\nYes\n1\n\n\nNo\n2\n\n\nValid skip\n6\n\n\nDon’t know\n7\n\n\nRefusal\n8\n\n\nNot stated\n9\n\n\n\n\n\n\n\nVariable Name: SP_010C\n\nConcept: Activities carried out to manage access to personal data\n\n\nQuestion Text/Note:\nHave you carried out any of the following to manage access to your personal data over the Internet during the past 12 months? Have you: Checked that the website where you provided personal data was secure\n\n\n\nAnswer Categories\nCode\n\n\n\n\nYes\n1\n\n\nNo\n2\n\n\nValid skip\n6\n\n\nDon’t know\n7\n\n\nRefusal\n8\n\n\nNot stated\n9\n\n\n\n\n\n\n\nVariable Name: SP_010D\n\nConcept: Activities carried out to manage access to personal data\n\n\nQuestion Text/Note:\nHave you carried out any of the following to manage access to your personal data over the Internet during the past 12 months? Have you: Changed the privacy settings on accounts or apps\n\n\n\nAnswer Categories\nCode\n\n\n\n\nYes\n1\n\n\nNo\n2\n\n\nValid skip\n6\n\n\nDon’t know\n7\n\n\nRefusal\n8\n\n\nNot stated\n9\n\n\n\n\nSecurity Measures - Setting Up\n\n\n\nVariable Name: SP_020A\n\nConcept: Verified identity over the Internet\n\n\nQuestion Text/Note:\nDuring the past 12 months, did you enable any of the following optional security features to verify your identity when accessing accounts or applications over the Internet? Did you enable: Answers to personalized security questions\n\n\n\nAnswer Categories\nCode\n\n\n\n\nYes\n1\n\n\nNo\n2\n\n\nValid skip\n6\n\n\nDon’t know\n7\n\n\nRefusal\n8\n\n\nNot stated\n9\n\n\n\n\n\n\n\nVariable Name: SP_020B\n\nConcept: Verified identity over the Internet\n\n\nQuestion Text/Note:\nDuring the past 12 months, did you enable any of the following optional security features to verify your identity when accessing accounts or applications over the Internet? Did you enable: Partner login\n\n\n\nAnswer Categories\nCode\n\n\n\n\nYes\n1\n\n\nNo\n2\n\n\nValid skip\n6\n\n\nDon’t know\n7\n\n\nRefusal\n8\n\n\nNot stated\n9\n\n\n\n\n\n\n\nVariable Name: SP_020C\n\nConcept: Verified identity over the Internet\n\n\nQuestion Text/Note:\nDuring the past 12 months, did you enable any of the following optional security features to verify your identity when accessing accounts or applications over the Internet? Did you enable: Two-factor authentication or two-step verification\n\n\n\nAnswer Categories\nCode\n\n\n\n\nYes\n1\n\n\nNo\n2\n\n\nValid skip\n6\n\n\nDon’t know\n7\n\n\nRefusal\n8\n\n\nNot stated\n9\n\n\n\n\n\n\n\nVariable Name: SP_020D\n\nConcept: Verified identity over the Internet\n\n\nQuestion Text/Note:\nDuring the past 12 months, did you enable any of the following optional security features to verify your identity when accessing accounts or applications over the Internet? Did you enable: Biometric security features for online functions\n\n\n\nAnswer Categories\nCode\n\n\n\n\nYes\n1\n\n\nNo\n2\n\n\nValid skip\n6\n\n\nDon’t know\n7\n\n\nRefusal\n8\n\n\nNot stated\n9\n\n\n\n\n\n\n\nVariable Name: SP_020E\n\nConcept: Verified identity over the Internet\n\n\nQuestion Text/Note:\nDuring the past 12 months, did you enable any of the following optional security features to verify your identity when accessing accounts or applications over the Internet? Did you enable: Password manager program\n\n\n\nAnswer Categories\nCode\n\n\n\n\nYes\n1\n\n\nNo\n2\n\n\nValid skip\n6\n\n\nDon’t know\n7\n\n\nRefusal\n8\n\n\nNot stated\n9\n\n\n\n\n\nds0 &lt;- ds0 %&gt;% mutate(\n    SEC_RES_LOC = case_when(\n        SP_010A == 1 ~ 1,\n        SP_010A == 2 ~ 0,\n        .default = -1\n    ),\n\n    SEC_RES_DAT = case_when(\n        SP_010B == 1 ~ 1,\n        SP_010B == 2 ~ 0,\n        .default = -1\n    ),\n\n    SEC_ACC_WEBSEC = case_when(\n        SP_010C == 1 ~ 1,\n        SP_010C == 2 ~ 0,\n        .default = -1\n    ),\n    \n    SEC_ACC_CHNGPRV = case_when(\n        SP_010D == 1 ~ 1,\n        SP_010D == 2 ~ 0,\n        .default = -1\n    ),\n\n    SECOPT_QS = case_when(\n        SP_020A == 1 ~ 1,\n        SP_020A == 2 ~ 0,\n        .default = -1\n    ),\n    \n    \n    SECOPT_PL = case_when(\n        SP_020B == 1 ~ 1,\n        SP_020B == 2 ~ 0,\n        .default = -1\n    ),\n\n    SECOPT_2FA = case_when(\n        SP_020C == 1 ~ 1,\n        SP_020C == 2 ~ 0,\n        .default = -1\n    ),\n    \n    SECOPT_BIO = case_when(\n        SP_020D == 1 ~ 1,\n        SP_020D == 2 ~ 0,\n        .default = -1\n    ),\n    \n    SECOPT_PAS = case_when(\n        SP_020E == 1 ~ 1,\n        SP_020E == 2 ~ 0,\n        .default = -1\n    )\n)\n\nTrust In Banks/Financial Institutes\n\n\n\nVariable Name: SP_040B\n\nConcept: Personal information - Trust in organizations\n\n\nQuestion Text/Note:\nIn general, on a scale from 1 to 5 where 1 means “cannot be trusted at all” and 5 means “can be trusted completely”, to what extent do you trust the following organizations with your personal information? Would you say: b. Banking or other financial institutions\n\n\n\nAnswer Categories\nCode\n\n\n\n\n1 - Cannot be trusted at all\n1\n\n\n2\n2\n\n\n3 - Neutral\n3\n\n\n4\n4\n\n\n5 - Can be trusted completely\n5\n\n\nValid skip\n6\n\n\nDon’t know\n7\n\n\nRefusal\n8\n\n\nNot stated\n9\n\n\n\n\n\nds0 &lt;- ds0 %&gt;% mutate(\n    TRST_BANK = case_when(\n        SP_040B == 1 ~ 1, #cannot be trusted \n        SP_040B == 2 ~ 2, \n        SP_040B == 3 ~ 3, #neutral\n        SP_040B == 4 ~ 4,\n        SP_040B == 5 ~ 5, #totally trusted \n        .default = 0\n    )\n)\n\nSelecting only the useful columns:\n\nds_useful &lt;- ds0 %&gt;% dplyr::select(wtpg:TRST_BANK)\n\nI have to make sure that online banking is actually capturing mobile banking, so, people must be smartphone users:\n\nds_mobilebank &lt;- ds_useful %&gt;% filter(SMRTPHN == 1)\n\n\ndim(ds_mobilebank)\n\n[1] 20136    22\n\n\nLooking at the data,\n\nglimpse(ds_mobilebank)\n\nRows: 20,136\nColumns: 22\n$ wtpg            &lt;dbl&gt; 1264.0837, 3413.0468, 585.5445, 378.8767, 4060.0513, 7…\n$ ID              &lt;fct&gt; 100001, 100002, 100004, 100005, 100007, 100008, 100009…\n$ PRVNC           &lt;chr&gt; \"QC\", \"MB\", \"QC\", \"SK\", \"QC\", \"QC\", \"AB\", \"ON\", \"SK\", …\n$ AGE             &lt;dbl&gt; 3, 1, 5, 4, 2, 4, 6, 3, 3, 6, 2, 3, 4, 5, 4, 5, 5, 4, …\n$ SEX             &lt;dbl&gt; 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, …\n$ EMP             &lt;dbl&gt; 1, 0, 0, 1, 1, 1, 0, 1, -1, 0, 1, 1, 1, -1, 1, 1, 0, 1…\n$ EDU             &lt;dbl&gt; 3, 1, 2, 1, 2, 2, 2, 2, 0, 3, 3, 3, 3, 2, 1, 3, 3, 2, …\n$ INCOME          &lt;dbl&gt; 2, 2, 4, 2, 2, 5, 5, 5, 5, 4, 3, 4, 3, 3, 1, 4, 2, 4, …\n$ SMRTPHN         &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ SMRTWTCH        &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ MBANK           &lt;dbl&gt; 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ EFF_TIME        &lt;dbl&gt; 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, …\n$ SEC_RES_LOC     &lt;dbl&gt; 1, 0, 1, 0, 0, 0, 0, 1, -1, 0, 1, 1, 1, 0, 1, 1, 0, 0,…\n$ SEC_RES_DAT     &lt;dbl&gt; 1, 0, 1, 0, 0, 0, 1, 0, -1, 0, 1, 1, 1, 0, 1, 1, 0, 0,…\n$ SEC_ACC_WEBSEC  &lt;dbl&gt; 1, 0, 0, 0, 1, 0, 1, 0, -1, 0, 0, 0, 0, 0, 1, 1, 0, 0,…\n$ SEC_ACC_CHNGPRV &lt;dbl&gt; 1, 0, 1, 0, 0, 0, 1, 0, -1, 0, 1, 0, 1, 0, 1, 1, 0, 0,…\n$ SECOPT_QS       &lt;dbl&gt; 1, 0, 1, 0, 0, 0, 1, 1, -1, 1, 1, 0, 0, 1, 1, 1, 0, 1,…\n$ SECOPT_PL       &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 1, 0, 1, 0, 0, 1,…\n$ SECOPT_2FA      &lt;dbl&gt; 1, 0, 1, 0, 0, 0, 0, 1, -1, 1, 1, 1, 1, 1, 1, 1, 0, 1,…\n$ SECOPT_BIO      &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 1, 0, -1, 0, 1, 1, 0, 0, 1, 1, 0, 1,…\n$ SECOPT_PAS      &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 1, 1, 0, 0, 1, 1, 0, 0,…\n$ TRST_BANK       &lt;dbl&gt; 3, 3, 3, 3, 3, 4, 3, 3, 0, 5, 4, 4, 3, 3, 3, 4, 4, 5, …\n\n\nI see there are some -1 values and some values that don’t make sense. Drop these. The data is large enough such that 100 rows won’t affect the analysis.\n\ncleaned_ds &lt;- ds_mobilebank %&gt;% filter(\n    !is.na(AGE) & \n    SEX != -1 & \n    EDU != -1 & \n    EDU != 0 & \n    INCOME != -1 & \n    SMRTWTCH != -1 & \n    MBANK != -1 & \n    EFF_TIME != -1 & \n    SEC_RES_LOC != -1 & \n    SEC_RES_DAT != -1 & \n    SEC_ACC_WEBSEC != -1 & \n    SEC_ACC_CHNGPRV != -1 & \n    SECOPT_QS != -1 & \n    SECOPT_PL != -1 & \n    SECOPT_2FA != -1 & \n    SECOPT_BIO != -1 & \n    SECOPT_PAS != -1 & \n    TRST_BANK != -1 & \n    TRST_BANK != 0 \n    )\n\n\ndim(cleaned_ds)\n\n[1] 18552    22\n\n\nSaving this in a database called wrk_ds (working database):\n\nwrk_ds &lt;- cleaned_ds",
    "crumbs": [
      "Home",
      "Projects",
      "Project 3. Digital Divide",
      "Device Divide"
    ]
  },
  {
    "objectID": "study3_DA.html#data-analysis",
    "href": "study3_DA.html#data-analysis",
    "title": "Device Divide",
    "section": "",
    "text": "In this project, I focused on analyzing how mental health relates to mobile banking adoption. I used data from the Canadian Internet Use Survey 2022, which includes questions about various digital habits, and demographics. You can find the dataset here.\nFor this project, I conducted a comparative analysis of two logistic regression models one for smartphone users, refered to from here on out and PHONE users and one for smart wearable users, refered to as WEAR users. Since the sampling methodology involved clustering by provinces, I considered robust standard errors for reporting the results. To build the models, I needed to conceptualize technology, in this case, m-banking adoption. Since I’m considering two different devices, I needed factors that impact m-banking decisions for both devices to be able to compare them.\nThis was very challenging because there are not many m-banking using smartwearable device studies! So, I broadened the scope to consider any technology adoption. This is ok to do as long as the factors are not specific to a niche context. The variables are Trust, Perceived Security, Perceived Value, and few demographic varaibles such as Age, Gender, Education and Income.\nHere are my hypotheses:\n\nH1: The association between Trust and m-banking adoption is the same for smartphone and smart wearable users\nH2: The association between Perceived security and m-banking adoption is the same for smartphone and smart wearable users.\nH3: The association between Perceived value (measured by time savings) and m-banking adoption is the same for smartphone and smart wearable users.\n\nH4.1 : The association between Age and m-banking adoption is the same for smartphone and smart wearable users.\nH4.2 : The association between Gender and m-banking adoption is the same for smartphone and smart wearable users.\nH4.3 : The association between Education and m-banking adoption is the same for smartphone and smart wearable users.\nH4.4 : The association between Income and m-banking adoption is the same for smartphone and smart wearable users.\n\n\n\n\nNote that not all libraries may be utilized. The most important ones are dplyr, lme4, tidyr, lavaan, ggplot2, psych, corrr, haven, poLCA and any related libraries to these.\n\n\n\nThis dataset is very similar to CIUS 2020 from study 2. I first started by reading the entire PUMF file available.\nThis gives you information on how the survey was set up, why, and how things were measured. Then, I looked at the individual survey questions to see the available data, and how they were measured. In general, questions are measured numerically were answeres follow as such: &gt; Yes : 1, No : 2, Valid Skip: 6, Don’t Know: 7, Refusal: 8, Not Stated: 9 Of course this differs question-by-question as some questions have other answer categories and some questions (which were note used in my study) asked for numerical input from the participants (like how much did you spend online last year). To help readers understand the data, I will include the question exactly as it appears in the CIUS 2022 PUMF Data Dictionary with corresponding answer choices and codes. These will be in a blue-bordered box, and will include the Variable name (on the PUMF file), Concept, Question Body and Answers. Then I will show you in R code how I’ve re-coded and used the question as a model variable. The Variables I need are as follows:\n\nMobile banking adoption (MBANK)\nProvince (PRVNC)\nAge Group (AGE)\nGender (SEX)\nEducation Level (EDU)\nIncome Quintile (INCOME)\nUser Type (USR_TYP) - based on the following\n\nSmartphone User (isSmartPhone)\nSmartwearable User (isSmartWear)\n\nSaved Time Because of m-banking (EFF_TIME)\nPerceived Security (PSEC) - based on the following\n\nSecurity measure: restricting access to location (SEC_RES_LOC)\nSecurity measure: restricting access to data (SEC_RES_DAT)\nSecurity check: checked security of a website (SEC_ACC_WEBSEC)\nSecurity check: changed privacy settings (SEC_ACC_CHNGPRV)\nSecurity feature: security questions (SECOPT_QS)\nSecurity feature: partner login (SECOPT_PL)\nSecurity feature: two factor authentication (SECOPT_2FA)\nSecurity feature: biometric (SECOPT_BIO)\nSecurity feature: password manager (SECOPT_PAS)\n\nTrust in Banks (TRST_BANK)\nFamily Relation Satisfaction (FAMSAT)\n\nThe data is available in various formats. To avoid data loss, I decided to use the .dta format (SAS file). You need the haven package to read SAS files. This is how you’d read a SAS file:\n\ndata_2022 &lt;- read_dta(\"data/00_CIUS2022.dta\")\nds00 &lt;- data_2022\ndim(ds00)\n\n[1] 25118   342\n\n\n\nds0 &lt;- ds00\n\n\n\n\n\nRenaming variables\nCleaning data: delete the skips and such for both categorical and numerical variables\nVerifying that our measure of latent constructs are strong enough\n\n\n\n\nSince these are not direct questions but information retrieved from other sources (such as postal codes for province), some of them do not have Skips/Don’t Know/Refusal/Not Stated answers. If they have, I have added those to the cards.\nProvince, Age, Sex, Education, Income:\n\n\n\nVariable Name: PROVINCE\n\nConcept: PROVINCE\n\n\nQuestion Text/Note:\nInformation derived using postal codes.\n\n\n\nAnswer Categories\nCode\n\n\n\n\nNewfoundland and Labrador\n10\n\n\nPrince Edward Island\n11\n\n\nNova Scotia\n12\n\n\nNew Brunswick\n13\n\n\nQuebec\n24\n\n\nOntario\n35\n\n\nManitoba\n46\n\n\nSaskatchewan\n47\n\n\nAlberta\n48\n\n\nBritish Columbia\n59\n\n\nValid skip\n96\n\n\nDon’t know\n97\n\n\nRefusal\n98\n\n\nNot stated\n99\n\n\n\n\n\n\n\nVariable Name: AGE_GRP\n\nConcept: Age Groups - Derived variable\n\n\nQuestion Text/Note:\nInformation derived from age of persons in household.\n\n\n\nAnswer Categories\nCode\n\n\n\n\n15 to 24 years\n01\n\n\n25 to 34 years\n02\n\n\n35 to 44 years\n03\n\n\n45 to 54 years\n04\n\n\n55 to 64 years\n05\n\n\n65 years and over\n06\n\n\nValid skip\n96\n\n\nDon’t know\n97\n\n\nRefusal\n98\n\n\nNot stated\n99\n\n\n\n\n\n\n\nVariable Name: GENDER\n\nConcept: Gender - Derived variable\n\n\nQuestion Text/Note:\nRefers to current gender which may be different from sex assigned at birth and may be different from what is indicated on legal documents. For data quality and confidentiality reasons, and because of the small population being measured, the dissemination of data according to ’Non binary’ Gender is not possible for this statistical program. So, this release uses a gender variable with only two categories. This variable is derived by looking at a large number of demographic characteristics from the respondent, it allows us to disseminate data on Gender that is reliable and unbiased.\n\n\n\nAnswer Categories\nCode\n\n\n\n\nMale\n1\n\n\nFemale\n2\n\n\nValid skip\n6\n\n\nDon’t know\n7\n\n\nRefusal\n8\n\n\nNot stated\n9\n\n\n\n\n\n\n\nVariable Name: EMP\n\nConcept: Employment status - Derived variable\n\n\nQuestion Text/Note:\n\n\n\nAnswer Categories\nCode\n\n\n\n\nEmployed\n1\n\n\nNot employed\n2\n\n\nValid skip\n6\n\n\nDon’t know\n7\n\n\nRefusal\n8\n\n\nNot stated\n9\n\n\n\n\n\n\n\nVariable Name: EDU\n\nConcept: Highest certificate - Derived variable\n\n\nQuestion Text/Note:\n\n\n\nAnswer Categories\nCode\n\n\n\n\nHigh school or less\n1\n\n\nSome post-secondary (incl. univ certificate)\n2\n\n\nUniversity degree\n3\n\n\nValid skip\n6\n\n\nDon’t know\n7\n\n\nRefusal\n8\n\n\nNot stated\n9\n\n\n\n\n\n\n\nVariable Name: HINCQUIN\n\nConcept: Census family income quintile - Derived variable\n\n\nQuestion Text/Note:\nInformation derived using HINC. In order to obtain equal weighted counts in each category, cases with incomes equal to the category cutoffs were randomly assigned to one of the two categories on either side of the cutoff.\nSource Annual Income Estimates for Census Families and Individuals (T1 Family File)\n\n\n\nAnswer Categories\nCode\n\n\n\n\nQuintile 1 - \\leq $42,256\n1\n\n\nQuintile 2 - $42,257 - $72,366\n2\n\n\nQuintile 3 - $72,367 - $107,480\n3\n\n\nQuintile 4 - $107,481 - $163,750\n4\n\n\nQuintile 5 - &gt; $163,750\n5\n\n\nValid skip\n6\n\n\nDon’t know\n7\n\n\nRefusal\n8\n\n\nNot stated\n9\n\n\n\n\n\nds0 &lt;- ds0 %&gt;% mutate(\n    \n    ID = as.factor(pumfid),\n    PRVNC = case_when(\n        province == 10 ~ \"NL\",\n        province == 11 ~ \"PEI\",\n        province == 12 ~ \"NS\",\n        province == 13 ~ \"NB\",\n        province == 24 ~ \"QC\",\n        province == 35 ~ \"ON\",\n        province == 46 ~ \"MB\",\n        province == 47 ~ \"SK\",\n        province == 48 ~ \"AB\",\n        province == 59 ~ \"BC\",\n        .default = \"default\"\n    ),\n\n    AGE = ifelse(\n        AGE_GRP &gt; 10,\n        0,\n        AGE_GRP\n    ),\n    \n    SEX = case_when(\n        gender == 1 ~ 0, #\"M\",\n        gender == 2 ~ 1, #\"F\",\n        .default = -1 #\"default\" #other\n    ),\n    \n    EMP = case_when(\n        emp == 1 ~ 1,\n        emp == 2 ~ 0, #no\n        .default = -1\n    ),\n    \n    EDU = case_when(\n        edu == 1 ~ 1, #\"Highschool\",\n        edu == 2 ~ 2, #\"College\",\n        edu == 3 ~ 3, #\"University\",\n        .default = 0 #\"default\"\n    ),\n    \n    INCOME = case_when(\n        hincquin == 1 ~ 1, #\"Q1\",\n        hincquin == 2 ~ 2, #\"Q2\",\n        hincquin == 3 ~ 3, #\"Q3\",\n        hincquin == 4 ~ 4, #\"Q4\",\n        hincquin == 5 ~ 5, #\"Q5\",\n        .default = 0 #\"default\"\n    )\n)\n\n\n\n\nDevices and Mbanking:\n\n\n\nVariable Name: DV_010A\n\nConcept: Devices used\n\n\nQuestion Text/Note:\nDuring the past three months, what devices did you use to access the Internet? Did you use: A smartphone\n\n\n\nAnswer Categories\nCode\n\n\n\n\nYes\n1\n\n\nNo\n2\n\n\nValid skip\n6\n\n\nDon’t know\n7\n\n\nRefusal\n8\n\n\nNot stated\n9\n\n\n\n\n\n\n\nVariable Name: DV_010G\n\nConcept: Devices used\n\n\nQuestion Text/Note:\nDuring the past three months, what devices did you use to access the Internet? Did you use: Internet-connected wearable smart devices\n\n\n\nAnswer Categories\nCode\n\n\n\n\nYes\n1\n\n\nNo\n2\n\n\nValid skip\n6\n\n\nDon’t know\n7\n\n\nRefusal\n8\n\n\nNot stated\n9\n\n\n\n\nCIUS’s Microdata User Guide has a section (section 4. Concepts and Defintions) but it does not include a definition for Internet connected smart wearable devices. Using the internet, some examples of these smart wearable devices are: - Smart glasses - Smart watch - Fitness Trackers - Smart Shirt - GPS devices (SGPS/GPRS Body Control) - Bluetooth Key Trackers - Smart Belts - Smart Rings - Smart Bracelets - Virtual Reality devices - Smart clothing\nMore specific to Canada, according to Ingenium.ca, the top devices are: - Smartwatches (Apple Watch, Samsung Galaxy Watch and Fitbits) - Fitness Trackers (Fitbit, Garmin) - Health Monitoring Devices (continuous glucose monitors)\nAlso true from CIUS 2020’s report: &gt; In addition, 14% of Canadians used Internet-connected wearable smart devices, such as a smart watch, Fit Bit or glucose monitoring device\nIt’s safe to assume that smart wearables most definitely include smartwatches.\n\n\n\nVariable Name: UI_050D\n\nConcept: Activities related to other online activities\n\n\nQuestion Text/Note:\nDuring the past three months, which of the following other online activities, have you done over the Internet? Have you: Conducted online banking\n\n\n\nAnswer Categories\nCode\n\n\n\n\nYes\n1\n\n\nNo\n2\n\n\nValid skip\n6\n\n\nDon’t know\n7\n\n\nRefusal\n8\n\n\nNot stated\n9\n\n\n\n\n\nds0 &lt;- ds0 %&gt;% mutate(\n    SMRTPHN = case_when(\n        DV_010A == 1 ~ 1,\n        DV_010A == 2 ~ 0,\n        .default = -1\n    ),\n    \n    SMRTWTCH = case_when(\n        DV_010G == 1 ~ 1,\n        DV_010G == 2 ~ 0,\n        .default = -1\n    ),\n    \n    MBANK = case_when(\n        UI_050D == 1 ~ 1,\n        UI_050D == 2 ~ 0,\n        .default = -1\n    )\n)\n\nTime Saving Effects\n\n\n\nVariable Name: UI_110E\n\nConcept: Effects of the use of online activities\n\n\nQuestion Text/Note:\nDuring the past 12 months, did your use of online activities have any of the following effects? Did it: Save you time\n\n\n\nAnswer Categories\nCode\n\n\n\n\nYes\n1\n\n\nNo\n2\n\n\nValid skip\n6\n\n\nDon’t know\n7\n\n\nRefusal\n8\n\n\nNot stated\n9\n\n\n\n\n\nds0 &lt;- ds0 %&gt;% mutate(\n    EFF_TIME = case_when(\n        UI_110E == 1 ~ 1,\n        UI_110E == 2 ~ 0,\n        .default = -1\n    )\n)\n\nAnd since the online activity I’m considering is mobile banking, this would be about mobile banking (more on this in my paper, as it’s not entirely true - this is a limitation).\nSecurity\n\n\n\nVariable Name: SP_010A\n\nConcept: Activities carried out to manage access to personal data\n\n\nQuestion Text/Note:\nHave you carried out any of the following to manage access to your personal data over the Internet during the past 12 months? Have you: Restricted or refused access to your geographical location\n\n\n\nAnswer Categories\nCode\n\n\n\n\nYes\n1\n\n\nNo\n2\n\n\nValid skip\n6\n\n\nDon’t know\n7\n\n\nRefusal\n8\n\n\nNot stated\n9\n\n\n\n\n\n\n\nVariable Name: SP_010B\n\nConcept: Activities carried out to manage access to personal data\n\n\nQuestion Text/Note:\nHave you carried out any of the following to manage access to your personal data over the Internet during the past 12 months? Have you: Refused allowing the use of personal data for advertising purposes\n\n\n\nAnswer Categories\nCode\n\n\n\n\nYes\n1\n\n\nNo\n2\n\n\nValid skip\n6\n\n\nDon’t know\n7\n\n\nRefusal\n8\n\n\nNot stated\n9\n\n\n\n\n\n\n\nVariable Name: SP_010C\n\nConcept: Activities carried out to manage access to personal data\n\n\nQuestion Text/Note:\nHave you carried out any of the following to manage access to your personal data over the Internet during the past 12 months? Have you: Checked that the website where you provided personal data was secure\n\n\n\nAnswer Categories\nCode\n\n\n\n\nYes\n1\n\n\nNo\n2\n\n\nValid skip\n6\n\n\nDon’t know\n7\n\n\nRefusal\n8\n\n\nNot stated\n9\n\n\n\n\n\n\n\nVariable Name: SP_010D\n\nConcept: Activities carried out to manage access to personal data\n\n\nQuestion Text/Note:\nHave you carried out any of the following to manage access to your personal data over the Internet during the past 12 months? Have you: Changed the privacy settings on accounts or apps\n\n\n\nAnswer Categories\nCode\n\n\n\n\nYes\n1\n\n\nNo\n2\n\n\nValid skip\n6\n\n\nDon’t know\n7\n\n\nRefusal\n8\n\n\nNot stated\n9\n\n\n\n\nSecurity Measures - Setting Up\n\n\n\nVariable Name: SP_020A\n\nConcept: Verified identity over the Internet\n\n\nQuestion Text/Note:\nDuring the past 12 months, did you enable any of the following optional security features to verify your identity when accessing accounts or applications over the Internet? Did you enable: Answers to personalized security questions\n\n\n\nAnswer Categories\nCode\n\n\n\n\nYes\n1\n\n\nNo\n2\n\n\nValid skip\n6\n\n\nDon’t know\n7\n\n\nRefusal\n8\n\n\nNot stated\n9\n\n\n\n\n\n\n\nVariable Name: SP_020B\n\nConcept: Verified identity over the Internet\n\n\nQuestion Text/Note:\nDuring the past 12 months, did you enable any of the following optional security features to verify your identity when accessing accounts or applications over the Internet? Did you enable: Partner login\n\n\n\nAnswer Categories\nCode\n\n\n\n\nYes\n1\n\n\nNo\n2\n\n\nValid skip\n6\n\n\nDon’t know\n7\n\n\nRefusal\n8\n\n\nNot stated\n9\n\n\n\n\n\n\n\nVariable Name: SP_020C\n\nConcept: Verified identity over the Internet\n\n\nQuestion Text/Note:\nDuring the past 12 months, did you enable any of the following optional security features to verify your identity when accessing accounts or applications over the Internet? Did you enable: Two-factor authentication or two-step verification\n\n\n\nAnswer Categories\nCode\n\n\n\n\nYes\n1\n\n\nNo\n2\n\n\nValid skip\n6\n\n\nDon’t know\n7\n\n\nRefusal\n8\n\n\nNot stated\n9\n\n\n\n\n\n\n\nVariable Name: SP_020D\n\nConcept: Verified identity over the Internet\n\n\nQuestion Text/Note:\nDuring the past 12 months, did you enable any of the following optional security features to verify your identity when accessing accounts or applications over the Internet? Did you enable: Biometric security features for online functions\n\n\n\nAnswer Categories\nCode\n\n\n\n\nYes\n1\n\n\nNo\n2\n\n\nValid skip\n6\n\n\nDon’t know\n7\n\n\nRefusal\n8\n\n\nNot stated\n9\n\n\n\n\n\n\n\nVariable Name: SP_020E\n\nConcept: Verified identity over the Internet\n\n\nQuestion Text/Note:\nDuring the past 12 months, did you enable any of the following optional security features to verify your identity when accessing accounts or applications over the Internet? Did you enable: Password manager program\n\n\n\nAnswer Categories\nCode\n\n\n\n\nYes\n1\n\n\nNo\n2\n\n\nValid skip\n6\n\n\nDon’t know\n7\n\n\nRefusal\n8\n\n\nNot stated\n9\n\n\n\n\n\nds0 &lt;- ds0 %&gt;% mutate(\n    SEC_RES_LOC = case_when(\n        SP_010A == 1 ~ 1,\n        SP_010A == 2 ~ 0,\n        .default = -1\n    ),\n\n    SEC_RES_DAT = case_when(\n        SP_010B == 1 ~ 1,\n        SP_010B == 2 ~ 0,\n        .default = -1\n    ),\n\n    SEC_ACC_WEBSEC = case_when(\n        SP_010C == 1 ~ 1,\n        SP_010C == 2 ~ 0,\n        .default = -1\n    ),\n    \n    SEC_ACC_CHNGPRV = case_when(\n        SP_010D == 1 ~ 1,\n        SP_010D == 2 ~ 0,\n        .default = -1\n    ),\n\n    SECOPT_QS = case_when(\n        SP_020A == 1 ~ 1,\n        SP_020A == 2 ~ 0,\n        .default = -1\n    ),\n    \n    \n    SECOPT_PL = case_when(\n        SP_020B == 1 ~ 1,\n        SP_020B == 2 ~ 0,\n        .default = -1\n    ),\n\n    SECOPT_2FA = case_when(\n        SP_020C == 1 ~ 1,\n        SP_020C == 2 ~ 0,\n        .default = -1\n    ),\n    \n    SECOPT_BIO = case_when(\n        SP_020D == 1 ~ 1,\n        SP_020D == 2 ~ 0,\n        .default = -1\n    ),\n    \n    SECOPT_PAS = case_when(\n        SP_020E == 1 ~ 1,\n        SP_020E == 2 ~ 0,\n        .default = -1\n    )\n)\n\nTrust In Banks/Financial Institutes\n\n\n\nVariable Name: SP_040B\n\nConcept: Personal information - Trust in organizations\n\n\nQuestion Text/Note:\nIn general, on a scale from 1 to 5 where 1 means “cannot be trusted at all” and 5 means “can be trusted completely”, to what extent do you trust the following organizations with your personal information? Would you say: b. Banking or other financial institutions\n\n\n\nAnswer Categories\nCode\n\n\n\n\n1 - Cannot be trusted at all\n1\n\n\n2\n2\n\n\n3 - Neutral\n3\n\n\n4\n4\n\n\n5 - Can be trusted completely\n5\n\n\nValid skip\n6\n\n\nDon’t know\n7\n\n\nRefusal\n8\n\n\nNot stated\n9\n\n\n\n\n\nds0 &lt;- ds0 %&gt;% mutate(\n    TRST_BANK = case_when(\n        SP_040B == 1 ~ 1, #cannot be trusted \n        SP_040B == 2 ~ 2, \n        SP_040B == 3 ~ 3, #neutral\n        SP_040B == 4 ~ 4,\n        SP_040B == 5 ~ 5, #totally trusted \n        .default = 0\n    )\n)\n\nSelecting only the useful columns:\n\nds_useful &lt;- ds0 %&gt;% dplyr::select(wtpg:TRST_BANK)\n\nI have to make sure that online banking is actually capturing mobile banking, so, people must be smartphone users:\n\nds_mobilebank &lt;- ds_useful %&gt;% filter(SMRTPHN == 1)\n\n\ndim(ds_mobilebank)\n\n[1] 20136    22\n\n\nLooking at the data,\n\nglimpse(ds_mobilebank)\n\nRows: 20,136\nColumns: 22\n$ wtpg            &lt;dbl&gt; 1264.0837, 3413.0468, 585.5445, 378.8767, 4060.0513, 7…\n$ ID              &lt;fct&gt; 100001, 100002, 100004, 100005, 100007, 100008, 100009…\n$ PRVNC           &lt;chr&gt; \"QC\", \"MB\", \"QC\", \"SK\", \"QC\", \"QC\", \"AB\", \"ON\", \"SK\", …\n$ AGE             &lt;dbl&gt; 3, 1, 5, 4, 2, 4, 6, 3, 3, 6, 2, 3, 4, 5, 4, 5, 5, 4, …\n$ SEX             &lt;dbl&gt; 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, …\n$ EMP             &lt;dbl&gt; 1, 0, 0, 1, 1, 1, 0, 1, -1, 0, 1, 1, 1, -1, 1, 1, 0, 1…\n$ EDU             &lt;dbl&gt; 3, 1, 2, 1, 2, 2, 2, 2, 0, 3, 3, 3, 3, 2, 1, 3, 3, 2, …\n$ INCOME          &lt;dbl&gt; 2, 2, 4, 2, 2, 5, 5, 5, 5, 4, 3, 4, 3, 3, 1, 4, 2, 4, …\n$ SMRTPHN         &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ SMRTWTCH        &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ MBANK           &lt;dbl&gt; 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ EFF_TIME        &lt;dbl&gt; 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, …\n$ SEC_RES_LOC     &lt;dbl&gt; 1, 0, 1, 0, 0, 0, 0, 1, -1, 0, 1, 1, 1, 0, 1, 1, 0, 0,…\n$ SEC_RES_DAT     &lt;dbl&gt; 1, 0, 1, 0, 0, 0, 1, 0, -1, 0, 1, 1, 1, 0, 1, 1, 0, 0,…\n$ SEC_ACC_WEBSEC  &lt;dbl&gt; 1, 0, 0, 0, 1, 0, 1, 0, -1, 0, 0, 0, 0, 0, 1, 1, 0, 0,…\n$ SEC_ACC_CHNGPRV &lt;dbl&gt; 1, 0, 1, 0, 0, 0, 1, 0, -1, 0, 1, 0, 1, 0, 1, 1, 0, 0,…\n$ SECOPT_QS       &lt;dbl&gt; 1, 0, 1, 0, 0, 0, 1, 1, -1, 1, 1, 0, 0, 1, 1, 1, 0, 1,…\n$ SECOPT_PL       &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 1, 0, 1, 0, 0, 1,…\n$ SECOPT_2FA      &lt;dbl&gt; 1, 0, 1, 0, 0, 0, 0, 1, -1, 1, 1, 1, 1, 1, 1, 1, 0, 1,…\n$ SECOPT_BIO      &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 1, 0, -1, 0, 1, 1, 0, 0, 1, 1, 0, 1,…\n$ SECOPT_PAS      &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 1, 1, 0, 0, 1, 1, 0, 0,…\n$ TRST_BANK       &lt;dbl&gt; 3, 3, 3, 3, 3, 4, 3, 3, 0, 5, 4, 4, 3, 3, 3, 4, 4, 5, …\n\n\nI see there are some -1 values and some values that don’t make sense. Drop these. The data is large enough such that 100 rows won’t affect the analysis.\n\ncleaned_ds &lt;- ds_mobilebank %&gt;% filter(\n    !is.na(AGE) & \n    SEX != -1 & \n    EDU != -1 & \n    EDU != 0 & \n    INCOME != -1 & \n    SMRTWTCH != -1 & \n    MBANK != -1 & \n    EFF_TIME != -1 & \n    SEC_RES_LOC != -1 & \n    SEC_RES_DAT != -1 & \n    SEC_ACC_WEBSEC != -1 & \n    SEC_ACC_CHNGPRV != -1 & \n    SECOPT_QS != -1 & \n    SECOPT_PL != -1 & \n    SECOPT_2FA != -1 & \n    SECOPT_BIO != -1 & \n    SECOPT_PAS != -1 & \n    TRST_BANK != -1 & \n    TRST_BANK != 0 \n    )\n\n\ndim(cleaned_ds)\n\n[1] 18552    22\n\n\nSaving this in a database called wrk_ds (working database):\n\nwrk_ds &lt;- cleaned_ds",
    "crumbs": [
      "Home",
      "Projects",
      "Project 3. Digital Divide",
      "Device Divide"
    ]
  },
  {
    "objectID": "study3_DA.html#perceived-security-measurement",
    "href": "study3_DA.html#perceived-security-measurement",
    "title": "Device Divide",
    "section": "Perceived Security Measurement",
    "text": "Perceived Security Measurement\nSince there is no perceived security measure I need to define it as a latent variable. In the CIUS2022, there’s no single, direct question asking:\n\nHow secure do you think mobile banking is?\n\nThat means “Perceived Security” isn’t directly measured. However, we have multiple related questions (e.g., about data protection, privacy, security measures, etc.). These indirect items are observable variables that reflect an unobservable (latent) concept: the user’s overall perception of security.\nI use Confirmatory Factor Analysis (CFA) to test if these items really reflect one underlying factor, i.e., PSEC. This increases measurement reliability. Here are the steps of CFA:\nStep 1. Reliability Check Using Cronbach’s Alpha, I check the internal consistency of the items. Usually, if \\alpha &gt; 0.7, the items are measuring the same idea.\n\nalph &lt;- psych::alpha(wrk_ds[, c(\"SEC_RES_LOC\", \"SEC_RES_DAT\", \"SEC_ACC_WEBSEC\", \"SEC_ACC_CHNGPRV\", \n            \"SECOPT_QS\", \"SECOPT_PL\", \"SECOPT_2FA\", \n            \"SECOPT_BIO\", \"SECOPT_PAS\")])\n\nalph$total$raw_alpha\n\n[1] 0.8103267\n\n\nWhich means the measurement is strong.\nStep 2. Exploratory Factor Check With Parallel Analysis, I can decide how many factors to extract.\n\nfa.parallel(wrk_ds[, c(\"SEC_RES_LOC\", \"SEC_RES_DAT\", \"SEC_ACC_WEBSEC\", \"SEC_ACC_CHNGPRV\", \n                       \"SECOPT_QS\", \"SECOPT_PL\", \"SECOPT_2FA\", \"SECOPT_BIO\", \"SECOPT_PAS\")], \n            fa = \"fa\")\n\n\nNote: the eighenvalue on the y-axis is basically the varaince each factor explains\nFrom a parallel scree plot, you should look for an “elbow” - this is where the curve changes direction, which looks like 2 here. Parallel analysis suggests 2 separate factors. That is, it could be that the factors I’m looking at are actually describing two different latent ideas:\n\nMaybe security measures like questions, 2FA, biometrics\nand security concerns overall, like not allowing access to location or data\n\nHowever, I still think they’re basically both perceptions of security of an app. You only do these things if you think the app is not secure or you’re worried about security. So, I’ll keep these factors and decide if something should be combined or dropped from CFA results.\nStep 3. Confirmatory Factor Analysis The mathematical model is \nX_i = \\lambda_i PSEC_i + \\epsilon_i\n Where X_i’s are the observed variables, \\lambda_i is factor loading of varaible i and \\epsilon is the error term. This is as if asking “can all 9 items be explained by 1 variable (PSEC)?”. I’m adding covariances to improve the model fit. How did I decide how to add covariates (this notation \\sim\\sim)? Trial and error, tbh! But also, mostly intuitive. The questions about security measures like setting up 2FA, questions, biometrics, password managers and partner login are all kind of related (to the same idea of “setting up and using security measures/features”). They have the same context on CIUS questionnairs, too. Similarly, the data and location access are kind of about the same idea: they’re both restrictions on what is shared about you. Lastly, changing privacy and checking a website’s security is are both activities that are not part of security features. There’s no set up, and all websites and apps have to allow you to be able to change privacy settings and you can check any website.\n\nf1 &lt;- '\n    f =~ SEC_RES_LOC + SEC_RES_DAT + SEC_ACC_WEBSEC + SEC_ACC_CHNGPRV + SECOPT_QS + SECOPT_PL + SECOPT_2FA + SECOPT_BIO + SECOPT_PAS\n    \n    # Adding covariances between related error terms (based on modindices)\n    SEC_RES_LOC ~~  SEC_RES_DAT\n\n    SECOPT_QS   ~~  SECOPT_2FA \n    SECOPT_BIO  ~~  SECOPT_PAS \n    SEC_ACC_WEBSEC  ~~  SEC_ACC_CHNGPRV  \n    SECOPT_QS   ~~  SECOPT_PL   \n'\ncompatibility_fac &lt;- cfa(f1, data = wrk_ds, std.lv = TRUE)\n\nsummary(compatibility_fac, fit.measures = TRUE, standardized = TRUE)\n\nlavaan 0.6.16 ended normally after 43 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        23\n\n  Number of observations                         18552\n\nModel Test User Model:\n                                                      \n  Test statistic                              1328.000\n  Degrees of freedom                                22\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                             42055.218\n  Degrees of freedom                                36\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.969\n  Tucker-Lewis Index (TLI)                       0.949\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -93888.613\n  Loglikelihood unrestricted model (H1)     -93224.613\n                                                      \n  Akaike (AIC)                              187823.227\n  Bayesian (BIC)                            188003.278\n  Sample-size adjusted Bayesian (SABIC)     187930.185\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.057\n  90 Percent confidence interval - lower         0.054\n  90 Percent confidence interval - upper         0.059\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    0.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.034\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  f =~                                                                  \n    SEC_RES_LOC       0.274    0.004   74.861    0.000    0.274    0.581\n    SEC_RES_DAT       0.300    0.004   82.428    0.000    0.300    0.627\n    SEC_ACC_WEBSEC    0.281    0.004   70.897    0.000    0.281    0.564\n    SEC_ACC_CHNGPR    0.336    0.004   88.679    0.000    0.336    0.673\n    SECOPT_QS         0.258    0.004   65.258    0.000    0.258    0.523\n    SECOPT_PL         0.219    0.004   58.760    0.000    0.219    0.466\n    SECOPT_2FA        0.275    0.003   81.371    0.000    0.275    0.620\n    SECOPT_BIO        0.221    0.004   58.219    0.000    0.221    0.462\n    SECOPT_PAS        0.217    0.004   55.881    0.000    0.217    0.446\n\nCovariances:\n                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n .SEC_RES_LOC ~~                                                         \n   .SEC_RES_DAT        0.059    0.001   40.627    0.000    0.059    0.414\n .SECOPT_QS ~~                                                           \n   .SECOPT_2FA         0.026    0.001   19.955    0.000    0.026    0.181\n .SECOPT_BIO ~~                                                          \n   .SECOPT_PAS         0.033    0.002   21.891    0.000    0.033    0.180\n .SEC_ACC_WEBSEC ~~                                                      \n   .SEC_ACC_CHNGPR     0.020    0.002   13.119    0.000    0.020    0.131\n .SECOPT_QS ~~                                                           \n   .SECOPT_PL          0.021    0.001   14.773    0.000    0.021    0.119\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .SEC_RES_LOC       0.147    0.002   80.302    0.000    0.147    0.663\n   .SEC_RES_DAT       0.139    0.002   76.666    0.000    0.139    0.607\n   .SEC_ACC_WEBSEC    0.169    0.002   79.760    0.000    0.169    0.682\n   .SEC_ACC_CHNGPR    0.137    0.002   70.015    0.000    0.137    0.547\n   .SECOPT_QS         0.176    0.002   83.349    0.000    0.176    0.726\n   .SECOPT_PL         0.173    0.002   88.126    0.000    0.173    0.783\n   .SECOPT_2FA        0.121    0.002   77.426    0.000    0.121    0.615\n   .SECOPT_BIO        0.180    0.002   88.309    0.000    0.180    0.787\n   .SECOPT_PAS        0.190    0.002   88.964    0.000    0.190    0.802\n    f                 1.000                               1.000    1.000\n\n\nFor CFA, this is how you’d interpret the results:\n\n\n\n\n\n\n\n\nMetric\nDesired Threshold\nDescription\n\n\n\n\nCFI (Comparative Fit Index)\n&gt; 0.90\nHigher is better fit\n\n\nTLI (Tucker-Lewis Index)\n&gt; 0.90\nHigher is better fit\n\n\nRMSEA (Root Mean Square Error of Approx.)\n&lt; 0.08 or ideally &lt; 0.05\nLower is better (error)\n\n\nSRMR (Standardized Root Mean Residual)\n&lt; 0.08\nLower is better\n\n\n\nThe more of these “checkboxes” your summary output checks, the better the fit for the factor. Factor loadings are about how strongly each factor represents/reflects the latent factor. The higher the values the better (and significant).\nMy results are excellent for now, but, it’s also good to check for residuals and correlations:\nStep 4. Are Items Correlated?\n\nmodindices(compatibility_fac, sort = TRUE, minimum.value = 10)\n\n               lhs op             rhs      mi    epc sepc.lv sepc.all sepc.nox\n53       SECOPT_PL ~~      SECOPT_PAS 169.942  0.018   0.018    0.100    0.100\n54      SECOPT_2FA ~~      SECOPT_BIO 128.215  0.014   0.014    0.092    0.092\n51       SECOPT_PL ~~      SECOPT_2FA 115.978  0.014   0.014    0.096    0.096\n32     SEC_RES_DAT ~~  SEC_ACC_WEBSEC 111.173  0.012   0.012    0.081    0.081\n52       SECOPT_PL ~~      SECOPT_BIO  92.814  0.013   0.013    0.074    0.074\n46 SEC_ACC_CHNGPRV ~~      SECOPT_2FA  86.313 -0.012  -0.012   -0.090   -0.090\n33     SEC_RES_DAT ~~ SEC_ACC_CHNGPRV  82.720  0.011   0.011    0.077    0.077\n26     SEC_RES_LOC ~~ SEC_ACC_CHNGPRV  63.660  0.009   0.009    0.064    0.064\n28     SEC_RES_LOC ~~       SECOPT_PL  58.695 -0.009  -0.009   -0.055   -0.055\n45 SEC_ACC_CHNGPRV ~~       SECOPT_PL  56.901 -0.010  -0.010   -0.066   -0.066\n30     SEC_RES_LOC ~~      SECOPT_BIO  46.796 -0.008  -0.008   -0.048   -0.048\n41  SEC_ACC_WEBSEC ~~      SECOPT_2FA  38.361 -0.008  -0.008   -0.054   -0.054\n35     SEC_RES_DAT ~~       SECOPT_PL  35.212 -0.007  -0.007   -0.044   -0.044\n55      SECOPT_2FA ~~      SECOPT_PAS  32.752  0.007   0.007    0.046    0.046\n36     SEC_RES_DAT ~~      SECOPT_2FA  30.066 -0.006  -0.006   -0.043   -0.043\n31     SEC_RES_LOC ~~      SECOPT_PAS  25.153 -0.006  -0.006   -0.035   -0.035\n38     SEC_RES_DAT ~~      SECOPT_PAS  23.792 -0.006  -0.006   -0.035   -0.035\n42  SEC_ACC_WEBSEC ~~      SECOPT_BIO  22.575 -0.007  -0.007   -0.038   -0.038\n43  SEC_ACC_WEBSEC ~~      SECOPT_PAS  19.748 -0.006  -0.006   -0.035   -0.035\n37     SEC_RES_DAT ~~      SECOPT_BIO  18.991 -0.005  -0.005   -0.032   -0.032\n34     SEC_RES_DAT ~~       SECOPT_QS  12.791 -0.004  -0.004   -0.026   -0.026\n50       SECOPT_QS ~~      SECOPT_PAS  11.720  0.005   0.005    0.026    0.026\n\n\nA few modification indeces are quite large, indicating these variables are highly correlated. So, it’s a good idea to add a covariance for them to the model. However, adding too much risks complicating the model and potentially overfitting. Since my analysis so far shows strong fit (above 90% CFI), I stop here. However, there’s room for improvement!\nStep 5. Check Sample-sized control RMSEA Since my sample size is quite large, the RMSEA is affected. In fact, all the metrics (especially \\chi^2) are. So, I’ll calculate the sample-sized normalized RMSEA:\n\nfitmeasures(compatibility_fac, \"rmsea\") / sqrt(18552)\n\nrmsea \n    0 \n\n\nAnd check the reliability of the model:\n\n#reliability(compatibility_fac)\n\nStep 6. Subgroup Analysis Before we define PSEC, I have to figure out how many different categories I want. The smallest would be 2: high and low. I want to check for unobserved subgroups/subclasses. This is called Latent Class Analysis (LCA). Since the values for the items had 0’s in them, I have to add 1’s to everything (required for poLCA).\n\nwrk_ds[, c(\"SEC_RES_LOC\", \"SEC_RES_DAT\", \"SEC_ACC_WEBSEC\", \"SEC_ACC_CHNGPRV\",\n           \"SECOPT_QS\", \"SECOPT_PL\", \"SECOPT_2FA\", \"SECOPT_BIO\", \"SECOPT_PAS\")] &lt;- \n  wrk_ds[, c(\"SEC_RES_LOC\", \"SEC_RES_DAT\", \"SEC_ACC_WEBSEC\", \"SEC_ACC_CHNGPRV\",\n             \"SECOPT_QS\", \"SECOPT_PL\", \"SECOPT_2FA\", \"SECOPT_BIO\", \"SECOPT_PAS\")] + 1\n\nform &lt;- cbind(SEC_RES_LOC, SEC_RES_DAT, SEC_ACC_WEBSEC, SEC_ACC_CHNGPRV,\n            SECOPT_QS, SECOPT_PL, SECOPT_2FA, SECOPT_BIO, SECOPT_PAS) ~ 1\n\n# Run LCA with 2 latent classes\nlca_model &lt;- poLCA(form, data = wrk_ds, nclass = 2, maxiter = 5000)\n\nConditional item response (column) probabilities,\n by outcome variable, for each class (row) \n \n$SEC_RES_LOC\n           Pr(1)  Pr(2)\nclass 1:  0.6822 0.3178\nclass 2:  0.1056 0.8944\n\n$SEC_RES_DAT\n           Pr(1)  Pr(2)\nclass 1:  0.7381 0.2619\nclass 2:  0.1067 0.8933\n\n$SEC_ACC_WEBSEC\n           Pr(1)  Pr(2)\nclass 1:  0.8797 0.1203\nclass 2:  0.3350 0.6650\n\n$SEC_ACC_CHNGPRV\n           Pr(1)  Pr(2)\nclass 1:  0.8775 0.1225\nclass 2:  0.2212 0.7788\n\n$SECOPT_QS\n           Pr(1)  Pr(2)\nclass 1:  0.7263 0.2737\nclass 2:  0.2124 0.7876\n\n$SECOPT_PL\n           Pr(1)  Pr(2)\nclass 1:  0.9084 0.0916\nclass 2:  0.5167 0.4833\n\n$SECOPT_2FA\n           Pr(1)  Pr(2)\nclass 1:  0.5936 0.4064\nclass 2:  0.0585 0.9415\n\n$SECOPT_BIO\n           Pr(1)  Pr(2)\nclass 1:  0.8906 0.1094\nclass 2:  0.4840 0.5160\n\n$SECOPT_PAS\n           Pr(1)  Pr(2)\nclass 1:  0.8515 0.1485\nclass 2:  0.4569 0.5431\n\nEstimated class population shares \n 0.3951 0.6049 \n \nPredicted class memberships (by modal posterior prob.) \n 0.3843 0.6157 \n \n========================================================= \nFit for 2 latent classes: \n========================================================= \nnumber of observations: 18552 \nnumber of estimated parameters: 19 \nresidual degrees of freedom: 492 \nmaximum log-likelihood: -93737.84 \n \nAIC(2): 187513.7\nBIC(2): 187662.4\nG^2(2): 9189.15 (Likelihood ratio/deviance statistic) \nX^2(2): 17134.16 (Chi-square goodness of fit) \n \n\nsummary(lca_model)\n\n               Length Class      Mode   \nllik               1  -none-     numeric\nattempts           1  -none-     numeric\nprobs.start        9  -none-     list   \nprobs              9  -none-     list   \nprobs.se           9  -none-     list   \nP.se               2  -none-     numeric\nposterior      37104  -none-     numeric\npredclass      18552  -none-     numeric\nP                  2  -none-     numeric\nnumiter            1  -none-     numeric\nprobs.start.ok     1  -none-     logical\ncoeff              1  -none-     logical\ncoeff.se           1  -none-     logical\ncoeff.V            1  -none-     logical\neflag              1  -none-     logical\nnpar               1  -none-     numeric\naic                1  -none-     numeric\nbic                1  -none-     numeric\nNobs               1  -none-     numeric\nChisq              1  -none-     numeric\npredcell          11  data.frame list   \nGsq                1  -none-     numeric\ny                  9  data.frame list   \nx                  1  data.frame list   \nN                  1  -none-     numeric\nmaxiter            1  -none-     numeric\nresid.df           1  -none-     numeric\ntime               1  difftime   numeric\ncall               5  -none-     call   \n\n\nEssentially, I’m grouping people into classes based on their security attitudes using the 9 items that I have. Let’s check the 2 classes results. How to interpret the results:\n\nFirst, Pr(1) is probability the person in class x picks 1 for an item (1 here is the previous 0). So:\n\n\n$SEC_RES_LOC Pr(1), class 1: 0.68 Pr(2), class 2: 0.11\n\nMeans people in class 1 are more likely to not restrict their location (SEC_RES_LOC = 0 has a higher chance in class 1). This can just be people with high and low security tolerance. The Estimated class population shares for classes are very close to the prediction of my model, i.e., Predicted class memberships (by modal posterior prob.). This means these are very good estimates. The fit statistics shows: AIC, BIC, deviance, and \\chi^2.\nNow let’s try for 2,3 and 4 classes and pick the best - using AIC/BIC (lower = better fit):\n\ninvisible(capture.output({lca_2 &lt;- poLCA(form, data = wrk_ds, nclass = 2)}))\ninvisible(capture.output({lca_3 &lt;- poLCA(form, data = wrk_ds, nclass = 3)}))\ninvisible(capture.output({lca_4 &lt;- poLCA(form, data = wrk_ds, nclass = 4)}))\n\n# Compare AIC and BIC\ndata.frame(\n  Classes = 2:4,\n  AIC = c(lca_2$aic, lca_3$aic, lca_4$aic),\n  BIC = c(lca_2$bic, lca_3$bic, lca_4$bic)\n)\n\n  Classes      AIC      BIC\n1       2 187513.7 187662.4\n2       3 183851.6 184078.7\n3       4 180666.1 180971.4\n\n\nThis is telling me PSEC should have 4 levels.\nStep 7. Calculate Perceived Security Another way to define PSEC is to use Item Response Theory (1 parameter logistic model, Rasch Model). This will give a continous value for PSEC, which is preferrable for my analysis. Each item shares the same slope and I estimate\n\nP(Y_{ij} = 1) = \\frac{1}{1 + e^{PSEC_j - b_i}}\n\nWhere PSEC_j is person j’s perceived security and b_i is item i’s threshold. The idea is of b_i is: how hard is it for someone to agree with something. So, in my example, how high/low does the person’s security perception have to be to skip setting up 2FA/set up 2FA.\n\nNote: IF PSEC = b, then the person has a 50-50 chance of agreeing.\n\nBottom line: - If an item has high threshold, only those with high perceived security agree with it. - If an item has low thresholds, most agree with.\nSo, let’s define PSEC:\n\n# Run a 1-parameter logistic model (Rasch Model)\nirt_model &lt;- mirt(wrk_ds[, c(\"SEC_RES_LOC\", \"SEC_RES_DAT\", \"SEC_ACC_WEBSEC\",\n                             \"SEC_ACC_CHNGPRV\", \"SECOPT_QS\", \"SECOPT_PL\",\n                             \"SECOPT_2FA\", \"SECOPT_BIO\", \"SECOPT_PAS\")], 1, itemtype=\"Rasch\")\n\n\nIteration: 1, Log-Lik: -94814.100, Max-Change: 0.40153\nIteration: 2, Log-Lik: -93680.902, Max-Change: 0.41232\nIteration: 3, Log-Lik: -93047.348, Max-Change: 0.36846\nIteration: 4, Log-Lik: -92734.044, Max-Change: 0.29365\nIteration: 5, Log-Lik: -92591.472, Max-Change: 0.21441\nIteration: 6, Log-Lik: -92529.943, Max-Change: 0.14683\nIteration: 7, Log-Lik: -92504.107, Max-Change: 0.09617\nIteration: 8, Log-Lik: -92493.300, Max-Change: 0.06119\nIteration: 9, Log-Lik: -92488.690, Max-Change: 0.03822\nIteration: 10, Log-Lik: -92486.647, Max-Change: 0.02357\nIteration: 11, Log-Lik: -92485.693, Max-Change: 0.01445\nIteration: 12, Log-Lik: -92485.221, Max-Change: 0.00882\nIteration: 13, Log-Lik: -92484.970, Max-Change: 0.00567\nIteration: 14, Log-Lik: -92484.829, Max-Change: 0.00316\nIteration: 15, Log-Lik: -92484.756, Max-Change: 0.00192\nIteration: 16, Log-Lik: -92484.712, Max-Change: 0.00125\nIteration: 17, Log-Lik: -92484.685, Max-Change: 0.00070\nIteration: 18, Log-Lik: -92484.670, Max-Change: 0.00040\nIteration: 19, Log-Lik: -92484.661, Max-Change: 0.00028\nIteration: 20, Log-Lik: -92484.655, Max-Change: 0.00015\nIteration: 21, Log-Lik: -92484.652, Max-Change: 0.00009\n\n# Extract person-level scores (PSEC)\nwrk_ds$PSEC &lt;- fscores(irt_model)\n\nThis is a 1-factor model (PSEC). We find the factor scores (fscores()), otherwise known as latent trat scores, which estimate each person’s position on the latent construct (basically, each person’s PSEC score). Scores are usually centered around 0, and the more negative values are lower scores.\n\ncoef(irt_model, simplify=TRUE)\n\n$items\n                a1      d g u\nSEC_RES_LOC      1  1.022 0 1\nSEC_RES_DAT      1  0.867 0 1\nSEC_ACC_WEBSEC   1 -0.348 0 1\nSEC_ACC_CHNGPRV  1  0.078 0 1\nSECOPT_QS        1  0.483 0 1\nSECOPT_PL        1 -1.113 0 1\nSECOPT_2FA       1  1.483 0 1\nSECOPT_BIO       1 -0.937 0 1\nSECOPT_PAS       1 -0.735 0 1\n\n$means\nF1 \n 0 \n\n$cov\n      F1\nF1 3.093\n\n\nSummary statistics of PSEC values:\n\nsummary(wrk_ds$PSEC)\n\n       F1            \n Min.   :-2.8540382  \n 1st Qu.:-1.3479926  \n Median : 0.1676207  \n Mean   : 0.0009833  \n 3rd Qu.: 1.2059331  \n Max.   : 2.7349158  \n\n\nSimilar to this, but much easier - since I already used lavaan to do CFA, you can just use the same package to calculate the PSEC values for users. This is what I will use for the paper:\n\nwrk_ds$PSEC &lt;- lavPredict(compatibility_fac)\nsummary(wrk_ds$PSEC)\n\n       f          \n Min.   :-1.5300  \n 1st Qu.:-0.7069  \n Median : 0.1029  \n Mean   : 0.0000  \n 3rd Qu.: 0.7779  \n Max.   : 1.3023",
    "crumbs": [
      "Home",
      "Projects",
      "Project 3. Digital Divide",
      "Device Divide"
    ]
  },
  {
    "objectID": "study3_DA.html#build-datasets-for-modeling",
    "href": "study3_DA.html#build-datasets-for-modeling",
    "title": "Device Divide",
    "section": "Build Datasets for Modeling",
    "text": "Build Datasets for Modeling\nSo, I need 3 different datasets for modeling: - Full Data: includes everything - PHON-only Data: only smartphone users - WEAR-only Data: only smartwear users\nLet’s separate the data:\n\nwrk_ds &lt;- wrk_ds %&gt;% mutate(\n    isSmartPhone = if_else(\n        SMRTPHN == 1,\n        1,\n        0\n    ),\n    isSmartWear = if_else(\n        SMRTWTCH == 1,\n        1,\n        0\n    )\n)\n\n\nSome Visualizations\n\nggplot(wrk_ds, aes(x = PSEC)) +\n  geom_histogram(binwidth = 0.5, fill = \"steelblue\", color = \"black\", alpha = 0.7) +\n  labs(title = \"Distribution of Perceived Security (PSEC) Scores\", \n       x = \"PSEC Score\", \n       y = \"Number of Users\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\np1 &lt;- ggplot(aes(x = as.factor(isSmartWear), y = MBANK, color = as.factor(PRVNC)), data = wrk_ds) + \n    stat_summary(fun.data = \"mean_cl_boot\", geom = 'line', aes(group = as.factor(PRVNC))) +\n    labs(x = \"Wearable User\", y = \"Probability of M-banking\", color = \"Province\") + \n    scale_color_brewer(palette = \"Paired\") +\n    theme_minimal()\n\np2 &lt;- ggplot(wrk_ds, aes(as.factor(PRVNC), MBANK, color = as.factor(isSmartWear))) +\n                  stat_summary(fun = mean, geom = \"point\") +\n                  stat_summary(fun.data = mean_cl_boot, geom = \"errorbar\", width = 0.4) +\n                  theme_set(theme_bw(base_size = 10)) +\n                  theme(legend.position = \"top\") +\n                  labs(x = \"Province\", y = \"Observed Probabilty of mobile banking\", color = \"Wearable User\") + theme_minimal()\n\nggarrange(p1, p2, ncol = 2)\n\n\n\n\n\n\n\n\nIt doesn’t really looks like there’s much grouping happening here! Let’s build the datasets:\n\nwrk_ds &lt;- wrk_ds %&gt;% mutate(\n    scaled_wtpg = wtpg/max(wtpg)\n)\n\n\nwrk_ds_fulldata &lt;- wrk_ds %&gt;% mutate(\n    # if name has \"_f\" after it, it's a factor. \n    # if name has a \"_c\" after it, it's mean centered. \n    # if name has no trailing letter, it's just an integer (probably 0-1)\n    \n    PRVNC_f = as.factor(PRVNC),\n    SEX_f = factor(SEX,levels = c(\"0\", \"1\")), \n    \n    EDU_f = factor(EDU, levels = c(\"1\", \"2\", \"3\")), \n    EDU_c = EDU - mean(EDU),\n    \n    AGE_f = as.factor(AGE),\n    AGE_f = relevel(AGE_f, ref = \"2\"),\n    AGE_c = AGE - mean(AGE),\n\n    INCOME_f = as.factor(INCOME), \n    INCOME_c = INCOME - mean(INCOME),\n    \n    EFF_TIME_f = as.factor(EFF_TIME),\n    \n\n    # added variables \n    \n    PSEC_c1 = PSEC - min(PSEC) + 1,\n    PSEC_c = PSEC - mean(PSEC), \n    PSEC_scaled = 1 + 7 * ((PSEC - min(PSEC)) / (max(PSEC) - min(PSEC))),\n    \n    TRST_BANK_f = as.factor(TRST_BANK),\n    TRST_BANK_f = relevel(TRST_BANK_f, ref = \"5\"),\n    TRST_BANK_c = TRST_BANK - mean(TRST_BANK),\n    \n    USR_TYP = case_when(\n        isSmartWear == 1 ~ \"WEAR\",\n        isSmartWear == 0 ~ \"PHON\",\n        .default = \"OTHER\"\n        )\n    )\n\n\nwrk_ds_fulldata_PHON &lt;- wrk_ds_fulldata %&gt;% filter(isSmartWear == 0) \nwrk_ds_fulldata_WEAR &lt;- wrk_ds_fulldata %&gt;% filter(isSmartWear == 1) \n\nLet’s see the counts for the groups:\n\nctab &lt;- table(wrk_ds_fulldata$MBANK, wrk_ds_fulldata$USR_TYP)\nctab\n\n   \n     PHON  WEAR\n  0  2214   169\n  1 13174  2995\n\n\nA simple \\chi^2 test shows these users are significantly different in how they m-bank:\n\nchisq.test(ctab)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  ctab\nX-squared = 191.04, df = 1, p-value &lt; 2.2e-16\n\n\nSummary Statistics of the dataset:\n\npsycDescribe &lt;- psych::describe(\n    wrk_ds_fulldata %&gt;% dplyr::select(isSmartWear, AGE, SEX, EDU, INCOME, EFF_TIME, TRST_BANK, PSEC) #if it's numeric.\n    ) \n\npsycDescribe &lt;- as.data.frame(psycDescribe)\nxtable(psycDescribe %&gt;% dplyr::select(n, mean, sd, median, min, max))\n\n% latex table generated in R 4.2.2 by xtable 1.8-4 package\n% Sun Jun 22 20:03:01 2025\n\\begin{table}[ht]\n\\centering\n\\begin{tabular}{rrrrrrr}\n  \\hline\n & n & mean & sd & median & min & max \\\\ \n  \\hline\nisSmartWear & 18552.00 & 0.17 & 0.38 & 0.00 & 0.00 & 1.00 \\\\ \n  AGE & 18552.00 & 4.12 & 1.50 & 4.00 & 1.00 & 6.00 \\\\ \n  SEX & 18552.00 & 0.52 & 0.50 & 1.00 & 0.00 & 1.00 \\\\ \n  EDU & 18552.00 & 2.15 & 0.79 & 2.00 & 1.00 & 3.00 \\\\ \n  INCOME & 18552.00 & 3.23 & 1.35 & 3.00 & 1.00 & 5.00 \\\\ \n  EFF\\_TIME & 18552.00 & 0.51 & 0.50 & 1.00 & 0.00 & 1.00 \\\\ \n  TRST\\_BANK & 18552.00 & 3.71 & 0.96 & 4.00 & 1.00 & 5.00 \\\\ \n  PSEC & 18552.00 & 0.00 & 0.88 & 0.10 & -1.53 & 1.30 \\\\ \n   \\hline\n\\end{tabular}\n\\end{table}",
    "crumbs": [
      "Home",
      "Projects",
      "Project 3. Digital Divide",
      "Device Divide"
    ]
  },
  {
    "objectID": "study3_DA.html#modeling",
    "href": "study3_DA.html#modeling",
    "title": "Device Divide",
    "section": "Modeling",
    "text": "Modeling\nFirst, since there’s clustering on provinces, I run a fixed effect model and test it against a simple logistic regression to see if there is a grouping effect. First of all, the model fails to converge -\n\nmodel_full_fixedeffect &lt;- glmer(\n    MBANK ~ AGE_f + SEX_f + EDU_f + INCOME_f + EFF_TIME_f + TRST_BANK_f + PSEC + (1 | PRVNC),\n    data = wrk_ds_fulldata,\n    family = binomial,\n    weights = scaled_wtpg\n)\n\n\nWarning :non-integer #successes in a binomial glm! Warning :failure to converge in 10000 evaluations Warning :convergence code 4 from Nelder_Mead: failure to converge in 10000 evaluations Warning :unable to evaluate scaled gradient Warning :Model failed to converge: degenerate Hessian with 1 negative eigenvalues\n\n\nmodel_full &lt;- glm(\n    MBANK ~ AGE_f + SEX_f + EDU_f + INCOME_f + EFF_TIME_f + TRST_BANK_f + PSEC,\n    data = wrk_ds_fulldata,\n    family = quasibinomial,\n    weights = scaled_wtpg\n)\n\nThe LR test shows that there is no improvement to the model, so going with the simpler model is better.\n\nlrtest(model_full, model_full_fixedeffect)\n\n\n\n\n#DF\nLogLik\nDf\nChisq\nPr(&gt;Chisq)\n\n\n\n\n19\n\n\n\n\n\n\n20\n-27.041\n1\n\n\n\n\n\nThe mathematical formulations of all three models follows this:\n\n\\begin{equation*}\n    \\begin{split}\n        & logit(MBANK) = \\\\\n        & \\hspace{1cm} \\beta_0 +\n        \\beta_1 \\ AGE_1 \\ +\n        \\beta_2 \\ AGE_3 \\ +\n        \\beta_3 \\ AGE_4 \\ +\n        \\beta_4 \\ AGE_5 \\ +\n        \\beta_5 \\ AGE_6 \\ + \\\\\n        & \\hspace{1cm} \\beta_6 \\ SEX_F \\ +\n        \\beta_7 \\ EDU_2 \\ +\n        \\beta_8 \\ EDU_3 \\ +\n        \\beta_{9} \\ INCOME_2 \\ +\n        \\beta_{10} \\ INCOME_3 \\ + \\\\\n        & \\hspace{1cm} \\beta_{11} \\ INCOME_4 \\ +\n        \\beta_{12} \\ INCOME_5 \\ +\n        \\beta_{13} \\ EFF\\_TIME \\ +\n        \\beta_{14} \\ TRST\\_BANK_1 \\ + \\\\\n        & \\hspace{1cm} \\beta_{15} \\ TRST\\_BANK_2 \\ +\n        \\beta_{16} \\ TRST\\_BANK_3 \\ +\n        \\beta_{17} \\ TRST\\_BANK_4 \\ +\n        \\beta_{18} \\ PSEC \\ + \\epsilon \\\\\n    \\end{split}\n\\end{equation*}\n\nThe only difference is the dataset each is coming from. I do need to calculate the robust cluster standard errors for more accurate results:\n\nmodel_full_cluster_se &lt;- vcovCL(model_full, cluster = wrk_ds_fulldata$PRVNC)\n\n# summary table of results \nmodel_full_summary_clustered &lt;- coeftest(model_full, vcov = model_full_cluster_se)\n\n\n# PHON-only \nmodel_phon &lt;- glm(\n    MBANK ~ AGE_f + SEX_f + EDU_f + INCOME_f + EFF_TIME_f + TRST_BANK_f + PSEC,\n    data = wrk_ds_fulldata_PHON,\n    family = quasibinomial,\n    weights = scaled_wtpg\n)\n\nmodel_phon_cluster_se &lt;- vcovCL(model_phon, cluster = wrk_ds_fulldata_PHON$PRVNC)\nmodel_phon_summary_clustered &lt;- coeftest(model_phon, vcov = model_phon_cluster_se)\n\n\n# WEAR-only\nmodel_wear &lt;- glm(\n    MBANK ~ AGE_f + SEX_f + EDU_f + INCOME_f + EFF_TIME_f + TRST_BANK_f + PSEC,\n    data = wrk_ds_fulldata_WEAR,\n    family = quasibinomial,\n    weights = scaled_wtpg\n)\n\nmodel_wear_cluster_se &lt;- vcovCL(model_wear, cluster = wrk_ds_fulldata_WEAR$PRVNC)\nmodel_wear_summary_clustered &lt;- coeftest(model_wear, vcov = model_wear_cluster_se)\n\nUsing textreg (in my original R markdown file, and htmlreg here for website preview) to produce \\LaTeX code for the table:\n\nscreenreg(\n  list(model_full_summary_clustered, model_phon_summary_clustered, model_wear_summary_clustered),\n  custom.model.names = c(\"Full Data (coeff)\", \n                         \"Phone Only (coeff)\", \n                         \"WEAR Only (coeff)\")\n)\n\n\n======================================================================\n              Full Data (coeff)  Phone Only (coeff)  WEAR Only (coeff)\n----------------------------------------------------------------------\n(Intercept)    1.83 ***           1.95 ***            1.23 **         \n              (0.16)             (0.20)              (0.39)           \nAGE_f1        -1.01 ***          -1.12 ***           -0.25            \n              (0.05)             (0.11)              (0.25)           \nAGE_f3         0.02              -0.06                0.52 **         \n              (0.10)             (0.12)              (0.16)           \nAGE_f4        -0.03              -0.12                1.00            \n              (0.13)             (0.18)              (0.52)           \nAGE_f5        -0.09              -0.13                0.16            \n              (0.17)             (0.20)              (0.30)           \nAGE_f6        -0.23              -0.29                0.39            \n              (0.13)             (0.15)              (0.35)           \nSEX_f1         0.11 **            0.09 ***            0.16            \n              (0.03)             (0.02)              (0.27)           \nEDU_f2         0.32 ***           0.25 ***            1.02 ***        \n              (0.07)             (0.05)              (0.29)           \nEDU_f3         0.47 ***           0.42 ***            0.94 ***        \n              (0.02)             (0.03)              (0.22)           \nINCOME_f2      0.15 *             0.20 *             -0.51            \n              (0.07)             (0.09)              (0.35)           \nINCOME_f3      0.20 *             0.21 *             -0.03            \n              (0.09)             (0.10)              (0.28)           \nINCOME_f4      0.43 ***           0.42 ***            0.19            \n              (0.12)             (0.12)              (0.42)           \nINCOME_f5      0.59 ***           0.60 ***            0.16            \n              (0.10)             (0.15)              (0.44)           \nEFF_TIME_f1    0.56 ***           0.54 ***            0.69 *          \n              (0.05)             (0.04)              (0.33)           \nTRST_BANK_f1  -1.44 ***          -1.53 ***           -0.00            \n              (0.18)             (0.17)              (0.59)           \nTRST_BANK_f2  -0.69 ***          -0.67 ***           -0.66 ***        \n              (0.17)             (0.18)              (0.19)           \nTRST_BANK_f3  -0.67 ***          -0.73 ***           -0.29            \n              (0.13)             (0.12)              (0.19)           \nTRST_BANK_f4  -0.11              -0.16                0.31            \n              (0.12)             (0.09)              (0.40)           \nPSEC           0.93 ***           0.94 ***            0.70 ***        \n              (0.05)             (0.04)              (0.09)           \n======================================================================\n*** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05\n\n\nThe R code to generate \\LaTeX code:\n\ntexreg(\n  list(model_full_summary_clustered, \n       model_phon_summary_clustered, \n       model_wear_summary_clustered),\n  custom.model.names = c(\"Full Data\", \"Phone-Only\", \"Wear-Only\"), \n  digits = 3,              \n  stars = c(0.001, 0.01, 0.05), # Significance levels for stars\n  single.row = FALSE, # Standard errors in parentheses below coefficients\n  custom.note = \"Significance levels: *p &lt; 0.05; **p &lt; 0.01; ***p &lt; 0.001\", \n  booktabs = TRUE,   # Use booktabs-style formatting\n  caption = \"Logistic Regression Results with Robust Standard Errors\"\n)",
    "crumbs": [
      "Home",
      "Projects",
      "Project 3. Digital Divide",
      "Device Divide"
    ]
  },
  {
    "objectID": "study3_DA.html#analysis-of-results",
    "href": "study3_DA.html#analysis-of-results",
    "title": "Device Divide",
    "section": "Analysis of Results",
    "text": "Analysis of Results\nSince the point it to compare the values for coefficients in each model, I will need to perform wald test. It’s essentially just a Z statistic calculation. Let’s consider \\hat{beta_{p1}} the estimated coefficient of variable 1 from the PHON only model and \\hat{beta_{w1}} the same variable’s estimate coefficient in WEAR only model. I want to know, is the difference between the two significantly different from zero? That is, H_0: \\hat{beta_{p1}} - \\hat{beta_{w1}} = 0. If I reject H_0, that means they’re significantly different. The statistic is calculated as the ratio of the estimate (difference) over the standard error the estimate (difference): \nZ = \\frac{\\hat{beta_{p1}} - \\hat{beta_{w1}}}{\\sqrt{\\sigma_{\\beta_{p1}} + \\sigma_{\\beta_{w1}}}}\n\n\ncalc_wald_test &lt;- function(model1, model2, variable_name){\n    coeff1 &lt;- model1[variable_name, 1]\n    coeff2 &lt;- model2[variable_name, 1]\n    \n    se1 &lt;- model1[variable_name, 2]\n    se2 &lt;- model1[variable_name, 2]\n    \n    diff &lt;- coeff1 - coeff2\n    se_diff &lt;- sqrt(se1^2 + se2^2)\n    z &lt;- diff / se_diff\n    p &lt;- 2 * (1 - pnorm(abs(z)))\n    res &lt;- c(z = z, p = p)\n\n    return(res)\n}\n\nHere’s how you do this: - Get the results for FullData vs PHON - Get the results for FullData vs WEAR - Get the results for PHON vs WEAR\n\ncoefs_ &lt;- c('AGE_f1', 'AGE_f3','AGE_f4','AGE_f5','AGE_f6',\n            'SEX_f1',\n            'EDU_f2','EDU_f3',\n            'INCOME_f2','INCOME_f3','INCOME_f4','INCOME_f5',\n            'EFF_TIME_f1', \n            'TRST_BANK_f1','TRST_BANK_f2','TRST_BANK_f3','TRST_BANK_f4',\n            'PSEC')\n    \n\nresults &lt;- c()\n\n# Loop through coefficients and format results\nfor (coef_ in coefs_) {\n\n    full_vs_phone &lt;- calc_wald_test(model_full_summary_clustered, model_phon_summary_clustered, coef_)\n    full_vs_wear &lt;- calc_wald_test(model_full_summary_clustered, model_wear_summary_clustered, coef_)\n    phone_vs_wear &lt;- calc_wald_test(model_phon_summary_clustered, model_wear_summary_clustered, coef_)\n    \n  ## Check significance (if p &gt; 0.05, set as \"no\", otherwise keep z-score)\n  # the dollar signs and ^{} are because I wanted to copy paste the results into LaTeX overleaf ! IGNORE \n    full_vs_phone_text &lt;- paste0(\"$ \", round(full_vs_phone[\"z\"], 3), \"^{} $ & \", \" $ \", round(full_vs_phone[\"p\"], 3), \"$\")\n    full_vs_wear_text &lt;- paste0(\"$ \", round(full_vs_wear[\"z\"], 3), \"^{} $ & \", \" $ \", round(full_vs_wear[\"p\"], 3), \"$\")\n    phone_vs_wear_text &lt;- paste0(\"$ \", round(phone_vs_wear[\"z\"], 3), \"^{} $ & \", \" $ \", round(phone_vs_wear[\"p\"], 3), \"$\")\n\n         \n  # Format the result for this coefficient\n  # again, the &'s are for overleaf / LaTeX convenience \n  result &lt;- paste0(\n    full_vs_phone_text, \" & \",\n    full_vs_wear_text, \" & \",\n    phone_vs_wear_text\n    )\n  \n  # Append to results list\n  results &lt;- c(results, result)\n}\n\n\n# Print the results\nfor (res in results) {\n  print(res)\n}\n\n[1] \"$ 1.485^{} $ &  $ 0.137$ & $ -10.352^{} $ &  $ 0$ & $ -5.625^{} $ &  $ 0$\"\n[1] \"$ 0.555^{} $ &  $ 0.579$ & $ -3.413^{} $ &  $ 0.001$ & $ -3.48^{} $ &  $ 0.001$\"\n[1] \"$ 0.46^{} $ &  $ 0.646$ & $ -5.48^{} $ &  $ 0$ & $ -4.488^{} $ &  $ 0$\"\n[1] \"$ 0.142^{} $ &  $ 0.887$ & $ -1.072^{} $ &  $ 0.284$ & $ -1.038^{} $ &  $ 0.299$\"\n[1] \"$ 0.33^{} $ &  $ 0.742$ & $ -3.488^{} $ &  $ 0$ & $ -3.159^{} $ &  $ 0.002$\"\n[1] \"$ 0.493^{} $ &  $ 0.622$ & $ -1.138^{} $ &  $ 0.255$ & $ -2.363^{} $ &  $ 0.018$\"\n[1] \"$ 0.698^{} $ &  $ 0.485$ & $ -6.641^{} $ &  $ 0$ & $ -10.406^{} $ &  $ 0$\"\n[1] \"$ 1.5^{} $ &  $ 0.134$ & $ -13.742^{} $ &  $ 0$ & $ -12.788^{} $ &  $ 0$\"\n[1] \"$ -0.525^{} $ &  $ 0.6$ & $ 6.484^{} $ &  $ 0$ & $ 5.403^{} $ &  $ 0$\"\n[1] \"$ -0.069^{} $ &  $ 0.945$ & $ 1.932^{} $ &  $ 0.053$ & $ 1.658^{} $ &  $ 0.097$\"\n[1] \"$ 0.071^{} $ &  $ 0.943$ & $ 1.456^{} $ &  $ 0.145$ & $ 1.351^{} $ &  $ 0.177$\"\n[1] \"$ -0.063^{} $ &  $ 0.95$ & $ 3.013^{} $ &  $ 0.003$ & $ 2.13^{} $ &  $ 0.033$\"\n[1] \"$ 0.382^{} $ &  $ 0.702$ & $ -1.923^{} $ &  $ 0.054$ & $ -2.489^{} $ &  $ 0.013$\"\n[1] \"$ 0.361^{} $ &  $ 0.718$ & $ -5.769^{} $ &  $ 0$ & $ -6.453^{} $ &  $ 0$\"\n[1] \"$ -0.071^{} $ &  $ 0.943$ & $ -0.141^{} $ &  $ 0.888$ & $ -0.065^{} $ &  $ 0.948$\"\n[1] \"$ 0.287^{} $ &  $ 0.774$ & $ -2.181^{} $ &  $ 0.029$ & $ -2.682^{} $ &  $ 0.007$\"\n[1] \"$ 0.301^{} $ &  $ 0.763$ & $ -2.475^{} $ &  $ 0.013$ & $ -3.559^{} $ &  $ 0$\"\n[1] \"$ -0.19^{} $ &  $ 0.849$ & $ 3.546^{} $ &  $ 0$ & $ 4.524^{} $ &  $ 0$\"",
    "crumbs": [
      "Home",
      "Projects",
      "Project 3. Digital Divide",
      "Device Divide"
    ]
  },
  {
    "objectID": "study3_DA.html#bonus-visualizations",
    "href": "study3_DA.html#bonus-visualizations",
    "title": "Device Divide",
    "section": "Bonus Visualizations",
    "text": "Bonus Visualizations\n\nage_gg &lt;- ggplot(wrk_ds_fulldata %&gt;%\n           group_by(AGE_f, USR_TYP) %&gt;%\n           mutate(Proportion_mbank = mean(MBANK)), \n       aes(x = relevel(AGE_f, ref = \"1\"), y = Proportion_mbank, color = USR_TYP, group = USR_TYP, linetype = USR_TYP)) +\n  geom_line(size = 1) +  \n  geom_point(size = 3) +  \n  labs(\n    x = \"AGE Group\",\n    y = \"Proportion MBANK\",\n    color = \"User Type\"\n  ) +\n  theme_minimal() + theme(strip.text = element_text(size = 12, face = \"bold\"), axis.text.x = element_text(face = \"bold\"))\n\nsex_gg &lt;- ggplot(wrk_ds_fulldata %&gt;%\n           group_by(SEX_f, USR_TYP) %&gt;%\n           mutate(Proportion_mbank = mean(MBANK)), \n       aes(x = relevel(SEX_f, ref = \"1\"), y = Proportion_mbank, color = USR_TYP, group = USR_TYP, linetype = USR_TYP)) +\n  geom_line(size = 1) +  \n  geom_point(size = 3) +  \n  labs(\n    x = \"Gender\",\n    y = \"Proportion MBANK\",\n    color = \"User Type\"\n  ) +\n  theme_minimal() + theme(strip.text = element_text(size = 12, face = \"bold\"), axis.text.x = element_text(face = \"bold\"))\n\n\n\nedu_gg &lt;- ggplot(wrk_ds_fulldata %&gt;%\n           group_by(EDU_f, USR_TYP) %&gt;%\n           mutate(Proportion_mbank = mean(MBANK)), \n       aes(x = EDU_f, y = Proportion_mbank, color = USR_TYP, group = USR_TYP, linetype = USR_TYP)) +\n  geom_line(size = 1) +  \n  geom_point(size = 3) +  \n  labs(\n    x = \"Education\",\n    y = \"Proportion MBANK\",\n    color = \"User Type\"\n  ) +\n  theme_minimal() + theme(strip.text = element_text(size = 12, face = \"bold\"), axis.text.x = element_text(face = \"bold\"))\n\ntrst_gg &lt;- ggplot(wrk_ds_fulldata %&gt;%\n           group_by(TRST_BANK_f, USR_TYP) %&gt;%\n           mutate(Proportion_mbank = mean(MBANK)), \n       aes(x = relevel(TRST_BANK_f, ref = \"1\"), y = Proportion_mbank, color = USR_TYP, group = USR_TYP, linetype = USR_TYP)) +\n  geom_line(size = 1) +  \n  geom_point(size = 3) +  \n  labs(\n    x = \"Trust in Bank\",\n    y = \"Proportion MBANK\",\n    color = \"User Type\"\n  ) +\n  theme_minimal() + theme(strip.text = element_text(size = 12, face = \"bold\"), axis.text.x = element_text(face = \"bold\"))\n\nsec_gg &lt;- ggplot(wrk_ds_fulldata %&gt;%\n           mutate(PSECBIN = cut(PSEC, breaks = 7)) %&gt;% \n           group_by(PSECBIN, USR_TYP) %&gt;%\n           mutate(Proportion_mbank = mean(MBANK)), \n       aes(x = PSECBIN, y = Proportion_mbank, color = USR_TYP, group = USR_TYP, linetype = USR_TYP)) +\n  geom_line(size = 1) +  \n  geom_point(size = 2, alpha = 0.6) +  \n  labs(\n    x = \"PSEC (Binned)\",\n    y = \"Proportion MBANK\",\n    color = \"User Type\"\n  ) +\n  theme_minimal() + theme(strip.text = element_text(size = 12, face = \"bold\"), axis.text.x = element_text(face = \"bold\", angle = 45, hjust = 1))\n\nincome_gg &lt;- ggplot(wrk_ds_fulldata %&gt;%\n           group_by(INCOME_f, USR_TYP) %&gt;%\n           mutate(Proportion_mbank = mean(MBANK)), \n       aes(x = INCOME_f, y = Proportion_mbank, color = USR_TYP, group = USR_TYP, linetype = USR_TYP)) +\n  geom_line(size = 1) +  \n  geom_point(size = 3) +  \n  labs(\n    x = \"Income\",\n    y = \"Proportion MBANK\",\n    color = \"User Type\"\n  ) +\n  theme_minimal() + theme(strip.text = element_text(size = 12, face = \"bold\"), axis.text.x = element_text(face = \"bold\"))\n\neff_gg &lt;- ggplot(wrk_ds_fulldata %&gt;%\n           group_by(EFF_TIME_f, USR_TYP) %&gt;%\n           mutate(Proportion_mbank = mean(MBANK)), \n       aes(x = EFF_TIME_f, y = Proportion_mbank, color = USR_TYP, group = USR_TYP, linetype = USR_TYP)) +\n  geom_line(size = 1) +  \n  geom_point(size = 3) +  \n  labs(\n    x = \"EFF_TIME\",\n    y = \"Proportion MBANK\",\n    color = \"User Type\"\n  ) +\n  theme_minimal() + theme(strip.text = element_text(size = 12, face = \"bold\"), axis.text.x = element_text(face = \"bold\"))\n\n\n\nggarrange(age_gg, sex_gg, edu_gg, income_gg, eff_gg, trst_gg, sec_gg,\n          ncol = 4, nrow = 2) #,\n\n\n\n\n\n\n\n         # labels = c(\"AGE\", \"EDU\", \"TRST\", \"SEC_CN\"))",
    "crumbs": [
      "Home",
      "Projects",
      "Project 3. Digital Divide",
      "Device Divide"
    ]
  }
]